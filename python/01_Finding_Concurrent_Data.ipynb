{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Finding Concurrent ECOSTRESS and EMIT Data\n",
    "\n",
    "**Summary**  \n",
    "\n",
    "Both the ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) and the Earth surface Mineral dust source InvesTigation (EMIT) instruments are located on the International Space Station (ISS). Their overlapping fields of view provide an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this notebook we will show how to utilize the [`earthaccess` Python library](https://github.com/nsidc/earthaccess) to find concurrent ECOSTRESS and EMIT data. \n",
    "\n",
    "<div>\n",
    "<img src=\"../img/concurrent_data.png\" width=\"750\"/>\n",
    "</div>\n",
    "\n",
    "**Background**\n",
    "\n",
    "The **ECOSTRESS** instrument is a multispectral thermal imaging radiometer designed to answer three overarching science questions:\n",
    "\n",
    "- How is the terrestrial biosphere responding to changes in water availability?\n",
    "- How do changes in diurnal vegetation water stress  the global carbon cycle?\n",
    "- Can agricultural vulnerability be reduced through advanced monitoring of agricultural water consumptive use and improved drought   estimation?\n",
    "\n",
    "The ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants.  Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field.\n",
    "\n",
    "More details about ECOSTRESS and its associated products can be found on the [ECOSTRESS website](https://ecostress.jpl.nasa.gov/) and [ECOSTRESS product pages](https://lpdaac.usgs.gov/product_search/?query=ECOSTRESS&status=Operational&view=cards&sort=title) hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\n",
    "\n",
    "The **EMIT** instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth's surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\n",
    "\n",
    "More details about EMIT and its associated products can be found on the [EMIT website](https://earth.jpl.nasa.gov/emit/) and [EMIT product pages](https://lpdaac.usgs.gov/product_search/?query=EMIT&status=Operational&view=cards&sort=title) hosted by the LP DAAC.\n",
    "\n",
    "**Requirements**  \n",
    " - [NASA Earthdata Account](https://urs.earthdata.nasa.gov/home)   \n",
    " - *No Python setup requirements if connected to the workshop cloud instance!*  \n",
    " - **Local Only** Set up Python Environment - See **setup_instructions.md** in the `/setup/` folder to set up a local compatible Python environment \n",
    "\n",
    "**Learning Objectives**  \n",
    "- How to use `earthaccess` to find concurrent EMIT and ECOSTRESS data.  \n",
    "- How to export a list of files and download them programmatically.  \n",
    "\n",
    "**Tutorial Outline**  \n",
    "\n",
    "1. Setup\n",
    "2. Searching for EMIT and ECOSTRESS Data\n",
    "3. Organizing and Filtering Results\n",
    "4. Visualizing Intersecting Coverage\n",
    "5. Creating a list of Results and Desired Asset URLs\n",
    "6. Streaming or Downloading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import the required Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import json\n",
    "import folium\n",
    "import earthaccess\n",
    "import warnings\n",
    "# import folium.plugins\n",
    "# import folium.raster_layers\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import math\n",
    "\n",
    "from branca.element import Figure\n",
    "from IPython.display import display\n",
    "from shapely import geometry\n",
    "from shapely.geometry import MultiPolygon\n",
    "# from skimage import io\n",
    "from datetime import timedelta\n",
    "from shapely.geometry.polygon import orient\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors as mcolors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 NASA Earthdata Login Credentials\n",
    "\n",
    "To download or stream NASA data you will need an Earthdata account, you can create one [here](https://urs.earthdata.nasa.gov/home). We will use the `login` function from the `earthaccess` library for authentication before downloading at the end of the notebook. This function can also be used to create a local `.netrc` file if it doesn't exist or add your login info to an existing `.netrc` file. If no Earthdata Login credentials are found in the `.netrc` you'll be prompted for them. This step is not necessary to conduct searches but is needed to download or stream data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Search for ECOSTRESS and EMIT Data\n",
    "\n",
    "Both EMIT and ECOSTRESS products are hosted by the Land Processes Distributed Active Archive Center (LP DAAC). In this example we will use the cloud-hosted EMIT_L2A_RFL and ECOSTRESS_L2T_LSTE products available from the LP DAAC to find data. Any results we find for these products, should be available for other products within the EMIT and ECOSTRESS collections. \n",
    "\n",
    "To find data we will use the [`earthaccess` Python library](https://github.com/nsidc/earthaccess). `earthaccess` searches NASA's Common Metadata Repository (CMR), a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists of granule search result URLs.\n",
    "\n",
    "Using `earthaccess` we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (ex: Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection). We can search using attributes such as collection, acquisition time, and spatial footprint. This process can also be used with other EMIT or ECOSTRESS products, other collections, or different data providers, as well as across multiple catalogs with some modification. \n",
    "\n",
    "### 2.1 Define Spatial Region of Interest\n",
    "\n",
    "For this example, our spatial region of interest (ROI) will be the a region near Santa Barbara, CA that contains the [Jack and Laura Dangermond Preserve](https://www.dangermondpreserve.org/) and the [Sedgwick Reserve](https://sedgwick.nrs.ucsb.edu/). \n",
    "\n",
    "In this example, we will create a rectangular ROI surrounding these two reserves as well as some of the agricultural region between. Even though the shape is rectangular we elect to search using a polygon rather than a standard bounding box in `earthaccess` because bounding boxes will typically have a larger spatial extent, capturing a lot of area we may not be interested in. This becomes more important for searches with larger ROIs than our example here. To search for intersections with a polygon using earthaccess, we need to format our ROI as a counterclockwise list of coordinate pairs. \n",
    "\n",
    "Open the `geojson` file containing the Dangermond and Sedgwick boundaries as a `geodataframe`, and check the coordinate reference system (CRS) of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "polygon = gpd.read_file('../data/agu_workshop_roi.geojson')\n",
    "polygon.crs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CRS is **EPSG:4326** (WGS84), which is also the CRS we want the data in to submit for our search.\n",
    "\n",
    "Next, let's examine our polygon a bit closer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this `geodataframe` consists of two polygons, that we want to include in our study site. We need to create an exterior boundary polygon containing these, and make sure the vertices are in counterclockwise order to submit them in our query. To do this, create a polygon consisting of all the geometries, then create a bounding rectangle. This will give us a simple exterior polygon around our two ROIs. After that, use the `orient` function to place our coordinate pairs in counterclockwise order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Merge all Polygon geometries and create external boundary\n",
    "roi_poly = polygon.union_all().envelope\n",
    "# Re-order vertices to counterclockwise\n",
    "roi_poly = orient(roi_poly, sign=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a `GeoDataFrame` consisting of the bounding box geometry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\"Name\":[\"ROI Bounding Box\"]})\n",
    "agu_bbox = gpd.GeoDataFrame({\"Name\":[\"ROI Bounding Box\"], \"geometry\":[roi_poly]},crs=\"EPSG:4326\")\n",
    "agu_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can write this bounding box to a file for use in future notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# agu_bbox.to_file('../data/roi_bbox.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go ahead and visualize our region of interest and the exterior boundary polygon containing ROIs. First add a function to help reformat bounding box coordinates to work with leaflet notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to convert a bounding box for use in leaflet notation\n",
    "\n",
    "def convert_bounds(bbox, invert_y=False):\n",
    "    \"\"\"\n",
    "    Helper method for changing bounding box representation to leaflet notation\n",
    "\n",
    "    ``(lon1, lat1, lon2, lat2) -> ((lat1, lon1), (lat2, lon2))``\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    if invert_y:\n",
    "        y1, y2 = y2, y1\n",
    "    return ((y1, x1), (y2, x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = Figure(width=\"750px\", height=\"375px\")\n",
    "map1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\n",
    "fig.add_child(map1)\n",
    "\n",
    "# Add Bounding Box Polygon\n",
    "folium.GeoJson(agu_bbox,\n",
    "                name='bounding_box',\n",
    "                ).add_to(map1)\n",
    "\n",
    "# Add roi geodataframe\n",
    "polygon.explore(\n",
    "    \"Name\",\n",
    "    popup=True,\n",
    "    categorical=True,\n",
    "    cmap='Set3',\n",
    "    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n",
    "    name=\"Regions of Interest\",\n",
    "    m=map1\n",
    ")\n",
    "\n",
    "map1.add_child(folium.LayerControl())\n",
    "map1.fit_bounds(bounds=convert_bounds(polygon.union_all().bounds))\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see our regions of interest (ROIs) and the exterior boundary polygon containing the ROIs that we opened. We can hover over different areas to see the name of each ROI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to convert our polygon to a list of coordinate pairs, so it will be accepted as a 'polygon' search parameter in `earthaccess`, as it expects a list of coordinate pairs in a counterclockwise order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set ROI as list of exterior polygon vertices as coordinate pairs\n",
    "roi = list(roi_poly.exterior.coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Define Collections of Interest\n",
    "\n",
    "We need to specify which products we want to search for. The best way to do this is using their concept-id. As mentioned above, we will conduct our search using the EMIT Level 2A Reflectance (EMITL2ARFL) and ECOSTRESS Level 2 Tiled Land Surface Temperature and Emissivity (ECO_L2T_LSTE). We can do some quick collection queries using `earthaccess` to retrieve the concept-id for each dataset. \n",
    "\n",
    ">Note: Here we use the Tiled ECOSTRESS LSTE Product. This will also work with the gridded LSTE and the swath; however, the swath product does not have a browse image for the visualization in section 4 and will require additional processing for subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# EMIT Collection Query\n",
    "emit_collection_query = earthaccess.collection_query().keyword('EMIT L2A Reflectance')\n",
    "emit_collection_query.fields(['ShortName','EntryTitle','Version']).get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ECOSTRESS Collection Query\n",
    "eco_collection_query = earthaccess.collection_query().keyword('ECOSTRESS L2 Tiled LSTE')\n",
    "eco_collection_query.fields(['ShortName','EntryTitle','Version']).get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your search returns multiple products, be sure to select the right concept-id For this example it will be the first one. We want to use the `LPCLOUD` ECOSTRESS Tiled Land Surface Temperature and Emissivity (concept-id: \"C2076090826-LPCLOUD\"). Create a list of these concept-ids for our data search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data Collections for our search\n",
    "concept_ids = ['C2408750690-LPCLOUD', 'C2076090826-LPCLOUD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Define Date Range\n",
    "\n",
    "For our date range, we'll look at data collected between January and October 2023. The `date_range` can be specified as a pair of dates, start and end (up to, not including)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define Date Range\n",
    "date_range = ('2023-01-01','2023-11-01')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Searching\n",
    "\n",
    "Submit a query using `earthaccess`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = earthaccess.search_data(\n",
    "    concept_id=concept_ids,\n",
    "    polygon=roi,\n",
    "    temporal=date_range,\n",
    ")\n",
    "print(f\"Granules Found: {len(results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Organizing and Filtering Results\n",
    "\n",
    "As we can see from above, the results object contains a list of objects with metadata and links. We can convert this to a more readable format, a dataframe. In addition, we can make it a `GeoDataFrame` by taking the spatial metadata and creating a shapely polygon representing the spatial coverage, and further customize which information we want to use from other metadata fields.\n",
    "\n",
    "First, we define some functions to help us create a shapely object for our geodataframe, and retrieve the specific browse image URLs that we want. By default, the browse image selected by `earthaccess` is the first one in the list, but the ECO_L2_LSTE has several browse images, and we want to make sure we retrieve the `png` file, which is a preview of the LSTE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions to Build Dataframe\n",
    "def get_shapely_object(result: earthaccess.results.DataGranule):\n",
    "    \"\"\"\n",
    "    Create a shapely polygon of spatial coverage from results metadata.\n",
    "    Will work for BoundingRectangles or GPolygons.\n",
    "    \"\"\"\n",
    "    # Get Geometry Keys\n",
    "    geo = result[\"umm\"][\"SpatialExtent\"][\"HorizontalSpatialDomain\"][\"Geometry\"]\n",
    "    keys = geo.keys()\n",
    "\n",
    "    if \"BoundingRectangles\" in keys:\n",
    "        bounding_rectangle = geo[\"BoundingRectangles\"][0]\n",
    "        # Create bbox tuple\n",
    "        bbox_coords = (\n",
    "            bounding_rectangle[\"WestBoundingCoordinate\"],\n",
    "            bounding_rectangle[\"SouthBoundingCoordinate\"],\n",
    "            bounding_rectangle[\"EastBoundingCoordinate\"],\n",
    "            bounding_rectangle[\"NorthBoundingCoordinate\"],\n",
    "        )\n",
    "        # Create shapely geometry from bbox\n",
    "        shape = geometry.box(*bbox_coords, ccw=True)\n",
    "    elif \"GPolygons\" in keys:\n",
    "        points = geo[\"GPolygons\"][0][\"Boundary\"][\"Points\"]\n",
    "        # Create shapely geometry from polygons\n",
    "        shape = geometry.Polygon([[p[\"Longitude\"], p[\"Latitude\"]] for p in points])\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"Provided result does not contain bounding boxes/polygons or is incompatible.\"\n",
    "        )\n",
    "    return shape\n",
    "\n",
    "\n",
    "def get_png(result: earthaccess.results.DataGranule):\n",
    "    \"\"\"\n",
    "    Retrieve a png browse image if it exists or first jpg in list of urls\n",
    "    \"\"\"\n",
    "    https_links = [link for link in result.dataviz_links() if \"https\" in link]\n",
    "    if len(https_links) == 1:\n",
    "        browse = https_links[0]\n",
    "    elif len(https_links) == 0:\n",
    "        browse = \"no browse image\"\n",
    "        warnings.warn(f\"There is no browse imagery for {result['umm']['GranuleUR']}.\")\n",
    "    else:\n",
    "        browse = [png for png in https_links if \".png\" in png][0]\n",
    "    return browse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create an additional function using those to create a `GeoDataFrame` containing our results data, a geometry extracted from the metadata, the browse imagery, product shortname, and data links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_to_gdf(results: list):\n",
    "    \"\"\"\n",
    "    Takes a list of results from earthaccess and converts to a geodataframe.\n",
    "    \"\"\"\n",
    "    # Create Dataframe of Results Metadata\n",
    "    results_df = pd.json_normalize(results)\n",
    "    # Shorten column Names\n",
    "    results_df.columns = [\n",
    "        col.split(\".\")[-1] if \".\" in col else col for col in results_df.columns\n",
    "    ]\n",
    "    # Create shapely polygons for result\n",
    "    geometries = [\n",
    "        get_shapely_object(results[index]) for index in results_df.index.to_list()\n",
    "    ]\n",
    "    # Convert to GeoDataframe\n",
    "    gdf = gpd.GeoDataFrame(results_df, geometry=geometries, crs=\"EPSG:4326\")\n",
    "    # Add browse imagery and data links and product shortname\n",
    "    gdf[\"browse\"] = [get_png(granule) for granule in results]\n",
    "    gdf[\"shortname\"] = [\n",
    "        result[\"umm\"][\"CollectionReference\"][\"ShortName\"] for result in results\n",
    "    ]\n",
    "    gdf[\"data\"] = [granule.data_links() for granule in results]\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our function to create a `GeoDataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = results_to_gdf(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preview our geodataframe to get an idea what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of columns with data that is not relevant to our goal, so we can drop those. To do that, list the names of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List Column Names\n",
    "gdf.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a list of columns to keep and use it to filter the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a list of columns to keep\n",
    "keep_cols = [\n",
    "    \"native-id\",\n",
    "    \"collection-concept-id\",\n",
    "    \"BeginningDateTime\",\n",
    "    \"EndingDateTime\",\n",
    "    \"CloudCover\",\n",
    "    \"DayNightFlag\",\n",
    "    \"geometry\",\n",
    "    \"browse\",\n",
    "    \"shortname\",\n",
    "    \"data\",\n",
    "]\n",
    "# Remove unneeded columns\n",
    "gdf = gdf[gdf.columns.intersection(keep_cols)]\n",
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will separate the results into two dataframes, one for ECOTRESS and one for EMIT, and print the number of results for each so we can monitor how many granules we're filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split into two dataframes - ECO and EMIT\n",
    "eco_gdf = gdf[gdf['native-id'].str.contains('ECO')]\n",
    "emit_gdf = gdf[gdf['native-id'].str.contains('EMIT')]\n",
    "print(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some additional filtering steps that we can do here for both instruments. Since we're using the ECOSTRESS tiled data (ECO_L2T LSTE) data, there are two tiles available for our area of interest because its located where the tiles overlap and we'll want to select one. Another caveat with this ECOSTRESS dataset is that there are sometimes multiple builds available for a scene. We'll want to select the newest build available for each.\n",
    "\n",
    "First filter to a single tile, then remove duplicate scenes produced with older software builds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select an ECOSTRESS Tile\n",
    "eco_gdf = eco_gdf[eco_gdf[\"native-id\"].str.contains(\"10SGD\")]\n",
    "\n",
    "# Remove Duplicate Granules from Older Builds\n",
    "df = eco_gdf[\"native-id\"].str.rsplit(\"_\", n=2, expand=True)\n",
    "df.columns = [\"base\", \"build_major\", \"build_minor\"]\n",
    "df[\"build_major\"] = df[\"build_major\"].astype(int)\n",
    "df[\"build_minor\"] = df[\"build_minor\"].astype(int)\n",
    "df[\"native-id\"] = eco_gdf[\"native-id\"]\n",
    "idx = df.groupby(\"base\")[[\"build_major\", \"build_minor\"]].idxmax().build_major\n",
    "eco_gdf = eco_gdf[eco_gdf[\"native-id\"].isin(df.loc[idx, \"native-id\"])]\n",
    "\n",
    "# Show change in results quantity\n",
    "print(f\" ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this reduced our ECOSTRESS results quantity of by quite a bit.\n",
    "\n",
    "Now we can fileter some EMIT scenes as well, by utilizing the `CloudCover` field from the results metadata. This field is not available for ECOSTRESS. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cloud Filter using EMIT Metadata - Not available for ECOSTRESS\n",
    "emit_gdf = emit_gdf[emit_gdf[\"CloudCover\"] <= 60]\n",
    "print(f\" ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still haven't filtered the locations where EMIT and ECOSTRESS have data at the same spatial location and time. The EMIT acquisition mask has been added to ECOSTRESS, so in most cases if EMIT is collecting data, so will ECOSTRESS, but there are edge cases where this is not true. To find intersecting scenes we'll write a function that finds spatial and temporal intersections for each row between the EMIT and ECOSTRESS geodataframes. \n",
    "\n",
    "First write a function to convert the `BeginningDateTime` and `EndDateTime`.\n",
    "\n",
    "> **You may have noticed that the date format is similar for ECOSTRESS and EMIT, but the ECOSTRESS data also includes fractional seconds. If using an different version of `pandas`, you may need to drop the `format='ISO8601'`argument to the `to_datetime` function, as shown in the commented-out line.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to Convert Columns to datetime\n",
    "def convert_col_dt(df:pd.DataFrame, columns:list, fmt=\"ISO8601\"):\n",
    "    \"\"\"\n",
    "    Convert a column to a datetime. Default format of ISO8601.\n",
    "    df: a pd.DataFrame.\n",
    "    columns: a list of column names containing datetime strings earthaccess results.\n",
    "    fmt: the datetime format of the string.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df[col] = pd.to_datetime(df[col], format=fmt)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can write a function to find spatial and temporal interections of the data. To do this, we'll conduct a spatial join to find intersecting geometries from our `emit_gdf` and `eco_gdf`, then we'll use the time windows defined by `BeginningDateTime` and `EndingDateTime` to evaluate if granules (rows) from each dataframe intersect. Additionally, we want to provide a suffix for the left and right `GeoDataFrames` so columns will be renamed in a meaninful way.\n",
    "\n",
    "This works great for ECOSTRESS and EMIT because they are both on the ISS and acquiring data within seconds of eachother (this can vary due to FOV and other instrument parameters). \n",
    "\n",
    "For other applications we may want to find data from another orbiting instrument where an overpass falls within an hour or day of collection. We can accomplish this by manually providing a timedelta, then using the `BeginningDateTime` and `EndDateTime`, we can find if these times fall within the provided timedelta.\n",
    "\n",
    "Lastly, because we want to visualize each set of concurrent scenes, we'll want our function to keep the geometries from the left and right geodataframes. By default, the spatial join function only keeps the left. To do this, we'll add new columns using the provided suffixes to maintain both geometries, then instead of keeping just the left geometry, we will create a multipolygon for the main geometry of each row that consists of two polygons. This two polygon geometry can be used later for plotting.\n",
    "\n",
    "> **Note that the spatial join and temporal filtering can be computationally intensive. If the area of interest is very large or initial search parameters have a long temporal range, breaking down the initial search into sections spatially or temporally is recommended.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_concurrent_scenes(lgdf, rgdf, lsuffix, rsuffix, time_delta=None):\n",
    "    \"\"\"\n",
    "    Finds the concurrent scenes between two results converted geodataframes.\n",
    "    lgdf: a geodataframe converted from earthaccess results\n",
    "    rgdf: a second geodataframe converted from earthaccess results\n",
    "    lsuffix: appended to lgdf column names\n",
    "    rsuffix: appended to rgdf column names\n",
    "    timedelta (seconds): an optional time delta to define an acceptable window between acquistions\n",
    "                         useful for data acquired hours or days apart\n",
    "\n",
    "    \"\"\"\n",
    "    # Copy lgdf, rgdf Geometry\n",
    "    lgdf.loc[:,f\"geometry_{lsuffix}\"] = lgdf.geometry\n",
    "    rgdf.loc[:,f\"geometry_{rsuffix}\"] = rgdf.geometry\n",
    "\n",
    "    # Conduct Spatial Join\n",
    "    joined = gpd.sjoin(\n",
    "        lgdf,\n",
    "        rgdf,\n",
    "        how=\"inner\",\n",
    "        predicate=\"intersects\",\n",
    "        lsuffix=lsuffix,\n",
    "        rsuffix=rsuffix,\n",
    "    )\n",
    "\n",
    "    # Convert Datetime fields\n",
    "    date_columns = [\n",
    "        f\"BeginningDateTime_{lsuffix}\",\n",
    "        f\"BeginningDateTime_{rsuffix}\",\n",
    "        f\"EndingDateTime_{lsuffix}\",\n",
    "        f\"EndingDateTime_{rsuffix}\",\n",
    "    ]\n",
    "    joined = convert_col_dt(joined, date_columns)\n",
    "\n",
    "    # Filter Based on Collection Times\n",
    "    if time_delta:\n",
    "        td = timedelta(seconds=time_delta)\n",
    "        joined[f\"MidDateTime_{lsuffix}\"] = (\n",
    "            joined[f\"BeginningDateTime_{lsuffix}\"]\n",
    "            + (\n",
    "                joined[f\"EndingDateTime_{lsuffix}\"]\n",
    "                - joined[f\"BeginningDateTime_{lsuffix}\"]\n",
    "            )\n",
    "            / 2\n",
    "        )\n",
    "        joined[f\"MidDateTime_{rsuffix}\"] = (\n",
    "            joined[f\"BeginningDateTime_{rsuffix}\"]\n",
    "            + (\n",
    "                joined[f\"EndingDateTime_{rsuffix}\"]\n",
    "                - joined[f\"BeginningDateTime_{rsuffix}\"]\n",
    "            )\n",
    "            / 2\n",
    "        )\n",
    "        concurrent_df = joined[\n",
    "            abs(joined[f\"MidDateTime_{lsuffix}\"] - joined[f\"MidDateTime_{rsuffix}\"])\n",
    "            <= td\n",
    "        ]\n",
    "        print(joined[f\"MidDateTime_{lsuffix}\"])\n",
    "    else:\n",
    "        concurrent_df = joined[\n",
    "            (\n",
    "                joined[f\"BeginningDateTime_{lsuffix}\"]\n",
    "                <= joined[f\"EndingDateTime_{rsuffix}\"]\n",
    "            )\n",
    "            & (\n",
    "                joined[f\"EndingDateTime_{lsuffix}\"]\n",
    "                >= joined[f\"BeginningDateTime_{rsuffix}\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    # Combine Geometry\n",
    "    concurrent_df.loc[:, \"geometry\"] = concurrent_df.apply(\n",
    "        lambda row: MultiPolygon(\n",
    "            [row[f\"geometry_{lsuffix}\"], row[f\"geometry_{rsuffix}\"]]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    return concurrent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run our function. Since we're working with ECOSTRESS and EMIT, we don't need to provide the `timedelta` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find Concurrent Scenes\n",
    "concurrent_df = find_concurrent_scenes(\n",
    "    emit_gdf, eco_gdf, lsuffix=\"EMIT\", rsuffix=\"ECO\"\n",
    ")\n",
    "concurrent_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When viewing our concurrent dataframe, we can see that all of the info from both our left (EMIT) and right (ECO) geodataframes is still there, including geomtries from each, and our new multipolygon geometry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Visualizing Intersecting Coverage\n",
    "\n",
    "Now that we have a geodataframe containing concurrent data within our intial search parameters, we can visualize them on a map using `folium`. To improve our visual, we'll first create a function to help style each polygon and set up a colormap for each row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to style polygons by row\n",
    "def style_by_row(row, cmap, ckey, **kwargs):\n",
    "    \"\"\"\n",
    "    Applies style by row within interrows.\n",
    "\n",
    "    row: A row (series) from a GeoDataFrame.\n",
    "    cmap:\n",
    "    ckey:\n",
    "    kwargs: additional style options for style dict.\n",
    "    \"\"\"\n",
    "    color = cmap.get(row[ckey], \"#000000\") if cmap else \"#000000\"\n",
    "\n",
    "    return {\"color\": color, **kwargs}\n",
    "\n",
    "# Creat a colormap for the data\n",
    "palette = plt.get_cmap(\"tab20\").colors\n",
    "ckeys = concurrent_df[\"native-id_EMIT\"].unique()\n",
    "cmap = {ckey: mcolors.to_hex(palette[i % len(palette)]) for i, ckey in enumerate(ckeys)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot using `folium`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Figure and Select Background Tiles\n",
    "fig = Figure(width=\"750\", height=\"375\")\n",
    "map1 = folium.Map(\n",
    "    tiles=\"https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}\", attr=\"Google\"\n",
    ")\n",
    "fig.add_child(map1)\n",
    "\n",
    "# Iterate through GeoDataFrame row\n",
    "for idx, row in concurrent_df.iterrows():\n",
    "\n",
    "    # Set up feature group to pair browse image with multipolygon feature\n",
    "    fg = folium.FeatureGroup(\n",
    "        name=f\"{row['native-id_EMIT']}, {row['native-id_ECO']}\", show=False\n",
    "    )\n",
    "\n",
    "    # Create Tooltip\n",
    "    tooltip = folium.Tooltip(f\"\"\"\n",
    "        <b> EMIT Granule-ID: </b> {row[\"native-id_EMIT\"]}<br>\n",
    "        <b> ECOSTRESS-ID: </b> {row[\"native-id_ECO\"]}<br>\n",
    "        <b> EMIT BeginningDateTime: </b> {row[\"BeginningDateTime_EMIT\"]}<br>\n",
    "        <b> ECO BeginningDateTime: </b> {row[\"BeginningDateTime_ECO\"]}<br>\n",
    "        <b> EMIT CloudCover: </b> {row[\"CloudCover_EMIT\"]}<br>\n",
    "        <b> ECO CloudCover: </b> {row[\"CloudCover_ECO\"]}<br>\n",
    "        \"\"\")\n",
    "\n",
    "    # Add each row as a geoJson layer\n",
    "    folium.GeoJson(\n",
    "        data=row[\"geometry\"].__geo_interface__,\n",
    "        tooltip=tooltip,\n",
    "        style_function=lambda feature, row=row: style_by_row(\n",
    "            row=row, cmap=cmap, ckey=\"native-id_EMIT\", fillOpacity=0, weight=2\n",
    "        ),\n",
    "    ).add_to(fg)\n",
    "\n",
    "    # Add the ECO browse Image\n",
    "    if row[\"browse_ECO\"] != \"no browse image\":\n",
    "        folium.raster_layers.ImageOverlay(\n",
    "            image=row[\"browse_ECO\"],\n",
    "            # geometry bounds: minx, miny, maxx, maxy. Change to miny, minx, maxy, maxx for folium\n",
    "            bounds=[\n",
    "                [row[\"geometry_ECO\"].bounds[1], row[\"geometry_ECO\"].bounds[0]],\n",
    "                [row[\"geometry_ECO\"].bounds[3], row[\"geometry_ECO\"].bounds[2]],\n",
    "            ],\n",
    "            interactive=False,\n",
    "            cross_origin=False,\n",
    "            opacity=0.75,\n",
    "            zindex=1,\n",
    "        ).add_to(fg)\n",
    "\n",
    "    # Add the FeatureGroup to the map\n",
    "    map1.add_child(fg)\n",
    "\n",
    "# Plot Region of Interest\n",
    "polygon.explore(\n",
    "    popup=False,\n",
    "    style_kwds=dict(color=\"#FFFF00\", fillOpacity=0, width=2),\n",
    "    name=\"Region of Interest\",\n",
    "    m=map1,\n",
    ")\n",
    "# Add Our Bounding Box\n",
    "folium.GeoJson(\n",
    "    roi_poly,\n",
    "    name=\"bounding_box\",\n",
    "    popup=False,\n",
    "    style_kwds=dict(color=\"#FFFF00\", fillOpacity=0, width=2),\n",
    ").add_to(map1)\n",
    "\n",
    "# Add Layer Control and set zoom.\n",
    "map1.fit_bounds(bounds=convert_bounds(concurrent_df.union_all().bounds))\n",
    "map1.add_child(folium.LayerControl())\n",
    "display(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, we start without loading any concurrent layers. Add or remove layers using the layer control in the top right. Each layer will contain an EMIT scene footprint, concurrent ECOSTRESS scene footprint, and ECOSTRESS browse scene. Note that there can be duplicates if multiple scenes intersect with a single scene."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Generating a list of URLs\n",
    "\n",
    "With our concurrent geodataframe we can pull the links to scenes out of our `data_EMIT` and `data_ECO` column. To do this, define a function that groups the data by the `native-id_EMIT` and builds a dictionary of links corresponding to each. We use a sublist to help remove duplicates and lump multiple corresponding scenes to a single `native-id_EMIT` scene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function to retrieve list of files intersecting with emit scene\n",
    "def get_file_list(concurrent_df):\n",
    "    \"\"\"\n",
    "    Retrieve a dictionary of EMIT scenes with a list of files from both instruments\n",
    "    acquired at the location and time of lgdf scene.\n",
    "    \"\"\"\n",
    "\n",
    "    files = (\n",
    "        concurrent_df.groupby(\"native-id_EMIT\")[[\"data_EMIT\", \"data_ECO\"]]\n",
    "        .apply(\n",
    "            lambda group: list(\n",
    "                dict.fromkeys(\n",
    "                    [item for sublist in group[\"data_EMIT\"] for item in sublist]\n",
    "                    + [item for sublist in group[\"data_ECO\"] for item in sublist]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "        .to_dict()\n",
    "    )\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the function to retrieve the dictionary of links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retrieve Links\n",
    "links_dict = get_file_list(concurrent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List EMIT Scenes (Keys)\n",
    "list(links_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show links associated with Scene\n",
    "links_dict['EMIT_L2A_RFL_001_20230219T202939_2305013_002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Granules often have several assets associated with them, for example, `ECO_L2T_LSTE` has several assets:\n",
    " - Water Mask (water)\n",
    " - Cloud Mask (cloud)\n",
    " - Quality (QC)\n",
    " - Land Surface Temperature (LST)\n",
    " - Land Surface Temperature Error (LST_err)\n",
    " - Wide Band Emissivity (EmisWB)\n",
    " - Height (height)\n",
    "and `EMIT_L2A_RFL` has:\n",
    " - Reflectance\n",
    " - Reflectance Uncertainty\n",
    " - Masks\n",
    "\n",
    "We can use string matching to iterate through our dictionary and remove assets we don't want. Carefully choose some strings that can be used to identify desired assets, then use list comprehension to remove assets that don't contain the strings from the desired assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify Strings\n",
    "desired_assets = ['_RFL_', '_MASK_', '_LST.']\n",
    "\n",
    "# Iterate through dictionary and filter\n",
    "filtered_links_dict = {\n",
    "    key: [asset for asset in assets if any(desired_asset in asset.split('/')[-1] for desired_asset in desired_assets)]\n",
    "    for key, assets in links_dict.items()\n",
    "}\n",
    "list(filtered_links_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_links_dict['EMIT_L2A_RFL_001_20230219T202939_2305013_002']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've filterd our results down to just the assets we want, we can write this dictionary out to a .json file to use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/search_results.json', 'w') as f:\n",
    "    json.dump(filtered_links_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Streaming or Downloading Data  \n",
    "\n",
    "For the workshop, we will stream the data, but either method can be used, and each has trade-offs based on the internet speed, storage space, or use case. The EMIT files are very large due to the number of bands, so operations can take some time if streaming with a slower internet connection. Since the workshop is hosted in a Cloud workspace, we can stream the data directly to the workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Streaming Data Workflow\n",
    "\n",
    "For an example of streaming both **netCDF** and **Cloud Optimized GeoTIFF (COG) data** please see notebook 2, [Working With EMIT Reflectance and ECOSTRESS LST](02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.ipynb).\n",
    "\n",
    "If you plan to stream the data, you can stop here and move to the next notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Downloading Data Workflow\n",
    "\n",
    "To download the scenes, we can use the `earthaccess` library to authenticate then download the files.\n",
    "\n",
    "First, log into Earthdata using the `login` function from the `earthaccess` library. The `persist=True` argument will create a local `.netrc` file if it doesn't exist, or add your login info to an existing `.netrc` file. If no Earthdata Login credentials are found in the `.netrc` you'll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\n",
    "\n",
    "We've included the canopy water content files in the repository to simplify the notebooks so users don't need to perform that operation for the examples in the repository. This means that only 3 granules from our list are required to execute the notebooks and walk through the notebooks in the repository. These are included in a separate text file, `required_granules.txt`. \n",
    "\n",
    "These can be downloading by uncommenting and running the following cells. \n",
    "\n",
    ">Note: If interested users can download all of the files using the cell below and recreate all of the canopy water content files following a workflow similar to the example in notebooks 2 and 3 for all of the necessary scenes. To do this, uncomment the `file_list` object with the `search_results.txt` filepath to download all of the results rather than just what is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "metadata": {},
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Authenticate using earthaccess\n",
    "earthaccess.login(persist=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the required granules. The function below writes all assets to a directory named using the `links_dict` keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get requests https Session using Earthdata Login Info\n",
    "fs = earthaccess.get_requests_https_session()\n",
    "# Retrieve granule asset ID from URL (to maintain existing naming convention)\n",
    "for granule, assets in links_dict.items():\n",
    "    out_dir = f\"../data/{granule}\"\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    for asset in assets:\n",
    "        out_fn = f\"{out_dir}{os.sep}{asset.split('/'[-1])}\"\n",
    "        if not os.path.isfile(out_fn):\n",
    "            with fs.get(asset,stream=True) as src:\n",
    "                with open(out_fn,'wb') as dst:\n",
    "                    for chunk in src.iter_content(chunk_size=64*1024*1024):\n",
    "                        dst.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, now you have downloaded concurrent data from the ECOSTRESS and EMIT instruments on the ISS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Contact Info:  \n",
    "\n",
    "Email: LPDAAC@usgs.gov  \n",
    "Voice: +1-866-573-3222  \n",
    "Organization: Land Processes Distributed Active Archive Center (LP DAAC)¹  \n",
    "Website: <https://lpdaac.usgs.gov/>  \n",
    "\n",
    "¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lpdaac_vitals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
