[
  {
    "objectID": "vitals.html",
    "href": "vitals.html",
    "title": "VITALS",
    "section": "",
    "text": "Please view the SBG Workshop Page for workshop details. Resources in the VITALS repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace. For local python environment setup instructions please see the Setup Instructions.\nWelcome to the VSWIR Imaging and Thermal Applications, Learning, and Science Repository! This repository provides Python Jupyter notebooks to help users work with visible to short-wave infrared imaging spectroscopy data, thermal infrared data, and other products from the Earth Surface Mineral Dust Source Investigation (EMIT) and ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) missions. The instruments overlapping fields of view provide an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets.\nIn the interest of open science this repository has been made public but is still under active development. Make sure to consult the change log for the most recent changes to the repository. Contributions from all parties are welcome.",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "vitals.html#contact-info",
    "href": "vitals.html#contact-info",
    "title": "VITALS",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Welcome",
      "Repository Description"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html",
    "href": "setup/setup_instructions.html",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "The how-tos and tutorials in this repository require a NASA Earthdata account, an installation of Git, and a compatible Python Environment. Resources in the VITALS repository have been developed using the Openscapes 2i2c JupyterHub cloud workspace.\nFor local Python environment setup we recommend using mamba to manage Python packages. To install mamba, download miniforge for your operating system. If using Windows, be sure to check the box to “Add mamba to my PATH environment variable” to enable use of mamba directly from your command line interface. Note that this may cause an issue if you have an existing mamba install through Anaconda.\n\n\nThese Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate lpdaac_vitals \nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.\n\n\n\nIf you plan to edit or contribute to the VITALS repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone or download the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/nasa/VITALS.git\n\n\n\n\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-21-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html#python-environment-setup",
    "href": "setup/setup_instructions.html#python-environment-setup",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "These Python Environments will work for all of the guides, how-to’s, and tutorials within this repository.\n\nUsing your preferred command line interface (command prompt, terminal, cmder, etc.) navigate to your local copy of the repository, then type the following to create a compatible Python environment.\nFor Windows:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 fiona=1.8.22 gdal hvplot geoviews rioxarray rasterio jupyter geopandas earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image jupyterlab seaborn dask ray-default\nFor MacOSX:\nmamba create -n lpdaac_vitals -c conda-forge --yes python=3.10 gdal=3.7.2 hvplot geoviews rioxarray rasterio geopandas fiona=1.9.4 jupyter earthaccess jupyter_bokeh h5py h5netcdf spectral scikit-image seaborn jupyterlab dask ray-default ray-dashboard\nNext, activate the Python Environment that you just created.\nmamba activate lpdaac_vitals \nNow you can launch Jupyter Notebook to open the notebooks included.\njupyter notebook \n\nStill having trouble getting a compatible Python environment set up? Contact LP DAAC User Services.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html#cloning-the-vitals-repository",
    "href": "setup/setup_instructions.html#cloning-the-vitals-repository",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "If you plan to edit or contribute to the VITALS repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone or download the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/nasa/VITALS.git",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "setup/setup_instructions.html#contact-info",
    "href": "setup/setup_instructions.html#contact-info",
    "title": "Repository Setup Instructions",
    "section": "",
    "text": "Email: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-21-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Setup Instructions",
      "Local Python Environment Setup"
    ]
  },
  {
    "objectID": "python/env_test.html",
    "href": "python/env_test.html",
    "title": "Python Environment tests",
    "section": "",
    "text": "# Basic Python\nimport os\nimport math\nimport numpy as np\n# Opening Geospatial Data\nimport geopandas as gpd\nimport rasterio as rio\nimport rioxarray as rxr\n# Plotting\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv \n# Other\nimport earthaccess\nfrom modules.emit_tools import emit_xarray\nfrom modules.ewt_calc import calc_ewt\n# Hard Coded File urls\nemit_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203751_2309114_002/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc'\necostress_url = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif'\nroi = gpd.read_file(\"../data/dangermond_boundary.geojson\")\n# Sign into EDL and get token\ntoken = earthaccess.login(persist=True).token['access_token']\n# Download EMIT Scene\n# Get requests https Session using Earthdata Login Info\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\ngranule_asset_id = emit_url.split('/')[-1]\n# Define Local Filepath\nfp = f'../data/{granule_asset_id}'\n# Download the Granule Asset if it doesn't exist\nif not os.path.isfile(fp):\n    with fs.get(emit_url,stream=True) as src:\n        with open(fp,'wb') as dst:\n            for chunk in src.iter_content(chunk_size=64*1024*1024):\n                dst.write(chunk)"
  },
  {
    "objectID": "python/env_test.html#test-emit-tools-reading-and-ortho",
    "href": "python/env_test.html#test-emit-tools-reading-and-ortho",
    "title": "Python Environment tests",
    "section": "Test EMIT tools reading and ortho",
    "text": "Test EMIT tools reading and ortho\n\n# Open EMIT and Orthorectify\nemit_ds = emit_xarray(fp, ortho=True)"
  },
  {
    "objectID": "python/env_test.html#geoviews-tiles-image-and-polygon-plotting",
    "href": "python/env_test.html#geoviews-tiles-image-and-polygon-plotting",
    "title": "Python Environment tests",
    "section": "Geoviews Tiles, Image, and Polygon Plotting",
    "text": "Geoviews Tiles, Image, and Polygon Plotting\n\n# Geoviews + Polygon Visualizations\nemit_ds['reflectance'].data[emit_ds['reflectance'].data == -9999] = np.nan\nsize_opts = dict(frame_height=405, frame_width=720, fontscale=2)\nmap_opts = dict(geo=True, crs='EPSG:4326', alpha=0.7, xlabel='Longitude',ylabel='Latitude')\nemit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',tiles='ESRI',**size_opts, **map_opts)*roi.hvplot(color='#d95f02',alpha=0.5, crs='EPSG:4326')\n\n\n\n\n\n  \n\n\n\n\n\n# Crop and Save\nemit_cropped = emit_ds.rio.clip(roi.geometry.values,roi.crs, all_touched=True)\nemit_cropped.to_netcdf(f'../data/{emit_cropped.granule_id}_dangermond.nc')"
  },
  {
    "objectID": "python/env_test.html#calculate-ewt-using-ray",
    "href": "python/env_test.html#calculate-ewt-using-ray",
    "title": "Python Environment tests",
    "section": "Calculate EWT Using ray",
    "text": "Calculate EWT Using ray\nThe figure below should have a region with much higher ewt in the bottom left corner of the ROI. This likely indicates presence of iceplant (holds a lot of water).\n\n%%time\n# Calculate EWT\ncropped_fp = '../data/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond.nc'\nds_cwc = calc_ewt(cropped_fp, '../data/', ewt_detection_limit=1.5, return_cwc=True)\n\n2024-10-28 13:45:26,994 INFO worker.py:1816 -- Started a local Ray instance.\nc:\\Users\\ebolch\\repos\\VITALS\\python\\modules\\ewt_calc.py:117: FutureWarning: The return type of `Dataset.dims` will be changed to return a set of dimension names in future, in order to be more consistent with `DataArray.dims`. To access a mapping from dimension names to lengths, please use `Dataset.sizes`.\n  data_vars = {\"cwc\": (list(emit_ds.dims.keys())[0:2], cwc.squeeze())}\n\n\n\n# Visualize EWT\nds_cwc.hvplot.image(cmap='viridis',tiles='ESRI',**size_opts, **map_opts)"
  },
  {
    "objectID": "python/env_test.html#test-interactive-plotting",
    "href": "python/env_test.html#test-interactive-plotting",
    "title": "Python Environment tests",
    "section": "Test Interactive Plotting",
    "text": "Test Interactive Plotting\nBe sure to check a few points and ensure the spectral figure works.\n\n# Mask out Bad Bands\nemit_ds['reflectance'].data[:,:,emit_ds['good_wavelengths'].data==0] = np.nan\n\n# Select RGB Bands\nemit_rgb = emit_ds.sel(wavelengths=[650, 560, 470], method='nearest')\n\n# Define and Apply Gamma Adjustment\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Assign NA's to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n\nemit_rgb = gamma_adjust(emit_rgb,white_background=True)\n\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = emit_rgb.hvplot.rgb(fontscale=1.5, xlabel='Longitude',ylabel='Latitude',frame_width=480, frame_height=480)\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = emit_ds.longitude.values[int(len(emit_ds.longitude) / 2)]\nymid = emit_ds.latitude.values[int(len(emit_ds.latitude) / 2)]\n\nfirst_point = ([xmid], [ymid], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n                                                                            color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)\n\nC:\\Users\\ebolch\\AppData\\Local\\Temp\\1\\ipykernel_12556\\166652677.py:8: RuntimeWarning: invalid value encountered in power\n  scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range"
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html",
    "href": "python/04_Dangermond_Land_Cover.html",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "",
    "text": "Authors\nChristiana Ade1 and Marie Johnson1,2\n1. Jet Propulsion Laboratory, California Institute of Technology\n2. University of Montana\nSummary\nIn this notebook, we will examine how Canopy Water Content (CWC) derived from EMIT data and Land Surface Temperature (LST) derived from ECOSTRESS change across three different vegetation types and dates in the Jack and Laura Dangermond preserve.\nLearning Objectives\n- Use product time series related to a real life scenario concerning invasive species in nature preserves.\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only Set up Python Environment - See setup_instructions.md in the /setup/ folder to set up a local compatible Python environment\n- Downloaded necessary files. This is done at the end of the 01_Finding_Concurrent_Data notebook.\nBackground\nIn 2017, The Nature Conservancy (TNC) acquired the preserve through a generous donation from Jack and Laura Dangermond. The preserve is 24,460 acres of a former private ranch situated at Point Conception, California. Point conception represents the boundary between Northern and Southern California, which separates terrestrial, marine, and coastal ecoregions. This is one of only a few areas globally where the boundary between marine and terrestrial ecosystems exist together. The preserve is one of the last “wild coast” areas in Southern California and it has some of the highest biodiversity globally. The Dangermond Preserve offers a unique opportunity to study the impacts of global change; sea level rise, the intensification of wildfire and drought providing a natural laboratory.\nThe preserve is naturally composed of oak tree forests and native grasslands; however, in recent years it faces several invasion issues from non-native grasses and iceplant (Carpobrotus spp.). The currently mapped invasive plant species at the Preserve include non-native grasslands, iceplant mats, and stands of black mustard (Brassica nigra), poison hemlock (Conium maculatum), thistles, and fennel (Foeniculum vulgare). Iceplant is one of the priority species listed under the preserve’s invasive species management plan.\nIceplant is an invasive species native to South Africa that was introduced to California in the 1500s. Because of its ability to stabilize soil in the 1950’s, the Department of Transportation used iceplant to prevent erosion on roadsides. This practice lasted for 20 years before it was discontinued, however, many homeowners still use iceplant for landscaping today. Although iceplant can help prevent coastal erosion, it threatens coastal biodiversity. Iceplant takes up a large amount of soil moisture, thereby taking moisture away from other species. It can also inhibit the establishment of native grass species as a result of a high nitrate accumulation in iceplant soils. Current management efforts to eradicate iceplant at the Dangermond Preserve include cattle grazing. TNC is interested in utilizing enhanced remote sensing techniques to understand iceplant characteristics and detect iceplant locations.\nExercise information\nThe cropped EMIT and ECOSTRESS imagery used in this notebook were created using code from previous tutorial notebooks. We have simply cropped this imagery to the Dangermond boundaries and saved them to the data directory. As a reference, the tutorial code needed crop EMIT and ECOSTRESS products is in 02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.ipynb in sections 2.2.3 Cropping EMIT data to a Region of Interest, 2.2.4 Write an output, 2.3.2 Cropping ECOSTRESS Data, and 2.3.3 Writing Outputs.\nTutorial references\n01_Finding_Concurrent_Data.ipynb - In this notebook we learned how to use earthaccess to find concurrent EMIT and ECOSTRESS data - We also learned how to export a list of files and download them programmatically.\n02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.ipynb - How to open and work with EMIT L2A Reflectance and ECOSTRESS L2T LSTE data - How to apply a quality mask to EMIT datasets - How to reproject and regrid data - How to crop EMIT and ECOSTRESS data\n03_EMIT_CWC_from_Reflectance.ipynb - Calculate the EWT of a single pixel - Calculate the EWT of a ROI\nRequired datasets\n1. Image datasets: This imagery has already been processed and uploaded to the data directory using code from previous notebooks. We will use three dates in 2023 to represent different seasons: April 1st, June 29th, and September 23rd.\nTutorial Outline\n4.1 View the CWC and LST images along with the vegetation class polygons\n4.2 Extract the CWC Raster Values\n4.3 Make a boxplot of CWC Extracted Values\n4.4 Extract and create box plot of LST images\n4.5 Investigate and discuss the differences between vegetation classes and dates for both CWC and LST\n4.6 Interactive playground! - Create your own polygons for extraction\nReferences * Bossard, C. C., Randall, J. M., & Hoshovsky, M. C. (Eds.). (2000). Invasive Plants of California’s Wildlands. University of California Press. * Butterfield, H.S., M. Reynolds, M.G. Gleason, M. Merrifield, B.S. Cohen, W.N. Heady, D. Cameron, T. Rick, E. Inlander, M. Katkowski, L. Riege, J. Knapp, S. Gennet, G. Gorga, K. Lin, K. Easterday, B. Leahy and M. Bell. 2019. Jack and Laura Dangermond Preserve Integrated Resources Management Plan. The Nature Conservancy. 112 pages.\n* Iceplant images https://centralcoastparks.org/ice-plant-the-iconic-but-destructive-piece-of-california-coastal-landscape/ https://sanctuaries.noaa.gov/news/feb15/invasive-species.html",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html#setup",
    "href": "python/04_Dangermond_Land_Cover.html#setup",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "Setup",
    "text": "Setup\n\n# Import required packages \nimport re\nimport os\nimport glob\nimport warnings\n\n# Some cells may generate warnings that we can ignore. Comment below lines to see.\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nfrom matplotlib import pyplot as plt\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv\nimport geopandas as gp\nimport panel as pn\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html#view-the-vegetation-polygons-and-the-emit-cwc-products",
    "href": "python/04_Dangermond_Land_Cover.html#view-the-vegetation-polygons-and-the-emit-cwc-products",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "4.1 View the vegetation polygons and the EMIT CWC products",
    "text": "4.1 View the vegetation polygons and the EMIT CWC products\nLet’s make a list called cwc_list of all the previously cropped EMIT CWC images by searching for tifs in our image directory cropped/dangermond/ewt/.\n\n# data directory - location of all images cropped to dangermond. Includes subfolders for ewt and lst. \ndata_dir = \"../data/\"\n\n# list all files in the ewt sub-directory of the data_dir path that end in tif\ncwc_list = glob.glob(os.path.join(data_dir, \"cwc\", \"*.tif\"))\ncwc_list\n\n['../data/cwc/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230219T202939_2305013_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230422T195924_2311213_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230405T190311_2309513_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20231014T224006_2328715_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230219T202951_2305013_003_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230219T202939_2305013_002_dangermond_cwc_merged.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif']\n\n\ncwc_list is a list of nine previously processed EMIT scenes over Dangermond. However, several of those scenes do not capture the full preserve. In these first parts of the notebook, we fill focus on three dates by making a filtered list of the cwc files called fil_cwc_list. We will use these dates to subset - Spring: 2023-04-01 - Summer: 2023-06-29 - Fall: 2023-09-23\n\n# the date and time codes strings we want to select from our larger cwc_list\ncwc_dates = ['20230401T203751', '20230629T170449', '20230923T232101']\n\n# Written as a loop so the files appear chronologically \nfil_cwc_list = [] # empty list\n\nfor date in cwc_dates:\n    # filter for appropriate dates and then add back\n    fil_cwc_list.extend([file for file in cwc_list if date in file])\nfil_cwc_list\n\n['../data/cwc/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif']\n\n\n\nLet’s visualize the three CWC scenes\nNext we will write a loop to visualize the change in CWC through time.\n\n# Initialize an empty list to store the plots\nplots = []\n\n# Iterate over each CWC file and create a plot\nfor file in fil_cwc_list:\n    # Open the raster file\n    ras = rxr.open_rasterio(file).squeeze('band', drop=True)\n    \n    # extract the date string from the file file name\n    ras_date =  re.search(r\"\\d{8}\", file).group()\n    date_object = datetime.strptime(ras_date, \"%Y%m%d\")\n    date_only = date_object.date()#.strftime(\"%Y-%m-%d\")\n    \n    # Create a plot using geoviews\n    plot = ras.hvplot.image(x='x', y='y', geo=True, cmap='blues',\n                                                    tiles='ESRI', \n                                                    title=f\"{date_only} {ras.long_name} ({ras.units})\",\n                                                    xlabel='Longitude', ylabel='Latitude',\n                                                    frame_width=400, frame_height= 300,\n                                                    fontscale=1, alpha=0.7)\n    \n    # Add the plot to the list\n    plots.append(plot)\n\n# Display all plots in a grid layout using Panel\ngrid = pn.GridSpec(sizing_mode='stretch_both')\nfor i, plot in enumerate(plots):\n    grid[i // 2, i % 2] = plot  # Adjust 3 to change the number of columns\n\ngrid.servable()\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\nThe southwest corner of Dangermond seems to have a much higher CWC than other regions. From Figure 2, we can see that this is likely a very large iceplant patch. What other patterns do you see?\n\n\n\nLet’s Load the Vegetation Polygons\nHere we will load previously delineated vegetation plots of iceplant, coastal live oak and non-native grasslands from the veg_plots_outline.geojson as the variable veg_poly. These plots come from a combination of field data and previous maps created for Dangermond preserve management.\n\n\n\nFigure 3. Examples of the different vegetation types in the veg_plots_outline.geojson.\n\n\n\n# Read in veg polygons\nveg_poly = gp.read_file(\"../data/veg_plots_outline.geojson\")\n\n\n# Let's load in the September image for the plot visualization \ncw3 = rxr.open_rasterio(fil_cwc_list[2]).squeeze('band',drop=True)\n\nLet’s visualize this polygon dataset on top of the September CWC scene. On the left side, plot_1 shows the vegetation plots in red overlayed on the CWC scene. On the right side plot_2 has the polygons colored by vegetation class.\n\n# Plot 1 with CWC September for reference \nplot_1 = cw3.hvplot.image(x='x',y='y',geo=True, cmap='blues',tiles='ESRI',\n                          title=f\"{cw3.long_name} ({cw3.units})\",\n                          xlabel='Longitude',ylabel='Latitude', frame_width=300,\n                          frame_height=300, fontscale=1, alpha = 0.7) * veg_poly.to_crs('EPSG:3857').hvplot(c='red',alpha=1)\n\n\n# Plot 2 with vegetation plot types varying by color\nplot_2 = veg_poly.hvplot.polygons(geo=True, c='Class', hover_cols='all', \n                                         xlabel='Longitude',ylabel='Latitude', frame_width=300,\n                                         frame_height=300, fontscale=1, title='Dangermond Vegetation Plots')\n\nplot_1 + plot_2\n\n\n\n\n\n  \n\n\n\n\nTo zoom in click the box or wheel zoom icon.\n\n\n4.2 Extract CWC Pixel Values from Vegetation Polygons\nIn this section, we will use a previously written extract function called extract_raster_values from the accompanying script jlpd_ras_funcs.py. This script includes all the more advanced functions written for this notebook which is why we are using import *.. Several of the other will be referenced later in the interactive portion of the notebook.\n\n# This is one way to load functions written from another script\nfrom modules.jldp_ras_funcs import *\n\nHere we will create a loop to extract the raster values from every pixel in each polygon of the veg_plots_outline.shp file for the spring, summer, and fall cwc_dates. The final output dataframe file will be called subset_df.\n\n# Create an empty list to store extracted values in\nex_df = [] \n\n# Loop through each raster in fil_cwc_list\nfor r in fil_cwc_list:\n    # Open the raster \n    ras = rxr.open_rasterio(r).squeeze('band', drop=True)\n        \n    # Extract raster values\n    ex_ras = extract_raster_values(ras, veg_poly)\n    \n    # Extract the image date from the raster itself and format correctly\n    date_object = datetime.strptime(ras.attrs['time_coverage_end'], \"%Y-%m-%dT%H:%M:%S%z\")\n    date_only = date_object.date()\n    \n    # Add date to new column in data frame\n    ex_ras['rasDate'] = date_only\n    \n    # Append to list\n    ex_df.append(ex_ras)\n    \n# concatenate  all list entries into a dataframe\nex_cwc = pd.concat(ex_df).reset_index(drop=True)\n # Rename column value to CWC\nex_cwc.rename(columns={'value': 'CWC'}, inplace=True)\n# Replace -9999 with NA in the CWC column\nex_cwc['CWC'].replace(-9999, np.nan, inplace=True) \n# Remove rows with NA values in the CWC column\nex_cwc = ex_cwc.dropna(subset=['CWC']) \n\n\n# Preview data\nex_cwc.head()\n\n\n\n\n\n\n\n\nx\ny\nspatial_ref\nCWC\ncell_number\nid\nClass\ngeometry\nrasDate\n\n\n\n\n0\n-120.461840\n34.452859\n0\n0.887800\n34.45285946671181_-120.46183955389516\n0\nice_plant\nMULTIPOLYGON (((-120.46214771722677 34.4533973...\n2023-04-01\n\n\n1\n-120.461840\n34.452317\n0\n0.887800\n34.45231723419155_-120.46183955389516\n0\nice_plant\nMULTIPOLYGON (((-120.46214771722677 34.4533973...\n2023-04-01\n\n\n2\n-120.461840\n34.451775\n0\n0.706839\n34.451775001671294_-120.46183955389516\n0\nice_plant\nMULTIPOLYGON (((-120.46214771722677 34.4533973...\n2023-04-01\n\n\n3\n-120.461840\n34.451233\n0\n0.732946\n34.451232769151034_-120.46183955389516\n0\nice_plant\nMULTIPOLYGON (((-120.46214771722677 34.4533973...\n2023-04-01\n\n\n4\n-120.461297\n34.452859\n0\n1.071603\n34.45285946671181_-120.4612973213749\n0\nice_plant\nMULTIPOLYGON (((-120.46214771722677 34.4533973...\n2023-04-01\n\n\n\n\n\n\n\nLet’s make a quick boxplot of our extracted values across seasons just to check that our extract function worked.\n\nfig, ax = plt.subplots(figsize=(7,3))\n# add boxplot \nb = sns.boxplot(x = 'rasDate', \n                y = 'CWC', \n                data = ex_cwc,\n                ax = ax,\n               width = 0.3,\n               palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}) # Create color palette\nb.set_ylabel(\"Canopy Water Content g/cm$\\mathregular{^2}$\", fontsize = 12)\nb.set_xlabel(\"Date\", fontsize = 12)\nb.set_title(\"Seasonal Changes in Canopy Water Content\", fontsize = 16)\n\n# overlay points for all values\nb = sns.stripplot(data = ex_cwc,\n                       x = 'rasDate', \n                       y = 'CWC', \n                       ax = ax,\n                      color = \"black\", # Colours the dots\n                      linewidth = 1,     # Dot outline width\n                      alpha = 0.4)       # Makes them transparent",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html#generate-boxplots-of-cwc-across-vegetation-types",
    "href": "python/04_Dangermond_Land_Cover.html#generate-boxplots-of-cwc-across-vegetation-types",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "4.3 Generate Boxplots of CWC Across Vegetation types",
    "text": "4.3 Generate Boxplots of CWC Across Vegetation types\nGreat! We managed to extract all the pixel values, but we are more interested in investigating the differences across vegetation plot types. Here, we separate the above plot into three faceted plots, one for each vegetation type. The final plot is called p_class.\n\np_class = sns.catplot(\n    x='rasDate', \n    y='CWC', \n    col='Class',  # This will create a separate plot for each unique value in the 'Class' column\n    data=ex_cwc, \n    kind='box',\n    col_wrap=3,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n    palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n)\n\n# set titles and x and y labels\np_class.set_titles(size = 18)\np_class.set_xlabels(\"Date\", fontsize = 18)\np_class.set_ylabels(\"Canopy Water Content g/cm$\\mathregular{^2}$\", fontsize = 18)\n\n\n\n\n\n\n\n\n\nWhat changes in CWC do you see across vegetation types and dates?",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html#extract-lst-values-and-create-corresponding-boxplot",
    "href": "python/04_Dangermond_Land_Cover.html#extract-lst-values-and-create-corresponding-boxplot",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "4.4 Extract LST values and create corresponding boxplot",
    "text": "4.4 Extract LST values and create corresponding boxplot\nWe are also interested in looking at changes in LST across time, so let’s repeat a similar process to extract LST values by vegetation type. The final extracted data frame is called final_df_lst\nCreate a list of all LST files called lst_list.\n\n# list all files in the ewt sub-directory of the data_dir path that end in tif\nlst_list = glob.glob(os.path.join(data_dir, \"lst\", \"*.tif\"))\nlst_list\n\n['../data/lst/ECOv002_L2T_LSTE_26921_001_10SGD_20230405T190258_0710_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_29901_007_10SGD_20231014T223936_0711_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_29576_005_10SGD_20230923T232104_0710_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_28238_012_10SGD_20230629T170416_0710_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST_dangermond.tif']\n\n\nCreate a subset of list of LST files using datetimes that are concurrent with EMIT acquisitions. The list is called fil_lst_list.\n\n# the date and time codes strings we want to select from our larger lst_list\nlst_dates = ['20230401T203733', '20230629T170416', '20230923T232104']\n\n# #loop so that the files appear chronologically like our lst_date list\nfil_lst_list = [] # empty list\nfor date in lst_dates:\n    # filter for appropriate dates and then add back\n    fil_lst_list.extend([file for file in lst_list if date in file])\nfil_lst_list\n\n['../data/lst/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_28238_012_10SGD_20230629T170416_0710_01_LST_dangermond.tif',\n '../data/lst/ECOv002_L2T_LSTE_29576_005_10SGD_20230923T232104_0710_01_LST_dangermond.tif']\n\n\n\nLet’s visualize all of our LST scenes.\n\n# Initialize an empty list to store the plots\nplots = []\n\n# Iterate over each LST file and create a plot\nfor file in fil_lst_list:\n    # Open the raster file\n    ras = rxr.open_rasterio(file).squeeze('band', drop=True)\n    \n    # Unlike EMIT rasters, extract date from filename using regex since it's not in metadata.\n    ras_date =  re.search(r\"\\d{8}\", file).group() # search for groups of 8 digits\n    date_object = datetime.strptime(ras_date, \"%Y%m%d\")\n    date_only = date_object.date()\n \n    # Convert Kelvin to Celsius\n    ras -= 273.15  \n    \n    # Add attributes for plotting\n    long_name_value = \"Land Surface Temperature\"\n    ras.attrs['long_name'] = long_name_value\n    units_value = \"°C\"  # Change the units to Celsius\n    ras.attrs['units'] = units_value\n    \n    # Create a plot using geoviews\n    plot = ras.hvplot.image(x='x', y='y', geo=True, cmap='reds',\n                                                    tiles='ESRI', \n                                                    title=f\"{date_only} {ras.long_name} ({ras.units})\",\n                                                    xlabel='Longitude', ylabel='Latitude',\n                                                    frame_width=400, frame_height= 300,\n                                                    fontscale=1, alpha=0.7)\n    \n    # Add the plot to the list\n    plots.append(plot)\n\n# Display all plots in a grid layout using Panel\ngrid = pn.GridSpec(sizing_mode='stretch_both')\nfor i, plot in enumerate(plots):\n    grid[i // 2, i % 2] = plot  # Adjust 3 to change the number of columns\n\ngrid.servable()\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nSimilar to before let’s extract all our raster values\n\n# create an empty list\nex_df_lst = []\n\n# loop through each raster in fil_lst_list\nfor r in fil_lst_list:\n    # extract values \n    ras = rxr.open_rasterio(r).squeeze('band',drop=True)\n    \n    # Convert from K to C\n    ras -= 273.15  \n    ex_ras = extract_raster_values(ras, veg_poly)\n    \n    # add date to the dataframe\n    rDate =  re.search(r\"\\d{8}\", r).group()\n    date_object = datetime.strptime(rDate, \"%Y%m%d\")\n    date_only = date_object.date()#.strftime(\"%Y-%m-%d\")\n    \n    # add column \n    ex_ras['rasDate'] = date_only\n    # add back to data frame with all values\n    ex_df_lst.append(ex_ras)\n\n# concatenate all extracts \nfinal_df_lst = pd.concat(ex_df_lst).reset_index(drop=True)\nfinal_df_lst.rename(columns = {'value':'LST'}, inplace = True) # Rename values column to LST\nfinal_df_lst = final_df_lst[final_df_lst['LST'] &gt; 0 ] # Removes any value less than 0\n\n\n# preview data\n#final_df_lst.head()\n\nLet’s take these values and make a box plot similar to before.\n\np_class_lst = sns.catplot(\n    x='rasDate', \n    y='LST', \n    col='Class',  # This will create a separate plot for each unique value in the 'Class' column\n    data=final_df_lst, \n    kind='box',\n    col_wrap=3,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n    palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n)\np_class_lst.set_titles(size = 18)\np_class_lst.set_xlabels(\"Date\", fontsize = 18)\np_class_lst.set_ylabels(\"Land Surface Temperature C\", fontsize = 18)\n\n\n\n\n\n\n\n\n\nWhat changes in LST do you see across vegetation types and dates?",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html#look-at-cwc-and-lst-together",
    "href": "python/04_Dangermond_Land_Cover.html#look-at-cwc-and-lst-together",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "4.5 Look at CWC and LST together",
    "text": "4.5 Look at CWC and LST together\nOften times we want to visualize two variables together, so let’s look at LST and CWC side by side.\n\n# Generate subplots with boxplots for different vegetation classes comparing CWC and LST\nn_classes = ex_cwc['Class'].nunique()\nfig, axes = plt.subplots(n_classes, 2, figsize=(15, 5 * n_classes), sharex='col')\n\n# Iterate over each class and plot\nfor i, class_name in enumerate(ex_cwc['Class'].unique()):\n    # Filter data for each class\n    class_subset = ex_cwc[ex_cwc['Class'] == class_name]\n    class_lst_subset = final_df_lst[final_df_lst['Class'] == class_name]\n\n    # Create boxplot for CWC on the first column\n    sns.boxplot(\n        x='rasDate', \n        y='CWC', \n        data=class_subset, \n        ax=axes[i, 0],  # Plot on the first column\n        width=0.3, \n        palette={\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n    )\n\n    # Create boxplot for LST on the second column\n    sns.boxplot(\n        x='rasDate', \n        y='LST', \n        data=class_lst_subset, \n        ax=axes[i, 1],  # Plot on the second column\n        width=0.3, \n        palette={\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n    )\n\n    # Set titles and labels\n    axes[i, 0].set_title(f'Class {class_name} - CWC')\n    axes[i, 1].set_title(f'Class {class_name} - LST')\n    axes[i, 0].set_xlabel('Date')\n    axes[i, 0].set_ylabel('CWC')\n    axes[i, 1].set_xlabel('Date')\n    axes[i, 1].set_ylabel('LST')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat do you notice about the changes in LST and CWC across vegetation types and dates? The CWC for iceplant is interesting because it remains very high across all dates indicating that these plants contain a lot of water throughout the year.",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/04_Dangermond_Land_Cover.html#interactive-playground",
    "href": "python/04_Dangermond_Land_Cover.html#interactive-playground",
    "title": "4 Exploring Canopy Water Content (CWC) and Land Surface Temperature (LST) across Vegetation Types in the Jack and Laura Dangermond Preserve",
    "section": "4.6 Interactive playground",
    "text": "4.6 Interactive playground\n\nNow let's have  some fun with what you have learned and let you draw your own polygons to study vegetation change!\n\nThis section relies on several functions from the jldp_ras_funcs.py script. The interactive polygon portion is based on https://github.com/auspatious/hyperspectral-notebooks/blob/main/04_EMIT_Interactive_Polygons.ipynb, with added functions to reformat the output polygons into a GeoPandas dataframe for use in the extract_raster_values function.\nHere we are limiting the number of polygons you can draw using POLY_LIMIT = 5 and are only displaying the canopy water content.\n\n# Limit the number of drawn polygons\nPOLY_LIMIT = 5\n\n\nLet’s display a CWC image and draw some polygons!\nHere, we will just load the canopy water content from september as our background image. This is because we already loaded it all the way above in section 4.1 when we first visualized the polygons. We have it opened as the variable cw3, which we then copy to the ds variable for using the visualization code.\n\nds = cw3\n\nIf you wanted to change the backround image you may do so using the code below.\n\n# if you wanted to change the background display do it here\n# for example, let's set it to the spring image (index zero in our file list)\n# ds = rxr.open_rasterio(fil_cwc_list[0]).squeeze('band',drop=True)\n\nSet up plotting parameters and change the color of each polygon that we will draw.\n\n# create color cycle and list of colors for the number of polygons \ncolor_cycle = hv.Cycle('Category10')\ncolors = [color_cycle[i] for i in range(POLY_LIMIT)]\n\n# add map variable similar to before\nmap = ds.hvplot.image(x='x',y='y',geo=True, cmap='blues',tiles='ESRI',\n                    title=f\"{ds.long_name} ({ds.units})\",\n                 xlabel='Longitude',ylabel='Latitude', frame_width=500,\n                 frame_height=500, fontscale=1.5, alpha = 0.7)\n\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = ds.x.values[int(len(ds.x) / 2)]\nymid = ds.y.values[int(len(ds.y) / 2)]\n\n# create holoview polygons\npolygons = hv.Polygons(\n    [],\n    kdims=[\"Longitude\", \"latitude\"],\n)\n# stream the drawn polygons\npolygons_stream = hv.streams.PolyDraw(\n    data=polygons.columns(),\n    source=polygons,\n    num_objects=POLY_LIMIT,\n    styles={'fill_color': color_cycle.values[0:POLY_LIMIT]}\n)\n\nTo draw a polygon activate the Polygon Draw Tool from the toolbar on the right hand side (fourth icon from the top). Then double click or click and hold (depending on your version of holoviews) somewhere on the map to start your polygon and double click to end drawing your polygon. If the hover window is in your way disable the hover tool by clicking on the last icon in the toolbar.\n\n# Plot the Map and Dynamic Map side by side\n(map * polygons)\n\n\n\n\n\n  \n\n\n\n\nWe will use the hv_stream_to_rio_geometries function to create a geojson style list of hand-drawn polygons and then convert them to a list. This list will then be converted to a geopandas dataframe for raster extraction.\n\n# list out the geometries of the different polygons \nmy_geometries = hv_stream_to_rio_geometries(polygons_stream.data)\ngeo_contents = list(my_geometries)\n#contents\n\nNow use the create_geodataframe function from jldp_ras_funcs.py to format your drawn polygons similarly to our veg_plot_outline.geojson file. This time we will not have a class column, but will have unique id column called poly_fid column. This column will have values that represent the order in which you drew polygons above, starting with index 0.\n\n# create a geodataframe from contents\nmy_poly_gdf = create_geodataframe(geo_contents, transform_needed=True)\nmy_poly_gdf\n\n\n\n\n\n\n\n\npoly_fid\ngeometry\n\n\n\n\n0\n0\nPOLYGON ((-120.48777 34.50011, -120.48725 34.4...\n\n\n1\n1\nPOLYGON ((-120.46722 34.47512, -120.46593 34.4...\n\n\n2\n2\nPOLYGON ((-120.46516 34.45733, -120.46542 34.4...\n\n\n\n\n\n\n\n\n\nNow that we have our polygons let’s make a box plot for our reference raster\nOur reference raster was variable ds.\n\n# extract values with previously loaded function\nex_ras = extract_raster_values(ds,my_poly_gdf)\n# remove all values less than 0 (which are -9999, all na values)\nex_ras = ex_ras[ex_ras['value'] &gt; 0 ] \n\nUsing the same plotting code from above.\n\np_class_selected = sns.catplot(\n    y='value', \n    col='poly_fid',  # This will create a separate plot for each unique value in the 'poly_fid' column\n    data=ex_ras, \n    kind='box',\n    col_wrap=3,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n    palette = {\"#56B4E9\", \"#009E73\", \"#CC79A7\"}\n)\np_class_lst.set_titles(size = 18)\np_class_lst.set_xlabels(\"Date\", fontsize = 18)\np_class_lst.set_ylabels(\"Canopy Water Content\", fontsize = 18)\n\n\n\n\n\n\n\n\nGreat! Our extraction and plotting worked!\n\n\nNow let’s extract data from more CWC dates\nIf you noticed at the beginning there were more than three CWC dates available when we printed cwc_list. Let’s add two more dates to our boxplots. * 02-19-2023 (this file will say merged because it is comprised of two EMIT scenes taken on the same day) * 10-14-2023\n\n# the date and time codes strings we want to select from our larger cwc_list\n# for the first one we are using the merged 0219 dataset\ncwc_dates = ['20230219T202939_2305013_002_dangermond_cwc_merged',\n             '20230401T203751', '20230629T170449', '20230923T232101',\n             '20231014T224006']\n#loop so that the files appear chronologically like our cwc_dates list\nfil_cwc_list = [] # empty list\nfor date in cwc_dates:\n    # filter for appropriate dates and then add back\n    fil_cwc_list.extend([file for file in cwc_list if date in file])\nfil_cwc_list\n\n['../data/cwc/EMIT_L2A_RFL_001_20230219T202939_2305013_002_dangermond_cwc_merged.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230629T170449_2318011_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20230923T232101_2326615_002_dangermond_cwc.tif',\n '../data/cwc/EMIT_L2A_RFL_001_20231014T224006_2328715_002_dangermond_cwc.tif']\n\n\n\n\nLet’s visualize all five images before extracting the data\n\n# Initialize an empty list to store the plots\nplots = []\n\n# Iterate over each CWC file and create a plot\nfor file in fil_cwc_list:\n    # Open the raster file\n    ras = rxr.open_rasterio(file).squeeze('band', drop=True)\n    \n    # extract the date string from the file name using string matching\n    ras_date =  re.search(r\"\\d{8}\", file).group()\n    date_object = datetime.strptime(ras_date, \"%Y%m%d\")\n    date_only = date_object.date()\n    \n    # Create a plot using geoviews\n    plot = ras.hvplot.image(x='x', y='y', geo=True, cmap='blues',\n                                                    tiles='ESRI', \n                                                    title=f\"{date_only} {ras.long_name} ({ras.units})\",\n                                                    xlabel='Longitude', ylabel='Latitude',\n                                                    frame_width=400, frame_height= 300,\n                                                    fontscale=1, alpha=0.7)\n    \n    # Add the plot to the list\n    plots.append(plot)\n\n# Display all plots in a grid layout using Panel\ngrid = pn.GridSpec(sizing_mode='stretch_both')\nfor i, plot in enumerate(plots):\n    grid[i // 2, i % 2] = plot  # Adjust 3 to change the number of columns\n\ngrid.servable()\n\n\n\n\n\n\n\n\n  \n\n\n\n\n\n\nNow let’s extract the data for the polygons we drew!\nYou should be pretty familiar with this loop by now. We did edit the line ex_ras = extract_raster_values(ras, my_poly_gdf) to use our polygon geodataframe my_poly_gdf for raster extraction instead of veg_poly.\n\n# create an empty list for the extracted values\nex_df = []\nfor r in fil_cwc_list:\n    ## extract values\n    ras = rxr.open_rasterio(r).squeeze('band',drop=True)\n    \n    ## here we replaced veg_poly with my_poly_gdf\n    ex_ras = extract_raster_values(ras, my_poly_gdf)\n    \n    # add date to the dataframe\n    date_object = datetime.strptime(ras.attrs['time_coverage_end'], \"%Y-%m-%dT%H:%M:%S%z\")\n    date_only = date_object.date()\n    \n    # add column \n    ex_ras['rasDate'] = date_only\n    # add back to data frame with all values\n    ex_df.append(ex_ras)\n\nex_cwc_five = pd.concat(ex_df).reset_index(drop=True)\n\n# change the name of the extracted data column\nex_cwc_five.rename(columns = {'value':'CWC'}, inplace = True)\n# remove all nan values from the raster\nex_cwc_five = ex_cwc_five[ex_cwc_five['CWC'] != -9999 ] \n\nPrint boxplots for all your extracted polygons. You should have 5 dates this time.\n\np_class_sel = sns.catplot(\n    x='rasDate', \n    y='CWC', \n    col='poly_fid',  # This will create a separate plot for each unique value in the 'poly_fid' column\n    data=ex_cwc_five, \n    kind='box',\n    col_wrap=2,  # Adjust this depending on how many plots per row you want\n    sharex=False, \n    sharey=False,\n    height=5, \n    aspect=1,\n    width = 0.3,\n)\n\np_class_sel.set_titles(size = 18)\np_class_sel.set_xlabels(\"Date\", fontsize = 18)\np_class_sel.set_ylabels(\"Canopy Water Content g/cm$\\mathregular{^2}$\", fontsize = 18)\n\n\n\n\n\n\n\n\n\nIf you have time, see if you can repeat this process for additional LST dates.",
    "crumbs": [
      "Python Notebooks",
      "4 Dangermond Land Cover"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "",
    "text": "Summary\nIn the previous notebook, we found and downloaded concurrent EMIT L2A Reflectance and ECOSTRESS L2 Land Surface Temperature and Emissivity scenes over our region of interest. In this notebook, we will open and explore the datasets along with ECOSTRESS L3 Evapotranspiration to better understand the structure, then we will conduct some common preprocessing routines to make the data usable together, including: applying quality data, reprojecting, placing data on a common grid, and cropping.\nBackground\nThe ECOSTRESS instrument is a multispectral thermal imaging radiometer designed to answer three overarching science questions:\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field. These temperature images, along with auxillary inputs, are used to produce one of the primary science outputs of ECOSTRESS: evapotranspiration, an indicator of plant health via the measure of evaporation and transpiration of water through a plant.\nMore details about ECOSTRESS and its associated products can be found on the ECOSTRESS website and ECOSTRESS product pages hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\nThe EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nReferences\n- Leith, Alex. 2023. Hyperspectral Notebooks. Jupyter Notebook. Auspatious. https://github.com/auspatious/hyperspectral-notebooks/tree/main\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only Set up Python Environment - See setup_instructions.md in the /setup/ folder to set up a local compatible Python environment\n- Downloaded necessary files. This is done at the end of the 01_Finding_Concurrent_Data notebook.\nLearning Objectives\n- How to open and work with EMIT L2A Reflectance and ECOSTRESS L2T LSTE data\n- How to apply a quality mask to EMIT datasets\n- How to reproject and regrid data\n- How to crop EMIT and ECOSTRESS data\nTutorial Outline\n2.1 Setup\n2.2 Opening and Exploring EMIT Data\n2.2.1 Applying Quality Masks to EMIT Data\n2.2.2 Interactive Plots\n2.2.3 Cropping EMIT Data\n2.2.4 Writing Outputs\n2.3 Opening and Exploring ECOSTRESS Data\n2.3.1 Reprojecting and Regridding ECOSTRESS Data\n2.3.2 Cropping ECOSTRESS Data\n2.3.3 Writing Outputs",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#setup",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#setup",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nImport Python libraries.\n\n# Import Packages\nimport warnings\n\n# Some cells may generate warnings that we can ignore. Comment below lines to see.\nwarnings.filterwarnings('ignore')\n\nimport math\nimport earthaccess\nimport numpy as np\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv\nimport geopandas as gp\nfrom modules.emit_tools import emit_xarray, spatial_subset, ortho_xr\n\nLog into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them.\n\nearthaccess.login(persist=True)\n\nFor the workshop, we will stream the data, but you can also use this notebook if you downloaded the data. Either method can be used to complete this notebook. If you downloaded the data in notebook 1, Finding Concurrent Data, you can comment out the cells in the streaming section, and run the cell in the downloading section. Functionally after those sections, there’s really no difference in the workflow, just how the data is accessed. For EMIT data, streaming is often less efficient if your internet connection is slow.",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#streaming-emit-data",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#streaming-emit-data",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "2.1.1 Streaming EMIT Data",
    "text": "2.1.1 Streaming EMIT Data\nFirst, read the data from the URLs we found in the previous notebook. Open the required_granules.txt file and read the URLs into a list.\n\n# List the files in the text file output from notebook 1.\nfile_list = '../data/required_granules.txt'\nwith open(file_list) as f:\n    urls = [line.rstrip('\\n') for line in f]\nurls\n\nNext we need to start an fsspec https session, and use it to open the urls. This will allow us to access the data in the cloud like its part of our local file system. We’ll use this method to access the EMIT netCDF files, but a slightly different workflow for the ECOSTRESS data, which we will show later.\n\n# Get Https Session using Earthdata Login Info\nfs = earthaccess.get_fsspec_https_session()\n\n# Define Local Filepath\nemit_fp = fs.open(urls[1])\nemit_qa_fp = fs.open(urls[2])",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#downloading-emit-data",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#downloading-emit-data",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "2.1.2 Downloading EMIT Data",
    "text": "2.1.2 Downloading EMIT Data\nIf you’ve already downloaded the data using the workflow shown in Section 6 of the Finding Concurrent Data notebook, you can just set filepaths using the cell below.\nDefine a filepath for an EMIT L2A Reflectance file, EMIT L2A Mask file, and an ECOSTRESS L2T LSTE and ECOSTRESS L2T Mask file. The files selected in this example are from April 1, 2023, at around 20:37.\n\n# emit_fp = \"../data/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc\"\n# emit_qa_fp = \"../data/EMIT_L2A_MASK_001_20230401T203751_2309114_002.nc\"\n# eco_fp = \"../data/ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\"\n# # eco_et_fp = \"../data/ECOv002_L3T_JET_26860_001_10SGD_20230401T203732_0700_01_ETdaily.tif\" # concurrent scene generated for tutorial 5\n# eco_et_fp = \"../data/ECOv002_L3T_JET_30644_005_10SGD_20231201T201005_0711_01_ETdaily.tif\" # newly available scene for ECOSTRESS ET",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#opening-and-exploring-emit-reflectance-data",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#opening-and-exploring-emit-reflectance-data",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "2.2 Opening and Exploring EMIT Reflectance Data",
    "text": "2.2 Opening and Exploring EMIT Reflectance Data\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. Inside the L2A Reflectance .nc file there are 3 groups. Groups can be thought of as containers to organize the data.\n\nThe root group that can be considered the main dataset contains the reflectance data described by the downtrack, crosstrack, and bands dimensions.\n\nThe sensor_band_parameters group containing the wavelength center and the full-width half maximum (FWHM) of each band.\n\nThe location group contains latitude and longitude values at the center of each pixel described by the crosstrack and downtrack dimensions, as well as a geometry lookup table (GLT) described by the ortho_x and ortho_y dimensions. The GLT is an orthorectified image (EPSG:4326) consisting of 2 layers containing downtrack and crosstrack indices. These index positions allow us to quickly project the raw data onto this geographic grid.\n\nTo work with the EMIT data, we will use the emit_tools module. There are other ways to work with the data and a more thorough explanation of the emit_tools in the EMIT-Data-Resources Repository.\nOpen the example EMIT scene using the emit_xarray function. In this step we will use the ortho=True argument to orthorectify the scene using the included GLT.\n\n# Load the data to speed up future cells\nemit_ds = emit_xarray(emit_fp, ortho=True).load()\nemit_ds\n\nWe can plot the spectra of an individual pixel closest to a latitude and longitude we want using the sel function from xarray. Using the good_wavelengths flag from the sensor_band_parameters group, mask out bands where water absorption features were assigned a value of -0.01 reflectance. Typically data around 1320-1440 nm and 1770-1970 nm is noisy due to the moisture present in the atmosphere; therefore, these spectral regions offer little information about targets and can be excluded from calculations.\n\nemit_ds['reflectance'].data[:,:,emit_ds['good_wavelengths'].data==0] = np.nan\n\nNow select a point and plot a spectra. In this example, we’ll first find the center of the scene and use those coordinates.\n\nscene_center = emit_ds.latitude.values[int(len(emit_ds.latitude)/2)],emit_ds.longitude.values[int(len(emit_ds.longitude)/2)]\nscene_center\n\n\npoint = emit_ds.sel(latitude=scene_center[0],longitude=scene_center[1], method='nearest')\npoint.hvplot.line(y='reflectance',x='wavelengths', color='black').opts(\n    title=f'Latitude = {point.latitude.values.round(3)}, Longitude = {point.longitude.values.round(3)}')\n\nWe can also plot individual bands spatially by selecting a wavelength, then plotting.\nFirst, mask the fill values with np.nan so they appeear transparent, then select the band with a wavelength of 850 nm and plot it using ESRI imagery as a basemap to get a better understanding of where the scene was acquired.\n\n# Mask Fill Values\nemit_ds['reflectance'].data[emit_ds['reflectance'].data == -9999] = np.nan\n\n\n# Select Wavelength and Plot\nemit_layer = emit_ds.sel(wavelengths=850,method='nearest')\nemit_layer.hvplot.image(cmap='viridis',geo=True, tiles='ESRI', crs='EPSG:4326', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{emit_layer.wavelengths:.3f} {emit_layer.wavelengths.units}\", xlabel='Longitude',ylabel='Latitude')\n\n\n2.2.1 Applying Quality Masks to EMIT Data\nThe EMIT L2A Mask file contains some bands that are direct masks (Cloud, Dilated, Cirrus, Water, Spacecraft), and some (AOD550 and H2O (g cm-2)) that contain information calculated during the L2A reflectance retrieval. These may be used as additional screening, depending on the application. The Aggregate Flag is the mask used during EMIT L2B Mineralogy calculations, which we will also use here, but not all users might want this particular mask.\n\nNote: It is more memory efficient to apply the mask before orthorectifying.\n\n\nemit_mask = emit_xarray(emit_qa_fp, ortho=True)\nemit_mask\n\nList the quality flags contained in the mask_bands dimension.\n\nemit_mask.mask_bands.data.tolist()\n\nAs mentioned, we will use the Aggregate Flag. Select that band with the sel function as we did for wavelengths before.\n\nemit_cloud_mask = emit_mask.sel(mask_bands='Aggregate Flag')\n\nLike we did with the reflectance data, set the data with a fill value (-9999) equal to np.nan to improve the visualization.\n\nemit_cloud_mask.mask.data[emit_cloud_mask.mask.data == -9999] = np.nan\n\nNow we can visualize our aggregate quality mask. You may have noticed before that we added a lot of parameters to our plotting function. If we want to consistently apply the same formatting for multiple plots, we can add those arguments to a dictionary that we can unpack into hvplot functions using **.\nCreate two dictionaries with plotting options.\n\nsize_opts = dict(frame_height=405, frame_width=720, fontscale=2)\nmap_opts = dict(geo=True, crs='EPSG:4326', alpha=0.7, xlabel='Longitude',ylabel='Latitude')\n\n\nemit_cloud_mask.hvplot.image(cmap='viridis', tiles='ESRI', **size_opts, **map_opts)\n\nValues of 1 in the mask indicate areas to omit. Apply the mask to our EMIT Data by assigning values where the mask.data == 1 to np.nan\n\nemit_ds.reflectance.data[emit_cloud_mask.mask.data == 1] = np.nan\n\nWe can confirm our masking worked with a spatial plot.\n\nemit_layer_filtered_plot = emit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',tiles='ESRI',**size_opts, **map_opts)\nemit_layer_filtered_plot\n\n\n\n2.2.2 Interactive Spectral Plots\nCombining the Spatial and Spectral information into a single visualization can be a powerful tool for exploring and inspecting imaging spectroscopy data. Using the streams module from Holoviews we can link a spatial map to a plot of spectra.\nWe could plot a single band image as we previously have, but using a multiband image, like an RGB may help infer what targets we’re examining. Build an RGB image following the steps below.\nSelect bands to represent red (650 nm), green (560 nm), and blue (470 nm) by finding the nearest to a wavelength chosen to represent that color.\n\nemit_rgb = emit_ds.sel(wavelengths=[650, 560, 470], method='nearest')\n\nWe may need to adjust balance the brightness of the selected wavelengths to make a prettier map. This will not affect the data, just the visuals. To do this we will use the function below. We can change the bright argument to increase or decrease the brightness of the scene as a whole. A value of 0.2 usually works pretty well.\n\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Assign NA's to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n\n\nemit_rgb = gamma_adjust(emit_rgb,white_background=True)\n\nNow that we have an RGB dataset, we can use that to create a spatial plot, and data selected by clicking on that ‘map’ can be inputs for a function to return values from the full dataset at that latitude and longitude location using the cell below. To visualize the spectral and spatial data side-by-side, we use the Point Draw tool from the holoviews library.\nDefine a limit to the quantity of points and spectra we will plot, a list of colors to cycle through, and an initial point. Then use the input from the Tap function to provide clicked x and y positions on the map and use these to retrieve spectra from the dataset at those coordinates.\nClick in the RGB image to add spectra to the plot. You can also click and hold the mouse button then drag previously place points. To remove a point click and hold the mouse button down, then press the backspace key.\n\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = emit_rgb.hvplot.rgb(fontscale=1.5, xlabel='Longitude',ylabel='Latitude',frame_width=480, frame_height=480)\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = emit_ds.longitude.values[int(len(emit_ds.longitude) / 2)]\nymid = emit_ds.latitude.values[int(len(emit_ds.latitude) / 2)]\n\nfirst_point = ([xmid], [ymid], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n                                                                            color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)\n\nWe can take these selected points and the corresponding reflectance spectra and save them as a .csv for later use.\nSelect 10 points by adding to the figure above. We will save these and use them in a to calculate Equivalent Water Thickness or Canopy water content in the next notebook.\nBuild a dictionary of the selected points and spectra, then export the spectra to a .csv file.\n\ndata = points_stream.data\nwavelengths = emit_ds.wavelengths.values\n\nrows = [[\"id\", \"x\", \"y\"] + [str(i) for i in wavelengths]]\n \nfor p in zip(data['x'], data['y'], data['id']):\n    x, y, i = p\n    spectra = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\").reflectance.values\n    row = [i, x, y] + list(spectra)\n    rows.append(row)\n\nWe’ve preselected 10 points, but feel free to uncomment the cell below to use your own. This will overwrite the file containing the preselected points.\n\n# with open('../data/emit_click_data.csv', 'w', newline='') as f:\n#     writer = csv.writer(f)\n#     writer.writerows(rows)\n\n\n\n2.2.3 Cropping EMIT data to a Region of Interest\nTo crop our dataset to our ROI we first need to open a shapefile of the region. Open the included geojson for Sedgwick Reserve and Plot it onto our EMIT 850nm reflectance spatial plot. To ensure the plotting of the shape and EMIT scene works, be sure to specify the CRS (this is done for the image in the map_opts dictionary).\n\nshape = gp.read_file(\"../data/dangermond_boundary.geojson\")\nshape\n\n\nemit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',**size_opts,**map_opts,tiles='ESRI')*shape.hvplot(color='#d95f02',alpha=0.5, crs='EPSG:4326')\n\nNow use the clip function from rasterio to crop the data to our ROI using our shape’s geometry and crs. The all_touched=True argument will ensure all pixels touched by our polygon will be included.\n\nemit_cropped = emit_ds.rio.clip(shape.geometry.values,shape.crs, all_touched=True)\n\nPlot the cropped data.\n\nemit_cropped.sel(wavelengths=850,method='nearest').hvplot.image(cmap='viridis', tiles='ESRI', **size_opts, **map_opts)\n\n\n\n2.2.4 Write an output\nLastly for our EMIT dataset, we can write a smaller output that we can use in later notebooks, to calculate Canopy water content or other applications. We use the granule_id from the dataset to keep a similar naming convention.\n\n# Write Clipped Output\nemit_cropped.to_netcdf(f'../data/{emit_cropped.granule_id}_dangermond.nc')",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#working-with-ecostress-l2t-land-surface-temperature-and-emissivity",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#working-with-ecostress-l2t-land-surface-temperature-and-emissivity",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "2.3. Working with ECOSTRESS L2T Land Surface Temperature and Emissivity",
    "text": "2.3. Working with ECOSTRESS L2T Land Surface Temperature and Emissivity\nFor this example we’re taking a look at the ECOSTRESS Level 2 Tiled Land Surface Temperature (ECO_L2T_LSTE) and Level 3 Tiled Evapotranspiration (ECO_L3T_JET) products. The Land Surface Temperature and Emissivity values are derived from five thermal infrared (TIR) bands using a physics-based Temperature and Emissivity Separation (TES) algorithm. The Level 3 Tiled Evapotranspiration product is estimated from a combination of Level 2 Tiled Land Surface Temperature products and other auxillary inputs. These tiled data products use a modified version of the Military Grid Reference System (MGRS) which divides Universal Transverse Mercator (UTM) zones into square tiles that are 109.8 km by 109.8 km with a 70 meter (m) spatial resolution.\nThe ECOSTRESS L3T Tiled Evapotranspiration (ECO_L3T_JET) product is now being delivered to the LP DAAC. Right now these are only being forward processed from around October 2023, but historical processing to cover dates will start within the next year. Since the time range used for this workshop isn’t available we have added a non-concurrent scene from our region of interest to the required_granules.txt to showcase the ET dataset.\n\n2.3.1 Streaming ECOSTRESS Tiled Data\nTo stream the ECOSTRESS data, which is formatted as a cloud-optimized geotiff (COG) we will use a different approach than for EMIT. We can use the rioxarray library, which is built on GDAL to access the data, just using a URL as the filepath, but first we need to set some GDAL configuration options to make sure that our authentication credentials are being passed properly to NASA Earthdata.\nSet the necessary GDAL configuration options.\n\n# GDAL configurations used to successfully access LP DAAC Cloud Assets via vsicurl \ngdal.SetConfigOption('GDAL_HTTP_COOKIEFILE','~/cookies.txt')\ngdal.SetConfigOption('GDAL_HTTP_COOKIEJAR', '~/cookies.txt')\ngdal.SetConfigOption('GDAL_DISABLE_READDIR_ON_OPEN','EMPTY_DIR')\ngdal.SetConfigOption('CPL_VSIL_CURL_ALLOWED_EXTENSIONS','TIF')\ngdal.SetConfigOption('GDAL_HTTP_UNSAFESSL', 'YES')\n\nOpen the LSTE and ET files using open_rasterio from the rioxarray library. Since the files consist of only 1 layer, we can squeeze it, removing the band dimension.\n\neco_lst_ds = rxr.open_rasterio(urls[0]).squeeze('band', drop=True)\neco_lst_ds\n\n\n# ECO ET Tile is URL 4\neco_et_ds = rxr.open_rasterio(urls[4]).squeeze('band', drop=True)\neco_et_ds\n\nAs mentioned the ECOSTRESS product we are using here is tiled and the coordinate reference system (CRS) is dependent on UTM zone. For this tile, we can look at the spatial_ref variable through the interactive object above to see details such as the well-known-text (WKT) representation of the CRS and other attributes.\nNow let’s plot the data using hvplot. Reproject the dataset first, and be sure to specify the CRS argument within the hvplot.image function so the ESRI imagery RBG background tile aligns with our scene.\n\neco_lst_ds.rio.reproject('EPSG:4326').hvplot.image(x='x',y='y',**size_opts, cmap='inferno',tiles='ESRI', xlabel='Longitude',ylabel='Latitude',title='ECOSTRESS LST (K)', crs='EPSG:4326')\n\nFor the ET data, we can plot the data in the same way, and even generate a color map to better visualize the data.\nCreate a gradient color map for the ET data.\n\n# Generating ECOSTRESS ET color ramp \nET_colors = [\n    \"#f6e8c3\",\n    \"#d8b365\",\n    \"#99974a\",\n    \"#53792d\",\n    \"#6bdfd2\",\n    \"#1839c5\"\n]\ndef interpolate_hex(hex1, hex2, ratio):\n    rgb1 = [int(hex1[i:i+2], 16) for i in (1, 3, 5)]\n    rgb2 = [int(hex2[i:i+2], 16) for i in (1, 3, 5)]\n    rgb = [int(rgb1[i] + (rgb2[i] - rgb1[i]) * ratio) for i in range(3)]\n    \n    return '#{:02x}{:02x}{:02x}'.format(*rgb)\n\nET_gradient = []  \n \nfor i in range(len(ET_colors) - 1):\n    for j in range(100):\n        ratio = j / float(100)\n        ET_gradient.append(interpolate_hex(ET_colors[i], ET_colors[i+1], ratio))\n\nET_gradient.append(ET_colors[-1])\n\nVisualize the ET data using the color map (remember this is a different scene, so it will look slightly different).\n\neco_et_ds.rio.reproject('EPSG:4326').hvplot.image(x='x',y='y',**size_opts, clim = (eco_et_ds.quantile(0.02),eco_et_ds.quantile(0.98)), cmap=ET_gradient, tiles='ESRI', xlabel='Longitude',ylabel='Latitude',title='ECOSTRESS ET (mm/day)', crs='EPSG:4326')\n\n\n\n2.3.1 Reprojecting and Regridding ECOSTRESS Data\nWe will need to reproject manually to pair this scene with the EMIT data, but we will not need to mask ECOSTRESS, because cloud masking has already been done for the tiled LSTE product.\nTo give a reasonable 1:1 comparison, in addition to reprojecting we want the data on the same grid, so each pixel from the ECOSTRESS scene corresponds to a pixel in the EMIT scene. To do this, we can use the reproject_match function from the rioxarray library. This will reproject and regrid our ECOSTRESS data to match the EMIT CRS and grid using the spatial_ref variable from each dataset. Since we’ve already cropped the EMIT scene, this will limit our ECOSTRESS scene to the extent of that cropped EMIT scene as well.\n\neco_lst_ds_regrid = eco_lst_ds.rio.reproject_match(emit_cropped)\neco_et_ds_regrid = eco_et_ds.rio.reproject_match(emit_cropped)\n\nWe can now visualize our reprojected, regridded ECOSTRESS LST and ET scenes.\n\neco_lst_ds_regrid.hvplot.image(geo=True, tiles='ESRI',cmap='inferno',**size_opts, xlabel='Longitude',ylabel='Latitude',title='Regridded ECOSTRESS LST (K)', crs='EPSG:4326')\n\n\neco_et_ds_regrid.hvplot.image(geo=True, tiles='ESRI',cmap=ET_gradient,**size_opts, clim=(eco_et_ds_regrid.quantile(0.02), eco_et_ds_regrid.quantile(0.98)), xlabel='Longitude',ylabel='Latitude',title='Regridded ECOSTRESS ET (mm/day)', crs='EPSG:4326')\n\n\n\n2.3.2 Cropping ECOSTRESS Data\nThis has been cropped to the extent, but we can further mask data outside of our region of interest by using the clip function like we did for EMIT data.\n\neco_dangermond = eco_lst_ds_regrid.rio.clip(shape.geometry.values,shape.crs, all_touched=True)\neco_et_dangermond = eco_et_ds_regrid.rio.clip(shape.geometry.values,shape.crs, all_touched=True)\n\n\neco_dangermond.hvplot.image(geo=True,cmap='inferno',**size_opts, tiles='ESRI',alpha=0.7, title='Cropped ECOSTRESS LST (K)', xlabel='Longitude',ylabel='Latitude', crs='EPSG:4326')\n\n\neco_et_dangermond.hvplot.image(geo=True,cmap=ET_gradient,**size_opts, clim=(eco_et_dangermond.quantile(0.02), eco_et_dangermond.quantile(0.98)), tiles='ESRI',alpha=0.7, title='Cropped ECOSTRESS ET (mm/day)', xlabel='Longitude',ylabel='Latitude', crs='EPSG:4326')\n\n\n\n2.3.3 Writing Outputs\nWe now have a subset ECOSTRESS scene that is aligned with EMIT data that we can export for our use in later notebooks.\nSave the ECOSTRESS LSTE scene.\n\n# Uncomment to overwrite included sample\n# eco_outname = f\"../data/{eco_fp.split('/')[-1].split('.')[0]}_dangermond.tif\"\n# eco_dangermond.rio.to_raster(raster_path=eco_outname, driver='COG')\n\n# eco_et_outname = f\"../data/{eco_et_fp.split('/')[-1].split('.')[0]}_et_dangermond.tif\"\n# eco_et_dangermond.rio.to_raster(raster_path=eco_et_outname, driver='COG')",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#contact-info",
    "href": "python/02_Working_with_EMIT_Reflectance_and_ECOSTRESS_LST.html#contact-info",
    "title": "2 Working with EMIT L2A Reflectance and ECOSTRESS L2 LSTE Products",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-20-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Notebooks",
      "2 EMIT Reflectance and ECOSTRESS LST"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Surface Biology and Geology: VITALS Workshop",
    "section": "",
    "text": "The International Space Station is a critical asset for the Earth science community – both for advancing critical science and applications priorities, and as a platform for technology demonstrations/pathfinders. These benefits have been particularly significant in recent years, with the installation and operation of instruments such as ECOSTRESS, a multispectral thermal instrument, and EMIT, a visible to short wave infrared imaging spectrometer with best-in-class signal to noise - both acquiring data at field-scale (&lt;70-m). With both sensors mounted on the ISS, there is an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this workshop we highlight the power of these tools when used together, through the use of open source tools and services, cloud compute resources to effectively combine data from ECOSTRESS and EMIT to perform scientific analyses and apply data to real world issues.\nThis workshop is hosted by NASA Land Processes Distributed Active Archive Center(LP DAAC) and NASA Jet Propulsion Laboratory (JPL) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance.",
    "crumbs": [
      "Welcome",
      "2024 SBG Workshop"
    ]
  },
  {
    "objectID": "index.html#agenda",
    "href": "index.html#agenda",
    "title": "Surface Biology and Geology: VITALS Workshop",
    "section": "Agenda",
    "text": "Agenda\n\n\n\n\n\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n2:00 PM\nIntroduction and Overview\nChristiana Ade\n\n\n2:05 PM\nGit, Open Science, and the Cloud Environment\nErik Bolch\n\n\n2:30 PM\nNotebook 1: Finding Concurrent EMIT and ECOSTRESS Data\nErik Bolch\n\n\n3:00 PM\nBreak\n\n\n\n3:10 PM\nNotebook 2: Exploring EMIT Reflectance and ECOSTRESS LST\nErik Bolch\n\n\n3:40 PM\nNotebook 3: Estimating Canopy Water Content\nNiklas Bohn  Erik Bolch\n\n\n4:00 PM\nBreak\n\n\n\n4:10 PM\nNotebook 4: Dangermond Preserve Land Cover Use Case\nChristiana Ade\n\n\n4:45 PM\nDiscussion and Wrap Up\nAll",
    "crumbs": [
      "Welcome",
      "2024 SBG Workshop"
    ]
  },
  {
    "objectID": "index.html#contact-info",
    "href": "index.html#contact-info",
    "title": "Surface Biology and Geology: VITALS Workshop",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-21-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Welcome",
      "2024 SBG Workshop"
    ]
  },
  {
    "objectID": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html",
    "href": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html",
    "title": "2 Working with EMIT L2A Reflectance and NEON L3 Directional Reflectance Data",
    "section": "",
    "text": "Summary\nIn the previous notebook, we found and downloaded co-located EMIT L2A Reflectance and NEON L3 Airborne Reflectance data over the NEON Niwot Ridge site in Colorado. In this notebook, we will open and explore both datasets to better understand the structure, then we will conduct some common preprocessing routines to work with the data together, including: applying quality data information, reprojecting, placing data on a common grid, and cropping.\nBackground\nThe EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nThe NEON Imaging Spectrometer (NIS) is an airborne imaging spectrometer built by JPL (AVIRIS-NG) and operated by the National Ecological Observatory Network’s (NEON) Airborne Observation Platform (AOP). NEON’s hyperspectral sensors collect measurements of sunlight reflected from the Earth’s surface in 426 narrow (~5 nm) spectral channels spanning wavelengths between ~ 380 - 2500 nm. NEON’s remote sensing data is intended to map and answer questions about a landscape, with ecological applications including identifying and classifying plant species and communities, mapping vegetation health, detecting disease or invasive species, and mapping droughts, wildfires, or other natural disturbances and their impacts.\nNEON surveys sites spanning the Continental US, Alaska, Hawaii, and Puerto Rico, during peak phenological greenness, capturing each site 3 out of every 5 years, for most terrestrial sites (*Hawaii and Puerto Rico are surveyed ~1 out of every 4-5 years). AOP’s Flight Schedules and Coverage provides more information about current and past airborne schedules.\nMore detailed information about NEON’s airborne sampling design can be found in the paper: Spanning scales: The airborne spatial and temporal sampling design of the National Ecological Observatory Network.\nReferences\n- Leith, Alex. 2023. Hyperspectral Notebooks. Jupyter Notebook. Auspatious. https://github.com/auspatious/hyperspectral-notebooks/tree/main - Musinsky et al. 2022. Spanning Scales: the airborne spatial and temporal sampling design o the National Ecological Observatory Network.\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only Set up Python Environment - See setup_instructions.md in the /setup/ folder to set up a local compatible Python environment\n- Downloaded necessary files. This is done at the end of the 01_Finding_Co-located_Data_NIWO notebook.\nLearning Objectives\n- How to open and work with EMIT L2A Reflectance and NEON L3 Hyperspectral Reflectance data\n- How to apply a quality mask to EMIT datasets\n- How to reproject and regrid data\n- How to crop EMIT data - How to interactively explore the Reflectance Spectra at multiple scales\nTutorial Outline\n2.1 Setup\n2.2 Opening and Exploring EMIT Reflectance Data\n2.2.1 Applying Quality Masks to EMIT Data\n2.2.2 Interactive Plots\n2.2.3 Cropping EMIT Data\n2.2.4 Writing Outputs\n2.3 Opening and Exploring NEON Reflectance Data\n2.3.1 Interactive Plots 2.3.2 Writing Outputs"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#setup",
    "href": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#setup",
    "title": "2 Working with EMIT L2A Reflectance and NEON L3 Directional Reflectance Data",
    "section": "2.1 Setup",
    "text": "2.1 Setup\nImport Python libraries.\n\n# Import Packages\nimport os, sys\nimport math\nimport numpy as np\nimport xarray as xr\nimport h5py\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nimport hvplot.xarray\nimport hvplot.pandas\nimport holoviews as hv\nimport geopandas as gp\n\n\nif '../../python/modules' not in sys.path:\n    sys.path.append('../../python/modules') \nfrom emit_tools import emit_xarray\n#import geoviews as gv\n#from holoviews.plotting.links import DataLink\n\nDefine a filepath for an EMIT L2A Reflectance file and EMIT L2A Mask file. The files selected in this example are from June 22, 2023, at around 19:32.\n\nemit_fp = \"./data/emit_refl/EMIT_L2A_RFL_001_20230625T170814_2317611_005.nc\"\nemit_qa_fp = \"./data/emit_refl/EMIT_L2A_MASK_001_20230625T170814_2317611_005.nc\""
  },
  {
    "objectID": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#opening-and-exploring-emit-reflectance-data",
    "href": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#opening-and-exploring-emit-reflectance-data",
    "title": "2 Working with EMIT L2A Reflectance and NEON L3 Directional Reflectance Data",
    "section": "2.2 Opening and Exploring EMIT Reflectance Data",
    "text": "2.2 Opening and Exploring EMIT Reflectance Data\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. Inside the L2A Reflectance .nc file there are 3 groups. Groups can be thought of as containers to organize the data.\n\nThe root group that can be considered the main dataset contains the reflectance data described by the downtrack, crosstrack, and bands dimensions.\n\nThe sensor_band_parameters group containing the wavelength center and the full-width half maximum (FWHM) of each band.\n\nThe location group contains latitude and longitude values at the center of each pixel described by the crosstrack and downtrack dimensions, as well as a geometry lookup table (GLT) described by the ortho_x and ortho_y dimensions. The GLT is an orthorectified image (EPSG:4326) consisting of 2 layers containing downtrack and crosstrack indices. These index positions allow us to quickly project the raw data onto this geographic grid.\n\nTo work with the EMIT data, we will use the emit_tools module. There are other ways to work with the data and a more thorough explanation of the emit_tools in the EMIT-Data-Resources Repository.\nOpen the example EMIT scene using the emit_xarray function. In this step we will use the ortho=True argument to orthorectify the scene using the included GLT.\n\nemit_ds = emit_xarray(emit_fp, ortho=True)\nemit_ds\n\n\nemit_ds.coords['spatial_ref']\n\nWe can plot the spectra of an individual pixel closest to a latitude and longitude we want using the sel function from xarray. Using the good_wavelengths flag from the sensor_band_parameters group, mask out bands where water absorption features were assigned a value of -0.01 reflectance. Typically data around 1320-1440 nm and 1770-1970 nm is noisy due to the moisture present in the atmosphere; therefore, these spectral regions offer little information about targets and can be excluded from calculations.\n\nemit_ds['reflectance'].data[:,:,emit_ds['good_wavelengths'].data==0] = np.nan\n\nNow select a point and plot a spectra. In this example, we’ll first find the center of the scene and use those coordinates.\n\nscene_center = emit_ds.latitude.values[int(len(emit_ds.latitude)/2)],emit_ds.longitude.values[int(len(emit_ds.longitude)/2)]\nscene_center\n\n\npoint = emit_ds.sel(latitude=scene_center[0],longitude=scene_center[1], method='nearest')\npoint.hvplot.line(y='reflectance',x='wavelengths', color='black').opts(\n    title=f'Latitude = {point.latitude.values.round(3)}, Longitude = {point.longitude.values.round(3)}')\n\nWe can also plot individual bands spatially by selecting a wavelength, then plotting. First mask out fill_values introduced during the orthorectification process by assigning their values to nan. Then select the band with a wavelengths of 850 nm and plot it using ESRI imagery as a basemap to get a better picture of the scene.\n\nemit_ds['reflectance'].data[emit_ds['reflectance'].data == -9999] = np.nan\n\n\nemit_layer = emit_ds.sel(wavelengths=850,method='nearest')\nemit_layer.hvplot.image(cmap='viridis',geo=True, tiles='ESRI', crs='EPSG:4326', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{emit_layer.wavelengths:.3f} {emit_layer.wavelengths.units}\", xlabel='Longitude',ylabel='Latitude')\n\n\n2.2.1 Applying Quality Masks to EMIT Data\nThe EMIT L2A Mask file contains some bands that are direct masks (Cloud, Dilated, Cirrus, Water, Spacecraft), and some (AOD550 and H2O (g cm-2)) that contain information calculated during the L2A reflectance retrieval. These may be used as additional screening, depending on the application. The Aggregate Flag is the mask used during EMIT L2B Mineralogy calculations, which we will also use here, but not all users might want this particular mask.\n\nNote: It is more memory efficient to apply the mask before orthorectifying, so during the automation section we will do that.\n\n\nemit_mask = emit_xarray(emit_qa_fp, ortho=True)\nemit_mask\n\n\n# optionally look at the info\n# emit_mask.info()\n\nList the quality flags contained in the mask_bands dimension.\n\nemit_mask.mask_bands.data.tolist()\n\nAs mentioned, we will use the Aggregate Flag. First mask areas with fill_value of -9999 like we did for reflectence, then select the Aggregate Flag band with the sel function as we did for wavelengths before.\n\nemit_mask['mask'].data[emit_mask['mask'].data == -9999] = np.nan\n\n\nemit_aggregate_mask = emit_mask.sel(mask_bands='Aggregate Flag')\n\nNow we can visualize our aggregate quality mask. You may have noticed before that we added a lot of parameters to our plotting function. If we want to consistently apply the same formatting for multiple plots, we can add those arguments to a dictionary that we can unpack into hvplot functions using **.\nCreate two dictionaries with plotting options.\n\nsize_opts = dict(frame_height=405, frame_width=720, fontscale=2)\nmap_opts = dict(geo=True, crs='EPSG:4326', alpha=0.7, xlabel='Longitude',ylabel='Latitude')\n\n\nemit_aggregate_mask.hvplot.image(cmap='viridis', tiles='ESRI', **size_opts, **map_opts)\n\nValues of 1 in the mask indicate areas to omit. Apply the mask to our EMIT Data by assigning values where the mask.data == 1 to np.nan\n\nemit_ds.reflectance.data[emit_aggregate_mask.mask.data == 1] = np.nan\n\nWe can confirm our masking worked with a spatial plot.\n\nemit_layer_filtered_plot = emit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',tiles='ESRI',**size_opts, **map_opts)\nemit_layer_filtered_plot\n\n\n\n2.2.2 Interactive Spectral Plots\nCombining the Spatial and Spectral information into a single visualization can be a powerful tool for exploring and inspecting imaging spectroscopy data. Using the streams module from Holoviews we can link a spatial map to a plot of spectra.\nWe could plot a single band image as we previously have, but using a multiband image, like an RGB may help infer what targets we’re examining. Build an RGB image following the steps below.\nSelect bands to represent red (650 nm), green (560 nm), and blue (470 nm) by finding the nearest to a wavelength chosen to represent that color.\n\nemit_rgb = emit_ds.sel(wavelengths=[650, 560, 470], method='nearest')\n\nWe may need to adjust balance the brightness of the selected wavelengths to make a prettier map. This will not affect the data, just the visuals. To do this we will use the function below. We can change the bright argument to increase or decrease the brightness of the scene as a whole. A value of 0.2 usually works pretty well.\n\ndef gamma_adjust(rgb_ds, bright=0.2, white_background=False):\n    array = rgb_ds.reflectance.data\n    gamma = math.log(bright)/math.log(np.nanmean(array)) # Create exponent for gamma scaling - can be adjusted by changing 0.2 \n    scaled = np.power(np.nan_to_num(array,nan=1),np.nan_to_num(gamma,nan=1)).clip(0,1) # Apply scaling and clip to 0-1 range\n    if white_background == True:\n        scaled = np.nan_to_num(scaled, nan = 1) # Set NANs to 1 so they appear white in plots\n    rgb_ds.reflectance.data = scaled\n    return rgb_ds\n\n\nemit_rgb = gamma_adjust(emit_rgb, white_background=True)\n\nNow that we have an RGB dataset, we can use that to create a spatial plot, and data selected by clicking on that ‘map’ can be inputs for a function to return values from the full dataset at that latitude and longitude location using the cell below. To visualize the spectral and spatial data side-by-side, we use the Point Draw tool from the holoviews library.\nDefine a limit to the quantity of points and spectra we will plot, a list of colors to cycle through, and an initial point. Then use the input from the Tap function to provide clicked x and y positions on the map and use these to retrieve spectra from the dataset at those coordinates.\nClick in the RGB image to add spectra to the plot. You can also click and hold the mouse button then drag previously place points. To remove a point click and hold the mouse button down, then press the backspace key.\n\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = emit_rgb.hvplot.rgb(fontscale=1.5, xlabel='Longitude',ylabel='Latitude',frame_width=480, frame_height=480)\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = emit_ds.longitude.values[int(len(emit_ds.longitude) / 2)]\nymid = emit_ds.latitude.values[int(len(emit_ds.latitude) / 2)]\n\nfirst_point = ([xmid], [ymid], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        data = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n                                                                            color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)\n\nWe can take these selected points and the corresponding reflectance spectra and save them as a .csv for later use.\nSelect 10 points by adding to the figure above. We will save these and use them in a to calculate Equivalent Water Thickness or Canopy water content in the next notebook.\nBuild a dictionary of the selected points and spectra, then export the spectra to a .csv file.\n\ndata = points_stream.data\nwavelengths = emit_ds.wavelengths.values\n\nrows = [[\"id\", \"x\", \"y\"] + [str(i) for i in wavelengths]]\n \nfor p in zip(data['x'], data['y'], data['id']):\n    x, y, i = p\n    spectra = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\").reflectance.values\n    row = [i, x, y] + list(spectra)\n    rows.append(row)\n\nWe’ve preselected 10 points, but feel free to uncomment the cell below to use your own. This will overwrite the file containing the preselected points.\n\n# with open('../data/emit_click_data.csv', 'w', newline='') as f:\n#     writer = csv.writer(f)\n#     writer.writerows(rows)\n\n\n\n2.2.3 Cropping EMIT data to a Region of Interest\nTo crop our dataset to our ROI, we first need to open a shapefile of the region. Open the downloaded shapefile of all AOP flight boxes, filter to the Niwot Ridge site (NIWO) and plot it onto the EMIT 850nm reflectance spatial plot. To ensure proper georeferencing of the EMIT scene, be sure to specify the CRS (this is done for the image in the map_opts dictionary).\n\n# shape = gp.read_file(\"../data/dangermond_boundary.geojson\")\naop_flightboxes = gp.read_file(\"./data/AOP_flightBoxes/AOP_flightboxesAllSites.shp\")\nniwo_polygon = aop_flightboxes[aop_flightboxes.siteID == 'NIWO']\nshape = niwo_polygon\n\n\nemit_ds.sel(wavelengths=850, method='nearest').hvplot.image(cmap='viridis',**size_opts,**map_opts,tiles='ESRI')*shape.hvplot(color='#FFFF00',alpha=0.5, crs='EPSG:4326')\n\nNow use the clip function from rasterio to crop the data to our ROI using our shape’s geometry and crs. The all_touched=True argument will ensure all pixels touched by our polygon will be included.\n\nemit_cropped = emit_ds.rio.clip(shape.geometry.values,shape.crs, all_touched=True)\n\nPlot the cropped data.\n\nemit_cropped.sel(wavelengths=850,method='nearest').hvplot.image(cmap='viridis', tiles='ESRI', **size_opts, **map_opts)\n\n\n# Create RGB Map of the cropped dataset\nemit_rgb_cropped = emit_cropped.sel(wavelengths=[650, 560, 470], method='nearest')\nemit_rgb_cropped = gamma_adjust(emit_rgb_cropped,white_background=True)\nemit_rgb_cropped.hvplot.rgb(fontscale=1.5, xlabel='Longitude',ylabel='Latitude',title = 'EMIT RGB Image over Niwot Ridge AOP Flight Box',geo = True, frame_width = 600) #frame_width=560, frame_height=480)\n\n\n\n2.2.4 Write an output\nLastly for our EMIT dataset, we can write a smaller output that we can use in later notebooks, to calculate Canopy water content or other applications. We use the granule_id from the dataset to keep a similar naming convention.\n\n# Write Clipped Output\nemit_cropped.to_netcdf(f'./data/{emit_cropped.granule_id}_NIWO.nc')"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#working-with-neon-hyperspectral-reflectance-data",
    "href": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#working-with-neon-hyperspectral-reflectance-data",
    "title": "2 Working with EMIT L2A Reflectance and NEON L3 Directional Reflectance Data",
    "section": "3.0 Working with NEON Hyperspectral Reflectance Data",
    "text": "3.0 Working with NEON Hyperspectral Reflectance Data\nFor this example we’ll take a look at the Level-3 (1km x 1km tiled) NEON AOP directional surface reflectance data (DP3.30006.001).\nFirst, we’ll define a function to convert from the hdf5 format into xarray so we can look at this data in a similar way to the EMIT data.\n\ndef aop_h5refl2xarray(h5_filename):\n    with h5py.File(h5_filename) as hdf5_file:\n        print('Reading in ', h5_filename)\n        sitename = list(hdf5_file.keys())[0]  # Adjusted to directly access the first key\n        h5_refl_group = hdf5_file[sitename]['Reflectance']\n        refl_dataset = h5_refl_group['Reflectance_Data']\n        refl_array = refl_dataset[()]\n        refl_shape = refl_array.shape\n        wavelengths = h5_refl_group['Metadata']['Spectral_Data']['Wavelength'][:]\n        fwhm = h5_refl_group['Metadata']['Spectral_Data']['FWHM'][:]\n\n        # create dictionary containing metadata information\n        metadata = {}\n        metadata['shape'] = refl_shape\n\n        metadata['no_data_value'] = float(\n            refl_dataset.attrs['Data_Ignore_Value'])\n        metadata['scale_factor'] = float(refl_dataset.attrs['Scale_Factor'])\n\n        # Extract the scale factor & Scale the reflectance data by the scale factor - this is memory intensive though!\n        # Can do it after the fact\n        # scale_factor = float(refl_dataset.attrs['Scale_Factor'])\n        # refl_array = refl_array.astype(float) / scale_factor\n\n        # Extract bad band windows\n        metadata['bad_band_window1'] = (\n            h5_refl_group.attrs['Band_Window_1_Nanometers'])\n        metadata['bad_band_window2'] = (\n            h5_refl_group.attrs['Band_Window_2_Nanometers'])\n\n        # Initialize good_wavelengths array with 1s\n        good_wavelengths = np.ones_like(wavelengths, dtype='float32')\n\n        # Mark wavelengths within the bad band windows as 0\n        for bad_window in [metadata['bad_band_window1'], metadata['bad_band_window2']]:\n            bad_indices = np.where((wavelengths &gt;= bad_window[0]) & (wavelengths &lt;= bad_window[1]))[0]\n            good_wavelengths[bad_indices] = 0\n        good_wavelengths[-10:] = 0 # the last 10 indices also tend to be noisy\n\n        metadata['projection'] = h5_refl_group['Metadata']['Coordinate_System']['Proj4'][()].decode('utf-8')\n        metadata['spatial_ref'] = h5_refl_group['Metadata']['Coordinate_System']['Coordinate_System_String'][()].decode('utf-8')\n        metadata['EPSG'] = int(h5_refl_group['Metadata']\n                               ['Coordinate_System']['EPSG Code'][()])\n        \n        map_info = str(\n            h5_refl_group['Metadata']['Coordinate_System']['Map_Info'][()]).split(\",\")\n        # extract the resolution & convert to floating decimal number\n        pixel_width = float(map_info[5])\n        pixel_height = float(map_info[6])\n        # extract the upper left-hand corner coordinates from mapInfo and cast to float\n        x_min = float(map_info[3])\n        y_max = float(map_info[4])\n        # calculate the xMax and yMin values from the dimensions\n        # xMax = left edge + (# of columns * resolution)\\n\",\n        x_max = x_min + (refl_shape[1]*float(pixel_width))\n        # yMin = top edge - (# of rows * resolution)\\n\",\n        y_min = y_max - (refl_shape[0]*float(pixel_height))\n        metadata['extent'] = (x_min, x_max, y_min, y_max)\n        metadata['ext_dict'] = {}\n        metadata['ext_dict']['x_min'] = x_min\n        metadata['ext_dict']['x_max'] = x_max\n        metadata['ext_dict']['y_min'] = y_min\n        metadata['ext_dict']['y_max'] = y_max\n\n        # Calculate UTM coordinates\n        x_coords = np.linspace(metadata['ext_dict']['x_min'], metadata['ext_dict']['x_max'], num=refl_shape[1])\n        y_coords = np.linspace(metadata['ext_dict']['y_min'], metadata['ext_dict']['y_max'], num=refl_shape[0])\n        \n        # Create the DataArray with coordinates including 'wavelengths','fwhm', & 'good_wavelengths'\n\n        refl_xr = xr.DataArray(refl_array, dims=[\"y\", \"x\", \"wavelengths\"], name=\"reflectance\",\n                       coords={\"x\": (\"x\", x_coords), \"y\": (\"y\", y_coords),\n                               \"wavelengths\": (\"wavelengths\", wavelengths), \n                               \"fwhm\": (\"wavelengths\", fwhm),\n                               \"good_wavelengths\": (\"wavelengths\", good_wavelengths)})\n                                # \"spatial_ref\": spatial_ref}) &lt;&lt; this is added w/ ds.rio.write_crs(\"epsg:32611\", inplace=True)\n        \n        # InvalidDimensionOrder: Invalid dimension order. Expected order: ('wavelengths', 'y', 'x'). You can use `DataArray.transpose('wavelengths', 'y', 'x')` to reorder your dimensions. Data variable: reflectance\n        # refl_xr = refl_xr.transpose('wavelengths', 'y', 'x') # reorder your dimensions. Data variable: reflectance\n        \n        # Create the Dataset\n        ds = xr.Dataset({\"reflectance\": refl_xr})\n\n        # Add metadata as attributes\n        for key, value in metadata.items():\n            if key not in ['shape', 'extent', 'ext_dict']:\n                ds.attrs[key] = value\n\n        # Set 'wavelengths', 'utm_x', and 'utm_y' as indexes\n        ds = ds.set_index(x=\"x\", y=\"y\", wavelengths=\"wavelengths\")\n\n        return ds\n\n\nh5_filepath = r\"./data/neon_refl\"\nh5_filename = \"NEON_D13_NIWO_DP3_454000_4431000_reflectance.h5\"\naop_refl_ds = aop_h5refl2xarray(os.path.join(h5_filepath,h5_filename))\n\n\naop_refl_ds\n\n\naop_refl_ds.attrs['EPSG']\n\n\n# add spatial reference information to Coordinates as follows (to create an rioxarray)\naop_refl_ds.rio.write_crs(f\"epsg:{aop_refl_ds.attrs['EPSG']}\", inplace=True)\n\nNext we can run a couple more pre-processing steps, of 1. scaling the reflectance data by the scale factor (NEON data are saved in an integer format, scaled by 10000, in order to save on space) 2. setting the water vapor absorption windows (defined as “bad band windows”) to NaN. Similar to the EMIT data, “good_wavelengths” are provided as one of the Coordinates in the aop_refl_ds xarray dataset, so we can use that information to keep only the valid wavelengths.\n\n# scale by the reflectance scale factor\naop_refl_ds['reflectance'].data = aop_refl_ds['reflectance'].data/aop_refl_ds.attrs['scale_factor']\n\n\n# set \"bad bands\" (water vapor absorption bands) to NaN\naop_refl_ds['reflectance'].data[:,:,aop_refl_ds['good_wavelengths'].data==0.0] = np.nan\n\n\naop_tile_center = aop_refl_ds.x.values[int(len(aop_refl_ds.x)/2)],aop_refl_ds.y.values[int(len(aop_refl_ds.y)/2)]\nprint('AOP tile center',aop_tile_center)\n\n\n# aop_spectra = aop_refl_ds.sel(x=aop_tile_center[0],y=aop_tile_center[1], method='nearest')\naop_point = (454495,4431420) # solar panels on MRS building roof\n# aop_point = aop_tile_center\naop_spectra = aop_refl_ds.sel(x=aop_point[0],y=aop_point[1], method='nearest')\naop_spectra.hvplot.line(y='reflectance',x='wavelengths', color='black').opts(\n    title=f'x = {aop_spectra.x.values.round(1)}, y = {aop_spectra.y.values.round(1)}')\n\n\naop_refl_ds.hvplot.image(x='x',y='y',**size_opts, cmap='viridis', clim = (0,0.5), tiles='ESRI', xlabel='Longitude',ylabel='Latitude',title='NEON AOP Reflectance', crs='EPSG:4326')\n\n\n# Re-project\n# aop_refl_ds.rio.reproject('EPSG:4326').hvplot.image(x='x',y='y',**size_opts, cmap='inferno',tiles='ESRI', xlabel='Longitude',ylabel='Latitude',title='NEON AOP Reflectance', crs='EPSG:4326')\n\n\n# Plot the RGB image of the NIWO tile\naop_rgb = aop_refl_ds.sel(wavelengths=[650, 560, 470], method='nearest')\naop_rgb = gamma_adjust(aop_rgb,white_background=True)\naop_rgb.hvplot.rgb(x='x',y='y',bands='wavelengths',xlabel='UTM x',ylabel='UTM y',title='NEON AOP Reflectance RGB',frame_width=480, frame_height=480)\n\n\n# emit_rgb = gamma_adjust(emit_rgb,white_background=True)\n# Interactive Points Plotting\n# Modified from https://github.com/auspatious/hyperspectral-notebooks/blob/main/03_EMIT_Interactive_Points.ipynb\nPOINT_LIMIT = 10\ncolor_cycle = hv.Cycle('Category20')\n\n# Create RGB Map\nmap = aop_rgb.hvplot.rgb(x='x',y='y',bands='wavelengths',fontscale=1.5, xlabel='UTM x',ylabel='UTM y',frame_width=480, frame_height=480)\n\n# Set up a holoviews points array to enable plotting of the clicked points\nxmid = aop_refl_ds.x.values[int(len(aop_refl_ds.x) / 2)]\nymid = aop_refl_ds.y.values[int(len(aop_refl_ds.y) / 2)]\n\nx0 = aop_refl_ds.x.values[0]\ny0 = aop_refl_ds.y.values[0]\n\n# first_point = ([xmid], [ymid], [0])\nfirst_point = ([x0], [y0], [0])\npoints = hv.Points(first_point, vdims='id')\npoints_stream = hv.streams.PointDraw(\n    data=points.columns(),\n    source=points,\n    drag=True,\n    num_objects=POINT_LIMIT,\n    styles={'fill_color': color_cycle.values[1:POINT_LIMIT+1], 'line_color': 'gray'}\n)\n\nposxy = hv.streams.PointerXY(source=map, x=xmid, y=ymid)\nclickxy = hv.streams.Tap(source=map, x=xmid, y=ymid)\n\n# Function to build spectral plot of clicked location to show on hover stream plot\ndef click_spectra(data):\n    coordinates = []\n    if data is None or not any(len(d) for d in data.values()):\n        coordinates.append(clicked_points[0][0], clicked_points[1][0])\n    else:\n        coordinates = [c for c in zip(data['x'], data['y'])]\n    \n    plots = []\n    for i, coords in enumerate(coordinates):\n        x, y = coords\n        # data = emit_ds.sel(longitude=x, latitude=y, method=\"nearest\")\n        data = aop_refl_ds.sel(x=x, y=y, method=\"nearest\")\n        plots.append(\n            data.hvplot.line(\n                y=\"reflectance\",\n                x=\"wavelengths\",\n                color=color_cycle,\n                label=f\"{i}\"\n            )\n        )\n        points_stream.data[\"id\"][i] = i\n    return hv.Overlay(plots)\n\ndef hover_spectra(x,y):\n    return aop_refl_ds.sel(x=x,y=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths', color='black', frame_width=400)\n    # return emit_ds.sel(longitude=x,latitude=y,method='nearest').hvplot.line(y='reflectance',x='wavelengths',\n    #                                                                         color='black', frame_width=400)\n# Define the Dynamic Maps\nclick_dmap = hv.DynamicMap(click_spectra, streams=[points_stream])\nhover_dmap = hv.DynamicMap(hover_spectra, streams=[posxy])\n# Plot the Map and Dynamic Map side by side\nhv.Layout(hover_dmap*click_dmap + map * points).cols(2).opts(\n    hv.opts.Points(active_tools=['point_draw'], size=10, tools=['hover'], color='white', line_color='gray'),\n    hv.opts.Overlay(show_legend=False, show_title=False, fontscale=1.5, frame_height=480)\n)"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#contact-info",
    "href": "community_contributed/NEON_EMIT/02_Exploring_NEON_and_EMIT_Reflectance_Data_NIWO.html#contact-info",
    "title": "2 Working with EMIT L2A Reflectance and NEON L3 Directional Reflectance Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nLand Processes Distributed Active Archive Center (LP DAAC)1\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nWebsite: https://lpdaac.usgs.gov/\n1Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.\nNational Ecological Observatory Network (NEON)2\nWebsite: https://www.neonscience.org/\nContact: https://www.neonscience.org/about/contact-us\nDate last modified: 08-27-2024\n2NEON is a project sponsored by the National Science Foundation and operated by Battelle."
  },
  {
    "objectID": "CODE_OF_CONDUCT.html",
    "href": "CODE_OF_CONDUCT.html",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.\n\n\n\nExamples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\n\n\n\nCommunity leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.\n\n\n\nThis Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.\n\n\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.\n\n\n\nCommunity leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.\n\n\n\n\nThis Code of Conduct is adapted from the Zarr Developers, available at https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-pledge",
    "href": "CODE_OF_CONDUCT.html#our-pledge",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.\nWe pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#our-standards",
    "href": "CODE_OF_CONDUCT.html#our-standards",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Examples of behavior that contributes to a positive environment for our community include:\n\nDemonstrating empathy and kindness toward other people\nBeing respectful of differing opinions, viewpoints, and experiences\nGiving and gracefully accepting constructive feedback\nAccepting responsibility and apologizing to those affected by our mistakes, and learning from the experience\nFocusing on what is best not just for us as individuals, but for the overall community\n\nExamples of unacceptable behavior include:\n\nThe use of sexualized language or imagery, and sexual attention or advances of any kind\nTrolling, insulting or derogatory comments, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or email address, without their explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "href": "CODE_OF_CONDUCT.html#enforcement-responsibilities",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.\nCommunity leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#scope",
    "href": "CODE_OF_CONDUCT.html#scope",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement",
    "href": "CODE_OF_CONDUCT.html#enforcement",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at LPDAAC@usgs.gov. All complaints will be reviewed and investigated promptly and fairly.\nAll community leaders are obligated to respect the privacy and security of the reporter of any incident.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "href": "CODE_OF_CONDUCT.html#enforcement-guidelines",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:\n\n\nCommunity Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.\nConsequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.\n\n\n\nCommunity Impact: A violation through a single incident or series of actions.\nConsequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.\n\n\n\nCommunity Impact: A serious violation of community standards, including sustained inappropriate behavior.\nConsequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.\n\n\n\nCommunity Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.\nConsequence: A permanent ban from any sort of public interaction within the community.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "CODE_OF_CONDUCT.html#attribution",
    "href": "CODE_OF_CONDUCT.html#attribution",
    "title": "Contributor Code of Conduct",
    "section": "",
    "text": "This Code of Conduct is adapted from the Zarr Developers, available at https://github.com/zarr-developers/.github/blob/main/CODE_OF_CONDUCT.md and from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.\nCommunity Impact Guidelines were inspired by Mozilla’s code of conduct enforcement ladder.\nFor answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.",
    "crumbs": [
      "Contributing",
      "Contributor Code of Conduct"
    ]
  },
  {
    "objectID": "2025_sbg_workshop.html",
    "href": "2025_sbg_workshop.html",
    "title": "Surface Biology and Geology 2025 Workshop: Advancing Capacity for SBG Data Users with Spaceborne and Airborne Precursor Data",
    "section": "",
    "text": "This workshop is part of an ongoing effort to build capacity within the Surface Biology and Geology (SBG) community by equipping users with knowledge, tools, and resources to effectively engage with precursor datasets and technologies. Specifically, it focuses on increasing awareness and utilization of airborne and spaceborne assets of opportunity such as EMIT, ECOSTRESS, and SBG-like airborne data and empowering users to develop scientific workflows and application-driven use cases that are relevant to the upcoming SBG mission.\nThe International Space Station is a critical asset for the Earth science community — both for advancing critical science and applications priorities, and as a platform for technology demonstrations/pathfinder missions. This is exemplified by the success of instruments like ECOSTRESS, a multispectral thermal radiometer, and EMIT, a visible to short wave infrared imaging spectrometer with best-in-class signal to noise. Operating at field-scale resolutions (&lt;70 m), these instruments provide complementary datasets that enable rich analyses of surface processes and environmental change. With both sensors co-located on the ISS, we now have a unique opportunity to explore the synergistic value of combined thermal and imaging spectroscopy data. In addition to these spaceborne assets, airborne campaigns such as Surface Biology and Geology High-Frequency Time Series (SHIFT) are collecting high-resolution thermal and imaging spectroscopy data that are directly comparable to future SBG mission products. These datasets present valuable opportunities to prototype analyses, develop scalable workflows, and explore science and applications that can inform future SBG capabilities.\nThe overarching goal of this workshop is to engage and empower the SBG data user community by facilitating data discovery and access. A key focus will be demonstrating how to locate and work with concurrent EMIT and ECOSTRESS observations that align with existing airborne data, enabling participants to build integrated workflows and prepare for future SBG mission data.\nThis workshop is hosted by NASA Land Processes Distributed Active Archive Center(LP DAAC) and NASA Jet Propulsion Laboratory (JPL) with support from the NASA Openscapes project. Hands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance. Enter your username and password (SBGTIM2025) to login and select 14.8 GB RAM, upto 3.75 CPUs Python Resource Allocation.",
    "crumbs": [
      "Welcome",
      "2025 SBG Workshop"
    ]
  },
  {
    "objectID": "2025_sbg_workshop.html#contact-info",
    "href": "2025_sbg_workshop.html#contact-info",
    "title": "Surface Biology and Geology 2025 Workshop: Advancing Capacity for SBG Data Users with Spaceborne and Airborne Precursor Data",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://www.earthdata.nasa.gov/centers/lp-daac\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Welcome",
      "2025 SBG Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html",
    "href": "2023_agu_workshop.html",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "",
    "text": "The International Space Station is a critical asset for the Earth science community – both for advancing critical science and applications priorities, and as a platform for technology demonstrations/pathfinders. These benefits have been particularly significant in recent years, with the installation and operation of instruments such as ECOSTRESS, a multispectral thermal instrument, and EMIT, a visible to short wave infrared imaging spectrometer with best-in-class signal to noise - both acquiring data at field-scale (&lt;70-m). With both sensors mounted on the ISS, there is an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this workshop we highlight the power of these tools when used together, through the use of open source tools and services, cloud compute resources to effectively combine data from ECOSTRESS and EMIT to perform scientific analyses and apply data to real world issues.\nThis workshop is hosted by NASA Land Processes Distributed Active Archive Center(LP DAAC) and NASA Jet Propulsion Laboratory (JPL) with support from the NASA Openscapes project.\nHands-on exercises will be executed from a Jupyter Hub on the Openscapes 2i2c cloud instance.",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#slides",
    "href": "2023_agu_workshop.html#slides",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Slides",
    "text": "Slides",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#agenda",
    "href": "2023_agu_workshop.html#agenda",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Agenda",
    "text": "Agenda\n\n\n\nTime\nDescription\nLeads/Instructors\n\n\n\n\n8:00-8:30 AM\nIntroduction, overview, learning objectives\nDana Chadwick  Madeleine Pascolini-Campbell\n\n\n8:30-9:30 AM\nFinding EMIT and ECOSTRESS Data:  - Earthdata Search  - AppEEARS  - VISIONS\nAaron Friesz  Dana Chadwick\n\n\n9:30-9:45 AM\nBreak\n\n\n\n9:45-10:05 AM\nIntroduction to Cloud Computing Environment\nAaron Friesz\n\n\n10:05-10:50 AM\nFinding Concurrent EMIT and ECOSTRESS Data\nErik Bolch\n\n\n10:50-11:05 AM\nBreak\n\n\n\n11:05-12:00 PM\nExploring EMIT Reflectance and ECOSTRESS LST\nErik Bolch\n\n\n12:00-1:00 PM\nLunch Break\n\n\n\n1:00-2:00 PM\nEstimating Canopy Water Content\nDana Chadwick  Niklas Bohn  Erik Bolch\n\n\n2:00-2:30 PM\nUse Case 1: Dangermond Preserve Land Cover\nChristiana Ade  Marie Johnson\n\n\n2:30-3:00 PM\nUse Case 2: Santa Barbara Agriculture\nClaire Villanueva-Weeks  Gregory Halverson\n\n\n3:00-3:30 PM\nDiscussion, feedback, resources to take home\nAll",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#learning-outcomes",
    "href": "2023_agu_workshop.html#learning-outcomes",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Learning Outcomes:",
    "text": "Learning Outcomes:\n\nImaging Spectroscopy and thermal measurements 101, the electromagnetic spectrum and sensor specific considerations\n\nHow to access EMIT and ECOSTRESS data\n\nData Preprocessing and Exploratory Analysis\n\nHow to manipulate, combine, and visualize EMIT and ECOSTRESS data",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#learning-focus",
    "href": "2023_agu_workshop.html#learning-focus",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Learning Focus:",
    "text": "Learning Focus:\nPractical Skills for Science",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#knowledge-career-level",
    "href": "2023_agu_workshop.html#knowledge-career-level",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Knowledge & Career Level:",
    "text": "Knowledge & Career Level:\nBeginner, Intermediate",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#target-audience",
    "href": "2023_agu_workshop.html#target-audience",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Target Audience:",
    "text": "Target Audience:\n\nEarth and Planetary Surface Processes\nHydrology\nGlobal Environmental Change\nOcean Sciences\nScience and Society\nBiogeosciences",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "2023_agu_workshop.html#contact-info",
    "href": "2023_agu_workshop.html#contact-info",
    "title": "Space Station Synergies: Applying ECOSTRESS and EMIT to ecological problems for Scientific Insight",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 12-05-2023\n¹Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Welcome",
      "2023 AGU Workshop"
    ]
  },
  {
    "objectID": "CHANGE_LOG.html",
    "href": "CHANGE_LOG.html",
    "title": "Change Log",
    "section": "",
    "text": "All notable changes to this project will be documented in this file. The format is based on Keep a Changelog and this project adheres to Semantic Versioning. _________________________________________________________________________\n\n\n\n\n\nChange name of ‘user_contributed’ directory to ‘community_contributed’ Updated contribute.md to reflect the change\n\n\n\n\n\n\n\n\n\nAdded community contributed notebooks and associated figures:\n\n01_Finding_Co-located_NEON_and_EMIT_Data_NIWO.ipynb\n02_Exploring_NEON_and_EMIT Reflectance_Data_NIWO.ipynb\n\n\n\n\n\n\n\n\n\n\n\nAdd streaming functionality to notebooks 2 and 3\nAdd ECOSTRESS ET example to notebook 2\nUpdate some text\nAdd SBG workshop contents\nminor changes to notebook 4\nupdate to newest emit_tools.py\n\n\n\n\n\n\n\n\n\n\nFixed some typos\nResize slides in slides.md\n\n\n\n\n\n\n\n\n\nUpdated earthaccess search to use concept-id for Notebook 1\nImplemented downloading of required scenes for Notebooks 2-5 into a cell in Notebook 1\nCorrected projection/crs arguments for plotting of EMIT and ECOSTRESS imagery\nMade minor changes to Notebook 5 to fix cloud/local compatibility issues\nFixed implementation of ewt_detection_limit threshold in ewt_calc.py\nImproved description of ewt_detection_limit in Notebook 3\nmisc typos/syntax fixes\n\n\n\nLocal environment support and setup instructions\n\nAdded EWT and ET cloud-optimized GeoTIFFs generated for the workshop to repository\nAdded list of required granules/scenes to execute notebooks\nAdded workshop slides.md\n\n\n\n\n\n\n\n\nImproved Finding Concurrent Data Notebook text/instructions\nRenamed contribute.md\nadded repo description\n\n\n\nRepository description\n\n\n\n\n\n\n\nUpdated notebook ROI to Carpinteria Salt Marsh\n\n\nAdded landcover.geojson\n\n\n\n\n\n\n\nUpdated contribute.md and added user contributed directory\n\n\nAdded user_contributed directory\n\n\n\n\n\n\n\nThis is the first update.\n\n\nFinding Concurrent Data Notebook"
  },
  {
    "objectID": "CHANGE_LOG.html#section",
    "href": "CHANGE_LOG.html#section",
    "title": "Change Log",
    "section": "",
    "text": "Change name of ‘user_contributed’ directory to ‘community_contributed’ Updated contribute.md to reflect the change"
  },
  {
    "objectID": "CHANGE_LOG.html#section-1",
    "href": "CHANGE_LOG.html#section-1",
    "title": "Change Log",
    "section": "",
    "text": "Added community contributed notebooks and associated figures:\n\n01_Finding_Co-located_NEON_and_EMIT_Data_NIWO.ipynb\n02_Exploring_NEON_and_EMIT Reflectance_Data_NIWO.ipynb"
  },
  {
    "objectID": "CHANGE_LOG.html#section-2",
    "href": "CHANGE_LOG.html#section-2",
    "title": "Change Log",
    "section": "",
    "text": "Add streaming functionality to notebooks 2 and 3\nAdd ECOSTRESS ET example to notebook 2\nUpdate some text\nAdd SBG workshop contents\nminor changes to notebook 4\nupdate to newest emit_tools.py"
  },
  {
    "objectID": "CHANGE_LOG.html#section-3",
    "href": "CHANGE_LOG.html#section-3",
    "title": "Change Log",
    "section": "",
    "text": "Fixed some typos\nResize slides in slides.md"
  },
  {
    "objectID": "CHANGE_LOG.html#section-4",
    "href": "CHANGE_LOG.html#section-4",
    "title": "Change Log",
    "section": "",
    "text": "Updated earthaccess search to use concept-id for Notebook 1\nImplemented downloading of required scenes for Notebooks 2-5 into a cell in Notebook 1\nCorrected projection/crs arguments for plotting of EMIT and ECOSTRESS imagery\nMade minor changes to Notebook 5 to fix cloud/local compatibility issues\nFixed implementation of ewt_detection_limit threshold in ewt_calc.py\nImproved description of ewt_detection_limit in Notebook 3\nmisc typos/syntax fixes\n\n\n\nLocal environment support and setup instructions\n\nAdded EWT and ET cloud-optimized GeoTIFFs generated for the workshop to repository\nAdded list of required granules/scenes to execute notebooks\nAdded workshop slides.md"
  },
  {
    "objectID": "CHANGE_LOG.html#section-5",
    "href": "CHANGE_LOG.html#section-5",
    "title": "Change Log",
    "section": "",
    "text": "Improved Finding Concurrent Data Notebook text/instructions\nRenamed contribute.md\nadded repo description\n\n\n\nRepository description"
  },
  {
    "objectID": "CHANGE_LOG.html#section-6",
    "href": "CHANGE_LOG.html#section-6",
    "title": "Change Log",
    "section": "",
    "text": "Updated notebook ROI to Carpinteria Salt Marsh\n\n\nAdded landcover.geojson"
  },
  {
    "objectID": "CHANGE_LOG.html#section-7",
    "href": "CHANGE_LOG.html#section-7",
    "title": "Change Log",
    "section": "",
    "text": "Updated contribute.md and added user contributed directory\n\n\nAdded user_contributed directory"
  },
  {
    "objectID": "CHANGE_LOG.html#section-8",
    "href": "CHANGE_LOG.html#section-8",
    "title": "Change Log",
    "section": "",
    "text": "This is the first update.\n\n\nFinding Concurrent Data Notebook"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "",
    "text": "Summary\nThe Earth surface Mineral dust source InvesTigation (EMIT) instrument is located on the International Space Station (ISS) and has collected data over a large area of the Continental US. The National Ecological Observatory Network (NEON) Airborne Observation Platform (AOP) collects aerial remote sensing data, including hyperspectral reflectance data over sites across the United States and Puerto Rico. In this notebook we will show how to utilize the earthaccess Python library to find spatially overlapping EMIT and NEON reflectance data at NEON’s Niwot Ridge site (NIWO) in the Rocky Mountains of Colorado.\nBackground\nThe EMIT instrument is an imaging spectrometer that measures light in visible (V) to short-wave (SWIR) infrared wavelengths; this is also referred to as a VSWIR sensor. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nThe NEON Imaging Spectrometer (NIS) is an airborne imaging spectrometer built by JPL (AVIRIS-NG) and operated by the National Ecological Observatory Network’s (NEON) Airborne Observation Platform (AOP). NEON’s hyperspectral sensors collect measurements of sunlight reflected from the Earth’s surface in 426 narrow (~5 nm) spectral channels spanning wavelengths between ~ 380 - 2500 nm. NEON’s remote sensing data is intended to map and answer questions about a landscape, with ecological applications including identifying and classifying plant species and communities, mapping vegetation health, detecting disease or invasive species, and mapping droughts, wildfires, or other natural disturbances and their impacts.\nNEON surveys sites spanning the continental US, during peak phenological greenness, capturing each site 3 out of every 5 years, for most terrestrial sites. AOP’s Flight Schedules and Coverage provide’s more information about the current and past schedules.\nMore detailed information about NEON’s airborne sampling design can be found in the paper: Spanning scales: The airborne spatial and temporal sampling design of the National Ecological Observatory Network.\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only Set up Python Environment - See setup_instructions.md in the /setup/ folder to set up a local compatible Python environment\nDownload the NEON Flight Boundary Shapefile: AOP_flightBoxes.zip\nLearning Objectives\n- Use functions provided in an external Python module to find and download available NEON airborne reflectance data. - Use earthaccess to find EMIT data that overlaps with a NEON site. - How to export a list of files and download them programmatically.\nTutorial Outline"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#setup",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#setup",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required Python libraries.\n\n# Import required libraries\nimport os, sys\nimport folium\nimport earthaccess\nimport warnings\nimport folium.plugins\nimport pandas as pd\nimport geopandas as gpd\nimport math\nimport requests\n\nfrom zipfile import ZipFile\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom shapely import geometry\nfrom skimage import io\nfrom datetime import timedelta\nfrom shapely.geometry.polygon import orient\nfrom matplotlib import pyplot as plt\n\n\n1.2 NEON Data API and Python Functions\nNote: In Sept-Oct 2024, these next two chunks of code can be replaced with the Python neonUtilities package, which has built in functions for downloading NEON AOP data.\n\n# function to download data stored on the internet in a public url to a local file\ndef download_url(url,download_dir):\n    if not os.path.isdir(download_dir):\n        os.makedirs(download_dir)\n    filename = url.split('/')[-1]\n    r = requests.get(url, allow_redirects=True)\n    file_object = open(os.path.join(download_dir,filename),'wb')\n    file_object.write(r.content)\n\n\nneon_code_folder = './neon_python_modules'\naop_download_module_url = \"https://raw.githubusercontent.com/NEONScience/NEON-Data-Skills/main/tutorials/Python/AOP/aop_python_modules/neon_aop_download_functions.py\"\ndownload_url(aop_download_module_url,neon_code_folder)\n#os.listdir(neon_code_folder) #optionally show the contents of this directory to confirm the files downloaded\n\n# add the code folder to the path and import the neon aop download functions module\nsys.path.insert(0,neon_code_folder)\nimport neon_aop_download_functions as aop_dl;\n\n\n\n1.3 NASA Earthdata Login Credentials\nTo download or stream NASA data you will need an Earthdata account, you can create one here. We will use the login function from the earthaccess library for authentication before downloading at the end of the notebook. This function can also be used to create a local .netrc file if it doesn’t exist or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. This step is not necessary to conduct searches but is needed to download or stream data."
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#search-for-neon-and-emit-data",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#search-for-neon-and-emit-data",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "2. Search for NEON and EMIT Data",
    "text": "2. Search for NEON and EMIT Data\nNEON data products are hosted on the NEON Data Portal, and can be accessed via an API. We will import a Python module including some functions that interact with the NEON data API to easily see what data are available (in what years), and download data.\nThe EMIT products are hosted by the Land Processes Distributed Active Archive Center (LP DAAC). In this example we will use the cloud-hosted EMIT_L2A_RFL and ECOSTRESS_L2T_LSTE products available from the LP DAAC to find data. Any results we find for these products, should be available for other products within the EMIT and ECOSTRESS collections.\nTo find data we will use the earthaccess Python library. earthaccess searches NASA’s Common Metadata Repository (CMR), a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists of granule search result URLs.\nUsing earthaccess we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (ex: Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection). We can search using attributes such as collection, acquisition time, and spatial footprint. This process can also be used with other EMIT or ECOSTRESS products, other collections, or different data providers, as well as across multiple catalogs with some modification.\n\n2.1 Define Spatial Regions of Interest (ROIs)\nFor this example, our spatial region of interest (ROI) will be the NEON site Niwot Ridge (NIWO) in the Rocky Mountains, Colorado.\nIn this example, we will create a rectangular ROI surrounding the NIWO flight box. We will search for co-located EMIT data using a polygon rather than a standard bounding box in earthaccess. To search for intersections with a polygon using earthaccess, we need to format our ROI as a counterclockwise list of coordinate pairs.\nDownload, Unzip, and Open the shape file (.shp) containing the AOP flight box boundaries, which can be downloaded from NEON Spatial Data and Maps. Read this into a geodataframe, explore the contents, and check the coordinate reference system (CRS) of the data.\n\n# Download and Unzip the NEON Flight Boundary Shapefile\nneon_boundary_url = \"https://www.neonscience.org/sites/default/files/AOP_flightBoxes_0.zip\"\n# Use download_url function to save the file to a directory\nos.makedirs('./data', exist_ok=True)\ndownload_url(neon_boundary_url,'./data')\n# Unzip the file\nwith ZipFile(f\"./data/{neon_boundary_url.split('/')[-1]}\", 'r') as zip_ref:\n    zip_ref.extractall('./data')\n\n\naop_flightboxes = gpd.read_file(\"./data/AOP_flightBoxes/AOP_flightboxesAllSites.shp\")\naop_flightboxes.head()\n\n\n\n\n\n\n\n\ndomain\ndomainName\nsiteName\nsiteID\nsiteType\nsampleType\npriority\nversion\nflightbxID\ngeometry\n\n\n\n\n0\nD01\nNortheast\nBartlett Experimental Forest NEON\nBART\nGradient\nTerrestrial\n1\n1\nD01_BART_R1_P1_v1\nPOLYGON ((-71.33426 43.99197, -71.33423 44.081...\n\n\n1\nD01\nNortheast\nHarvard Forest & Quabbin Watershed NEON\nHARV\nCore\nTerrestrial\n1\n1\nD01_HARV_C1_P1_v1\nPOLYGON ((-72.14819 42.57510, -72.14776 42.383...\n\n\n2\nD01\nNortheast\nHarvard Forest & Quabbin Watershed NEON\nHARV\nCore\nTerrestrial\n3\n1\nD01_HARV_C1_P3_v1\nPOLYGON ((-72.10812 42.43653, -72.14788 42.436...\n\n\n3\nD01\nNortheast\nLower Hop Brook NEON\nHOPB\nCore\nAquatic\n2\n1\nD01_HOPB_C1_P2_v1\nPOLYGON ((-72.36635 42.46399, -72.36635 42.514...\n\n\n4\nD19\nTaiga\nHealy NEON\nHEAL\nGradient\nTerrestrial\n1\n1\nD19_HEAL_R3_P1_v1\nPOLYGON ((-149.31505 63.82981, -149.31505 63.9...\n\n\n\n\n\n\n\n\naop_flightboxes.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe CRS is EPSG:4326 (WGS84), which is also the CRS we want the data in to submit for our search of EMIT data.\nNext, let’s examine the AOP flightboxes polygons further. For this exercise, we’ll first look at Niwot Ridge site in Colorado.\n\naop_flightboxes[aop_flightboxes.siteID == 'NIWO']\n\n\n\n\n\n\n\n\ndomain\ndomainName\nsiteName\nsiteID\nsiteType\nsampleType\npriority\nversion\nflightbxID\ngeometry\n\n\n\n\n49\nD13\nSouthern Rockies & Colorado Plateau\nNiwot Ridge NEON\nNIWO\nCore\nTerrestrial\n1\n2\nD13_NIWO_C1_P1_v2\nPOLYGON ((-105.48908 40.07292, -105.48912 39.9...\n\n\n\n\n\n\n\nWe can see the NIWO geodataframe consists of a single polygon, that we want to include in our study site (sometimes NEON sites may have more than one polygon, as there are sometimes multiple areas, with different priorities for collection).\n\n# write this to a new variable called \"niwo_polygon\"\nniwo_polygon = aop_flightboxes[aop_flightboxes.siteID == 'NIWO']\n# subset to only include columns of interest\nniwo_polygon = niwo_polygon[['domain','siteName','siteID','sampleType','flightbxID','priority','geometry']]\n\n\n# Create external boundary of the shape\nniwo_roi_poly = niwo_polygon.unary_union.envelope\n# Re-order vertices to counterclockwise\nniwo_roi_poly = orient(niwo_roi_poly, sign=1.0)\n\nMake a GeoDataFrame consisting of the bounding box geometry.\n\nniwo_df = pd.DataFrame({\"Name\":[\"NIWO ROI Bounding Box\"]})\nniwo_bbox = gpd.GeoDataFrame({\"Name\":[\"NIWO ROI Bounding Box\"], \"geometry\":[niwo_roi_poly]},crs=\"EPSG:4326\")\nniwo_bbox\n\n\n\n\n\n\n\n\nName\ngeometry\n\n\n\n\n0\nNIWO ROI Bounding Box\nPOLYGON ((-105.64789 39.98286, -105.48908 39.9...\n\n\n\n\n\n\n\nWe can write this bounding box to a geojson file for use in future notebooks. This is commented out for now, but you can uncomment and run the cell below, if desired.\n\n#niwo_bbox.to_file('../data/niwo_bbox.geojson', driver='GeoJSON')\n\nNext we can visualize our region of interest and the exterior boundary polygon containing ROIs. First add a function to help reformat bounding box coordinates to work with leaflet notation.\n\n# Function to convert a bounding box for use in leaflet notation\ndef convert_bounds(bbox, invert_y=False):\n    \"\"\"\n    Helper method for changing bounding box representation to leaflet notation\n\n    ``(lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))``\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add NIWO Bounding Box\nfolium.GeoJson(niwo_bbox, name='bounding_box').add_to(map1)\n\n# Add roi geodataframe\nniwo_polygon.explore(\"flightbxID\",\n                     popup=True,\n                     categorical=True,\n                     cmap='Set3',\n                     style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n                     name=\"Niwot Ridge ROI\",\n                     m=map1)\n\nmap1.add_child(folium.LayerControl())\nmap1.fit_bounds(bounds=convert_bounds(niwo_polygon.unary_union.bounds))\ndisplay(fig)\n\n\n\n\nAbove we can see the Niwot Ridge flightbox, and the exterior boundary polygon containing the full area.\nLastly, we need to convert our polygon to a list of coordinate pairs, to create our Region of Interest (ROI).\n\n# Set ROI as list of exterior polygon vertices as coordinate pairs\nniwo_roi = list(niwo_roi_poly.exterior.coords)\nniwo_roi\n\n[(-105.64788824641289, 39.98286247719818),\n (-105.48907581013356, 39.98286247719818),\n (-105.48907581013356, 40.07295974374222),\n (-105.64788824641289, 40.07295974374222),\n (-105.64788824641289, 39.98286247719818)]\n\n\nFinally we can look at the available NEON hyperspectral reflectance data over NIWO. NEON hyperspectral reflectance data are currently available under two different revisions, as AOP is in the process of implementing a BRDF (Bidirectional Reflectance Distribution Function), but this has not been applied to the full archive of data yet. These data are available under two revisions of the data product ID DP3.30006 - DP3.30006.001 are the directional surface reflectance, and DP3.30006.002 are the bidirectional (BRDF- and topographic- corrected) surface reflectance. Let’s see what’s available for each of these data products.\n\nrefl_rev1_dpid = 'DP3.30006.001'\nrefl_rev2_dpid = 'DP3.30006.002'\nsite = 'NIWO'\n\n\nprint('Directional Reflectance Data Available at NEON Site NIWO')\naop_dl.list_available_urls(refl_rev1_dpid,site)\n\nDirectional Reflectance Data Available at NEON Site NIWO\n\n\n['https://data.neonscience.org/api/v0/data/DP3.30006.001/NIWO/2017-09',\n 'https://data.neonscience.org/api/v0/data/DP3.30006.001/NIWO/2018-08',\n 'https://data.neonscience.org/api/v0/data/DP3.30006.001/NIWO/2019-08',\n 'https://data.neonscience.org/api/v0/data/DP3.30006.001/NIWO/2020-08']\n\n\n\nprint('Bidirectional Reflectance Data Available at NEON Site NIWO')\naop_dl.list_available_urls(refl_rev2_dpid,site)\n\nBidirectional Reflectance Data Available at NEON Site NIWO\nWARNING: no urls found for product DP3.30006.002 at site NIWO\n\n\nThe bidirectional data for the later visits of NIWO are not yet available. We’ll start by looking at the directional surface reflectance for NIWO in 2020.\n\nyear = '2020'\n\nWe can download the reflectance data using the function aop_dl.download_aop_files. You can change the download path if desired.\nBefore we download the data, let’s look at the spatial extent. AOP data are provided in UTM projection. We can do this by downloading shapefiles of the tile boundaries. These are available as metadata provided along with the reflectance data products. The full boundary of the site is a file called “merged_tiles.shp/.shx”. Let’s only download this shapefile so we can see the file extent, and determine the UTM coordinates of the tiles we wish to download.\n\n# aop_dl.download_aop_files?\n\n\n# download the full-boundary shape files from the lidar data. These are delivered as metadata as part of one of the Lidar data products, so note that the data product id (DPID) is different\naop_dl.download_aop_files('DP1.30003.001',site,year,'./data/neon_refl',match_string='merged_tiles.shp',check_size=False)\naop_dl.download_aop_files('DP1.30003.001',site,year,'./data/neon_refl',match_string='merged_tiles.shx',check_size=False)\n\nDownload size: 0.61 kB\ndownloading 2020_NIWO_4_merged_tiles.shp to ./data/neon_refl\nDownload size: 0.0 kB\ndownloading 2020_NIWO_4_merged_tiles.shx to ./data/neon_refl\n\n\n\n# optionally display the contents in the data/neon_refl folder to make sure the files have downloaded\nos.listdir('./data/neon_refl')\n\n['2020_NIWO_4_merged_tiles.shp',\n '2020_NIWO_4_merged_tiles.shx',\n 'NEON_D13_NIWO_DP3_454000_4431000_reflectance.h5']\n\n\n\nniwo_2020_gdf = gpd.read_file(os.path.join('./data/neon_refl','2020_NIWO_4_merged_tiles.shp'))\nniwo_2020_gdf.plot(alpha=0.5);\nax = plt.gca(); ax.ticklabel_format(style='plain') \nax.set_title('AOP Coverage of ' + site + ' in ' + year);\nplt.xticks(rotation=90); #optionally rotate the xtick labels\n\n\n\n\n\n\n\n\nThe reflectance data can be large in size, so for now, we’ll just download a single tile, which encompasses the CU Boulder Mountain Research Station at Niwot Ridge. We can do that with the download_aop_files function as follows. This time leave out the check_size input parameter, and that will default to True. This will prompt you to download after displaying the download size. This reflectance file is ~615 MB, so make sure you have enough space on your local disk before downloading.\n\n# download a reflectance hdf5 tile\naop_dl.download_aop_files('DP3.30006.001',site,year,'./data/neon_refl',match_string='454000_4431000_reflectance.h5')\n\nDownload size: 615.17 MB\ndownloading NEON_D13_NIWO_DP3_454000_4431000_reflectance.h5 to ./data/neon_refl\n\n\n\n\n2.2 Define EMIT Collections of Interest\nWe need to specify which products we want to search for. The best way to do this is using their concept-id. As mentioned above, we will conduct our search using the EMIT Level 2A Reflectance (EMITL2ARFL). We can do some quick collection queries using earthaccess to retrieve the concept-id for each dataset.\n\n# EMIT Collection Query\nemit_collection_query = earthaccess.collection_query().keyword('EMIT L2A Reflectance')\nemit_collection_query.fields(['ShortName','EntryTitle','Version']).get()\n\n[{\n   \"meta\": {\n     \"concept-id\": \"C2408750690-LPCLOUD\",\n     \"granule-count\": 97750,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2ARFL\",\n     \"EntryTitle\": \"EMIT L2A Estimated Surface Reflectance and Uncertainty and Masks 60 m V001\",\n     \"Version\": \"001\"\n   }\n }]\n\n\nIf your search returns multiple products, be sure to select the right concept-id For this example it will be the first one. We want to use the LPCLOUD ECOSTRESS Tiled Land Surface Temperature and Emissivity (concept-id: “C2076090826-LPCLOUD”). Create a list of these concept-ids for our data search.\n\n# Data Collections for our search\nemit_concept_id = ['C2408750690-LPCLOUD']\n\n\n\n2.3 Define Date Range\nFor our date range, we’ll look at data collected between January 2022 and October 2023. The date_range can be specified as a pair of dates, start and end (up to, not including).\n\n# Define Date Range\ndate_range = ('2022-01-01','2023-11-01')\n\n\n\n2.4 Searching\nSubmit a query using earthaccess, usin the niwo_roi as the region of interest.\n\nemit_query_results = earthaccess.search_data(\n    concept_id=emit_concept_id,\n    polygon=niwo_roi,\n    temporal=date_range,\n    count=500)"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#organizing-and-filtering-results",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#organizing-and-filtering-results",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "3. Organizing and Filtering Results",
    "text": "3. Organizing and Filtering Results\nAs we can see from above, the results object contains a list of objects with metadata and links. We can convert this to a more readable format, a dataframe. In addition, we can make it a geodataframe by taking the spatial metadata and creating a shapely polygon representing the spatial coverage, and further customize which information we want to use from other metadata fields.\nFirst, we define some functions to help us create a shapely object for our geodataframe, and retrieve the specific browse image URLs that we want. By default, the browse image selected by earthaccess is the first one in the list, but the ECO_L2_LSTE has several browse images, and we want to make sure we retrieve the png file, which is a preview of the LSTE.\n\n# Function to create shapely polygon of spatial coverage\ndef get_shapely_object(result:earthaccess.results.DataGranule):\n    # Get Geometry Keys\n    geo = result['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']\n    keys = geo.keys()\n\n    if 'BoundingRectangles' in keys:\n        bounding_rectangle = geo['BoundingRectangles'][0]\n        # Create bbox tuple\n        bbox_coords = (bounding_rectangle['WestBoundingCoordinate'],bounding_rectangle['SouthBoundingCoordinate'],\n                    bounding_rectangle['EastBoundingCoordinate'],bounding_rectangle['NorthBoundingCoordinate'])\n        # Create shapely geometry from bbox\n        shape = geometry.box(*bbox_coords, ccw=True)\n    elif 'GPolygons' in keys:\n        points = geo['GPolygons'][0]['Boundary']['Points']\n        # Create shapely geometry from polygons\n        shape = geometry.Polygon([[p['Longitude'],p['Latitude']] for p in points])\n    else:\n         raise ValueError('Provided result does not contain bounding boxes/polygons or is incompatible.')\n    return(shape)\n\n# Retrieve png browse image if it exists or first jpg in list of urls\ndef get_png(result:earthaccess.results.DataGranule):\n    https_links = [link for link in result.dataviz_links() if 'https' in link]\n    if len(https_links) == 1:\n        browse = https_links[0]\n    elif len(https_links) == 0:\n        browse = 'no browse image'\n        warnings.warn(f\"There is no browse imagery for {result['umm']['GranuleUR']}.\")\n    else:\n        browse = [png for png in https_links if '.png' in png][0]\n    return(browse)\n\nNow that we have our functions we can create a dataframe, then calculate and add our shapely geometries to make a geodataframe. After that, add a column for our browse image urls and print the number of granules in our results, so we can monitor the quantity we are working with a we winnow down to the data we want.\n\n# Create Dataframe of Results Metadata\nemit_results_df = pd.json_normalize(emit_query_results)\n# Create shapely polygons for result\ngeometries = [get_shapely_object(emit_query_results[index]) for index in emit_results_df.index.to_list()]\n# Convert to GeoDataframe\nemit_gdf = gpd.GeoDataFrame(emit_results_df, geometry=geometries, crs=\"EPSG:4326\")\n# Remove emit_results_df, no longer needed\ndel emit_results_df\n# Add browse imagery links\nemit_gdf['browse'] = [get_png(granule) for granule in emit_query_results]\nemit_gdf['shortname'] = [result['umm']['CollectionReference']['ShortName'] for result in emit_query_results]\n# Preview GeoDataframe\nprint(f'{emit_gdf.shape[0]} granules total')\n\n15 granules total\n\n\nPreview our geodataframe to get an idea what it looks like.\n\nemit_gdf.head()\n\n\n\n\n\n\n\n\nsize\nmeta.concept-type\nmeta.concept-id\nmeta.revision-id\nmeta.native-id\nmeta.collection-concept-id\nmeta.provider-id\nmeta.format\nmeta.revision-date\numm.TemporalExtent.RangeDateTime.BeginningDateTime\n...\numm.DataGranule.DayNightFlag\numm.DataGranule.ArchiveAndDistributionInformation\numm.DataGranule.ProductionDateTime\numm.Platforms\numm.MetadataSpecification.URL\numm.MetadataSpecification.Name\numm.MetadataSpecification.Version\ngeometry\nbrowse\nshortname\n\n\n\n\n0\n3578.655884\ngranule\nG2597398990-LPCLOUD\n1\nEMIT_L2A_RFL_001_20220816T210039_2222814_001\nC2408750690-LPCLOUD\nLPCLOUD\napplication/vnd.nasa.cmr.umm+json\n2023-01-27T10:36:41.872Z\n2022-08-16T21:00:39Z\n...\nDay\n[{'Name': 'EMIT_L2A_RFL_001_20220816T210039_22...\n2023-01-27T10:26:44Z\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nPOLYGON ((-105.64632 40.42052, -106.05467 39.7...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n1\n3581.975898\ngranule\nG2597690501-LPCLOUD\n1\nEMIT_L2A_RFL_001_20220820T192256_2223213_003\nC2408750690-LPCLOUD\nLPCLOUD\napplication/vnd.nasa.cmr.umm+json\n2023-01-28T00:54:23.821Z\n2022-08-20T19:22:56Z\n...\nDay\n[{'Name': 'EMIT_L2A_RFL_001_20220820T192256_22...\n2023-01-28T00:42:40Z\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nPOLYGON ((-105.71240 41.02232, -106.10945 40.3...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n2\n3580.405610\ngranule\nG2624104672-LPCLOUD\n1\nEMIT_L2A_RFL_001_20230203T185001_2303413_010\nC2408750690-LPCLOUD\nLPCLOUD\napplication/vnd.nasa.cmr.umm+json\n2023-03-02T03:11:43.968Z\n2023-02-03T18:50:01Z\n...\nDay\n[{'Name': 'EMIT_L2A_RFL_001_20230203T185001_23...\n2023-03-02T03:01:34Z\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nPOLYGON ((-105.33098 40.69196, -106.29222 40.1...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n3\n3578.689785\ngranule\nG2646169485-LPCLOUD\n1\nEMIT_L2A_RFL_001_20230329T212754_2308814_017\nC2408750690-LPCLOUD\nLPCLOUD\napplication/vnd.nasa.cmr.umm+json\n2023-04-02T02:42:57.159Z\n2023-03-29T21:27:54Z\n...\nDay\n[{'Name': 'EMIT_L2A_RFL_001_20230329T212754_23...\n2023-04-02T02:32:39Z\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nPOLYGON ((-105.67938 40.79424, -106.64290 40.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n4\n3577.591364\ngranule\nG2646169234-LPCLOUD\n1\nEMIT_L2A_RFL_001_20230329T212806_2308814_018\nC2408750690-LPCLOUD\nLPCLOUD\napplication/vnd.nasa.cmr.umm+json\n2023-04-02T02:42:41.276Z\n2023-03-29T21:28:06Z\n...\nDay\n[{'Name': 'EMIT_L2A_RFL_001_20230329T212806_23...\n2023-04-02T02:32:39Z\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nPOLYGON ((-104.90044 41.24032, -105.88304 40.6...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n\n\n5 rows × 31 columns\n\n\n\nThere are a lot of columns with data that is not relevant for this exercise, so we can drop those. To do that, list the names of columns.\n\n# List Column Names\nemit_gdf.columns\n\nIndex(['size', 'meta.concept-type', 'meta.concept-id', 'meta.revision-id',\n       'meta.native-id', 'meta.collection-concept-id', 'meta.provider-id',\n       'meta.format', 'meta.revision-date',\n       'umm.TemporalExtent.RangeDateTime.BeginningDateTime',\n       'umm.TemporalExtent.RangeDateTime.EndingDateTime', 'umm.GranuleUR',\n       'umm.AdditionalAttributes',\n       'umm.SpatialExtent.HorizontalSpatialDomain.Geometry.GPolygons',\n       'umm.ProviderDates', 'umm.CollectionReference.ShortName',\n       'umm.CollectionReference.Version', 'umm.PGEVersionClass.PGEName',\n       'umm.PGEVersionClass.PGEVersion', 'umm.RelatedUrls', 'umm.CloudCover',\n       'umm.DataGranule.DayNightFlag',\n       'umm.DataGranule.ArchiveAndDistributionInformation',\n       'umm.DataGranule.ProductionDateTime', 'umm.Platforms',\n       'umm.MetadataSpecification.URL', 'umm.MetadataSpecification.Name',\n       'umm.MetadataSpecification.Version', 'geometry', 'browse', 'shortname'],\n      dtype='object')\n\n\nNow create a list of columns to keep and use it to filter the dataframe.\n\n# Create a list of columns to keep\nkeep_cols = ['meta.concept-id','meta.native-id', 'umm.TemporalExtent.RangeDateTime.BeginningDateTime','umm.TemporalExtent.RangeDateTime.EndingDateTime','umm.CloudCover','umm.DataGranule.DayNightFlag','geometry','browse', 'shortname']\n# Remove unneeded columns\nemit_gdf = emit_gdf[emit_gdf.columns.intersection(keep_cols)]\nemit_gdf.head()\n\n\n\n\n\n\n\n\nmeta.concept-id\nmeta.native-id\numm.TemporalExtent.RangeDateTime.BeginningDateTime\numm.TemporalExtent.RangeDateTime.EndingDateTime\numm.CloudCover\numm.DataGranule.DayNightFlag\ngeometry\nbrowse\nshortname\n\n\n\n\n0\nG2597398990-LPCLOUD\nEMIT_L2A_RFL_001_20220816T210039_2222814_001\n2022-08-16T21:00:39Z\n2022-08-16T21:00:51Z\n98\nDay\nPOLYGON ((-105.64632 40.42052, -106.05467 39.7...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n1\nG2597690501-LPCLOUD\nEMIT_L2A_RFL_001_20220820T192256_2223213_003\n2022-08-20T19:22:56Z\n2022-08-20T19:23:08Z\n85\nDay\nPOLYGON ((-105.71240 41.02232, -106.10945 40.3...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n2\nG2624104672-LPCLOUD\nEMIT_L2A_RFL_001_20230203T185001_2303413_010\n2023-02-03T18:50:01Z\n2023-02-03T18:50:13Z\n95\nDay\nPOLYGON ((-105.33098 40.69196, -106.29222 40.1...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n3\nG2646169485-LPCLOUD\nEMIT_L2A_RFL_001_20230329T212754_2308814_017\n2023-03-29T21:27:54Z\n2023-03-29T21:28:06Z\n100\nDay\nPOLYGON ((-105.67938 40.79424, -106.64290 40.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n4\nG2646169234-LPCLOUD\nEMIT_L2A_RFL_001_20230329T212806_2308814_018\n2023-03-29T21:28:06Z\n2023-03-29T21:28:17Z\n96\nDay\nPOLYGON ((-104.90044 41.24032, -105.88304 40.6...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n\n\n\n\n\nThis is looking better, but we can make it more readable by renaming our columns.\n\n# Rename some Columns\nemit_gdf.rename(columns = {'meta.concept-id':'concept_id','meta.native-id':'granule',\n                           'umm.TemporalExtent.RangeDateTime.BeginningDateTime':'start_datetime',\n                           'umm.TemporalExtent.RangeDateTime.EndingDateTime':'end_datetime',\n                           'umm.CloudCover':'cloud_cover',\n                           'umm.DataGranule.DayNightFlag':'day_night'}, inplace=True)\nemit_gdf.head()\n\n\n\n\n\n\n\n\nconcept_id\ngranule\nstart_datetime\nend_datetime\ncloud_cover\nday_night\ngeometry\nbrowse\nshortname\n\n\n\n\n0\nG2597398990-LPCLOUD\nEMIT_L2A_RFL_001_20220816T210039_2222814_001\n2022-08-16T21:00:39Z\n2022-08-16T21:00:51Z\n98\nDay\nPOLYGON ((-105.64632 40.42052, -106.05467 39.7...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n1\nG2597690501-LPCLOUD\nEMIT_L2A_RFL_001_20220820T192256_2223213_003\n2022-08-20T19:22:56Z\n2022-08-20T19:23:08Z\n85\nDay\nPOLYGON ((-105.71240 41.02232, -106.10945 40.3...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n2\nG2624104672-LPCLOUD\nEMIT_L2A_RFL_001_20230203T185001_2303413_010\n2023-02-03T18:50:01Z\n2023-02-03T18:50:13Z\n95\nDay\nPOLYGON ((-105.33098 40.69196, -106.29222 40.1...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n3\nG2646169485-LPCLOUD\nEMIT_L2A_RFL_001_20230329T212754_2308814_017\n2023-03-29T21:27:54Z\n2023-03-29T21:28:06Z\n100\nDay\nPOLYGON ((-105.67938 40.79424, -106.64290 40.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n4\nG2646169234-LPCLOUD\nEMIT_L2A_RFL_001_20230329T212806_2308814_018\n2023-03-29T21:28:06Z\n2023-03-29T21:28:17Z\n96\nDay\nPOLYGON ((-104.90044 41.24032, -105.88304 40.6...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n\n\n\n\n\n\n\n\nNote: If querying on-premises (not cloud) LP DAAC datasets, the meta.concept-id will not show as xxxxxx-LPCLOUD. For these datasets, the granule name can be retrieved from the umm.DataGranule.Identifiers column.\n\nWe can filter using the day/night flag as well, since we need a daytime collection to be comparable to the NEON data (which is captured at a high solar angle).\n\n# emit_gdf = emit_gdf[emit_gdf['day_night'].str.contains('Day')]\n\nOur first step toward filtering the datasets will be to add a column with a datetime.\n\nYou may have noticed that the date format is similar for ECOSTRESS and EMIT, but the ECOSTRESS data also includes fractional seconds. If working locally using lpdaac_vitals python environment, you may need to pass the format='ISO8601'argument to the to_datetime function, as shown in the commented-out line due to a difference in versions of pandas.\n\n\n#emit_gdf['datetime_obj'] = pd.to_datetime(emit_gdf['start_datetime']) # 2i2c\nemit_gdf['datetime_obj'] = pd.to_datetime(emit_gdf['start_datetime'], format='ISO8601') # Local ENV\n\nWe can roughly visualize the quantity of results by month at our location using a histogram with 8 bins (Jan - Oct).\n\nemit_gdf.hist(column='datetime_obj', by='shortname', bins=10, color='green', edgecolor='black', linewidth=1, sharey=True);"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#visualizing-intersecting-coverage",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#visualizing-intersecting-coverage",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "4. Visualizing Intersecting Coverage",
    "text": "4. Visualizing Intersecting Coverage\nNow that we have geodataframes containing some co-located data, we can visualize them on a map using folium. It’s often difficult to visualize a large time-series of scenes, so we’ve included an example in Appendix A1 on how to filter to a single day.\n\n# Plot Using Folium\n# Create Figure and Select Background Tiles\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add NIWO Bounding Box\nfolium.GeoJson(niwo_bbox,\n                name='bounding_box',).add_to(map1)\n\n# Add roi geodataframe\nniwo_polygon.explore(\"flightbxID\",\n                      popup=True,\n                      categorical=True,\n                      cmap='Set3',\n                      style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n                      name=\"Niwot Ridge ROI\",\n                      m=map1)\n\n# Plot STAC EMITL2ARFL Results - note we must drop the datetime_obj columns for this to work\nemit_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"EMIT\",\n    m=map1,\n    legend=False\n)\n\nmap1.fit_bounds(bounds=convert_bounds(emit_gdf.unary_union.bounds))\nmap1.add_child(folium.LayerControl())\ndisplay(fig)\n\n\n\n\n\n4.2 Previewing EMIT Browse Imagery\nThe EMIT browse imagery is not orthorectified, so to get an idea what scenes look like, we can plot them in a grid using matplotlib.\n\nNote: The black space is indicative of onboard cloud masking that occurs before data is downlinked from the ISS.\n\n\ncols = 3\nrows = math.ceil(len(emit_gdf)/cols)\nfig, ax = plt.subplots(rows, cols, figsize=(20,20))\nax = ax.flatten()\n\nfor _n, index in enumerate(emit_gdf.index.to_list()):\n    img = io.imread(emit_gdf['browse'][index])\n    ax[_n].imshow(img)\n    ax[_n].set_title(f\"Index: {index} - {emit_gdf['granule'][index]}\")\n    ax[_n].axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.3 Further Filtering\nWe can see that some of these granules likely won’t work because of the large amount of cloud cover, we can use a list of these to filter them out. Make a list of indexes to filter out.\nFilter out the bad granules.\n\n# set a threshold for cloud cover and filter to remove scenes with &gt;50% cloud cover\nemit_gdf_clear = emit_gdf[emit_gdf.cloud_cover &lt; 50]\n\n\nemit_gdf_clear\n\n\n\n\n\n\n\n\nconcept_id\ngranule\nstart_datetime\nend_datetime\ncloud_cover\nday_night\ngeometry\nbrowse\nshortname\ndatetime_obj\n\n\n\n\n10\nG2736967625-LPCLOUD\nEMIT_L2A_RFL_001_20230625T170814_2317611_005\n2023-06-25T17:08:14Z\n2023-06-25T17:08:26Z\n8\nDay\nPOLYGON ((-105.92612 40.60380, -106.33020 39.9...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-06-25 17:08:14+00:00\n\n\n\n\n\n\n\nWe can see that there is only one scene with &lt;50% cloud cover. Luckily, this is captured in late June (June 25), which is close in time to when NEON typically surveys Niwot Ridge. NEON surveys during “peak-greenness”, when leaves are most photosynthetically active, which at NIWO usually occurs in July - August (in 2020 NIWO was surveyed on July 20, July 31, August 1, and August 7), in 2023 NIWO was surveyed on July 24, August 15, and August 21). Data from 2023 are not yet available from NEON.\nLet’s take a look at the clear-weather EMIT dataset (Index 10):\n\n10: EMIT_L2A_RFL_001_20230625T170814_2317611_005\n\nWe can plot this scene as follows:\n\nfig, ax = plt.subplots(1, 1, figsize=(8,8))\nimg = io.imread(emit_gdf_clear['browse'][10])\nax.imshow(img)\nax.set_title(f\"{emit_gdf_clear['granule'][10]}\")\nax.axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nWe can now go back to our folium plot above and re-run the cell to update it based on our filtering.\n\n# Plot Using Folium\n# Create Figure and Select Background Tiles\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add NIWO Bounding Box\nfolium.GeoJson(niwo_bbox,\n                name='bounding_box',\n                ).add_to(map1)\n\n# Add roi geodataframe\nniwo_polygon.explore(\"flightbxID\",\n                      popup=True,\n                      categorical=True,\n                      cmap='Set3',\n                      style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n                      name=\"Niwot Ridge ROI\",\n                      m=map1)\n\n# Plot STAC EMITL2ARFL Results - note we must drop the datetime_obj columns for this to work\nemit_gdf_clear.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"EMIT\",\n    m=map1,\n    legend=False\n)\n\nmap1.fit_bounds(bounds=convert_bounds(emit_gdf.unary_union.bounds))\nmap1.add_child(folium.LayerControl())\ndisplay(fig)"
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#generating-a-list-of-urls-and-downloading-data",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#generating-a-list-of-urls-and-downloading-data",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "5. Generating a list of URLs and downloading data",
    "text": "5. Generating a list of URLs and downloading data\nCreating a list of results URLs will include all of these assets, so if we only want a subset we need an additional filter to keep the specific assets we want.\nIf you look back, you can see we kept the same indexing throughout the notebook. This enables us to simply subset the earthaccess results object to retrieve the results we want.\nCreate a list of index values to keep.\n\nkeep_granules = [10]\n\nFilter the results list.\n\nfiltered_results = [result for i, result in enumerate(emit_query_results) if i in keep_granules]\n\nNow we can download all of the associated assets, or retrieve the URLS and further filter them to specifically what we want.\nFirst, log into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\nNow we can download all assets using the following cell.\n\n# Download All Assets for Granules in Filtered Results\n#earthaccess.download(filtered_results, '../data/NIWO/emit_refl')\n\nOr we can create a list of URLs and use that to further refine which files we download.\n\n# Retrieve URLS for Assets\nresults_urls = [granule.data_links() for granule in filtered_results]\n\n\nresults_urls\n\n[['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230625T170814_2317611_005/EMIT_L2A_RFL_001_20230625T170814_2317611_005.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230625T170814_2317611_005/EMIT_L2A_RFLUNCERT_001_20230625T170814_2317611_005.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230625T170814_2317611_005/EMIT_L2A_MASK_001_20230625T170814_2317611_005.nc']]\n\n\nWe can see this is a nested list. Granules often have several assets associated with them, for example, EMIT_L2A has several assets:\n\nRFL\nRFLUNCERT\nMASK\n\nThe results list we just generated contains URLs to all of these assets nested by granule. We can further filter our results list using string matching to remove unwanted assets.\nCreate a list of strings and enumerate through our results_url list to filter out unwanted assets and remove the nesting.\n\nfiltered_asset_links = []\n# Pick Desired Assets (leave _ on RFL to distinguish from RFLUNC, LST. to distinguish from LST_err)\ndesired_assets = ['RFL_','MASK', 'LST.'] # Add more or do individually for reflectance, reflectance uncertainty, or mask\n# Step through each sublist (granule) and filter based on desired assets.\nfor n, granule in enumerate(results_urls):\n    for url in granule: \n        asset_name = url.split('/')[-1]\n        if any(asset in asset_name for asset in desired_assets):\n            filtered_asset_links.append(url)\nfiltered_asset_links\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230625T170814_2317611_005/EMIT_L2A_RFL_001_20230625T170814_2317611_005.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230625T170814_2317611_005/EMIT_L2A_MASK_001_20230625T170814_2317611_005.nc']\n\n\nWe can see that this removed the REFLUNCERT data. We can also write this list of files to a text file to have a record of data used, or stream the data using https as we access them. For streaming the data, the EMIT files are very large, so operations can take some time.\n\nwith open('./data/emit_search_results.txt', 'w') as f:\n    for line in filtered_asset_links:\n        f.write(f\"{line}\\n\")\n\nOpen the list of required granules.\n\nNote: You can download all of the files using the cell below and recreate all of the canopy water content files following a workflow similar to the example in notebooks 2 and 3 for all of the necessary scenes. To do this, uncomment the file_list object with the emit_search_results.txt filepath to download all of the results rather than just what is required.\n\n\n# Open Text File and Read Lines\n#file_list = './data/required_granules.txt'\nfile_list = './data/emit_search_results.txt'\nwith open(file_list) as f:\n    urls = [line.rstrip('\\n') for line in f]\n\nDownload the required granules.\n\n# Create a directory to store the downloaded emit data\nos.makedirs('./data/emit_refl', exist_ok=True)\n# Get requests https session using Earthdata Login Info\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\nfor url in urls:\n    granule_asset_id = url.split('/')[-1]\n    # Define Local Filepath\n    fp = f'./data/emit_refl/{granule_asset_id}'\n    # Download the Granule Asset if it doesn't exist\n    if not os.path.isfile(fp):\n        with fs.get(url,stream=True) as src:\n            with open(fp,'wb') as dst:\n                for chunk in src.iter_content(chunk_size=64*1024*1024):\n                    dst.write(chunk)\n\nCongratulations, now you have downloaded co-located hyperspectral reflectance data from NEON airborne collections and the EMIT instrument on the ISS."
  },
  {
    "objectID": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#contact-info",
    "href": "community_contributed/NEON_EMIT/01_Finding_Co-located_NEON_EMIT_Data_NIWO.html#contact-info",
    "title": "1 Finding Co-located EMIT and NEON AOP Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nLand Processes Distributed Active Archive Center (LP DAAC)1\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nWebsite: https://lpdaac.usgs.gov/\n1Work performed under USGS contract G15PD00467 for NASA contract NNG14HH33I.\nNational Ecological Observatory Network (NEON)2\nWebsite: https://www.neonscience.org/\nContact: https://www.neonscience.org/about/contact-us\nDate last modified: 08-27-2024\n2NEON is a project sponsored by the National Science Foundation and operated by Battelle."
  },
  {
    "objectID": "CONTRIBUTING.html",
    "href": "CONTRIBUTING.html",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "Please submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\nWe want your help! Even if you’re not a coder! There are several ways you can contribute to this repository:\n\nReport an Issue or make a recommendation\nUpdate code, documentation, notebooks, or other files (even fixing typos)\nPropose a new notebook\n\nIn the sections below we outline how to approach each of these types of contributions. If you’re new to GitHub, you can sign up here. There are a bunch of great resources on the GitHub Quickstart page. The GitHub Cheatsheet is also quite helpful, even for experienced users. Please reach out to lpdaac@usgs.gov with questions or concerns.\n\n\nIf you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!\n\n\n\nTo contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.\n\n\n\n\n\nIn the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a community_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the community_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.\n\n\n\nThese contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "href": "CONTRIBUTING.html#report-an-issue-or-make-a-recommendation",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "If you’ve found a problem with the repository, we want to know about it! Please submit an Issue. Before submitting, we would appreciate if you check to see if a similar issue already exists. If not, create a new issue, providing as much detail as possible. Things like screenshots and code excerpts demonstrating the problem are very helpful!",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "href": "CONTRIBUTING.html#updating-code-documentation-notebooks-or-other-files",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "To contribute a solution to an issue or make a change to files within the repository we’ve created a typical outline of how to do that below. If you want to make a simple change, like correcting a typo within a markdown document or other documentation, there’s a great video explaining how to do that without leaving the GitHub website here. To make a more complex change to a notebook, code, or other file follow the instructions below.\n\nPlease create an Issue or comment on an existing issue describing the changes you intend to make.\n\nCreate a fork of this repository. This will create your own copy of the repository. When working from your fork, you can do whatever you want, you won’t mess up anyone else’s work so you’re safe to try things out. Worst case scenario you can delete your fork and recreate it.\n\nClone your fork to your local computer or cloud workspace using your preferred command line interface after navigating to the directory you want to place the repository in:\ngit clone your-fork-repository-url\n\nChange directories to the one you cloned\n\ncd repository-name\n\nAdd the upstream repository, this is the original repository that you want to contribute to.\n\ngit remote add upstream original-repository-url\n\nYou can use the following to view the remote repositories:\n\ngit remote -v\n\nupstream, which refers to the original repository\n\norigin, which refers to your personal fork\n\nDevelop your contribution:\n\nCreate a new branch named appropriately for the feature you want to work on:\n\ngit checkout -b new-branch-name\n\nOften, updates to an upstream repository will occur while you are developing changes on your personal fork. You can pull the latest changes from upstream\n\ngit pull upstream dev\n\nYou can check the status of your local copy of the repository to see what changes have been made using:\n\ngit status\n\nCommit locally as you progress using git add and git commit. For example, updating a readme.md file:\n\ngit add readme.md\ngit commit -m \"updated readme file\"\n\nYou can check the status of your local copy of the repository again to see what pending changes have not been added or committed using:\n\ngit status\n\nAfter making some changes, push your changes back to your fork on GitHub:\n\ngit push origin branch-name\n\nEnter username and password, depending on your settings, you may need to use a Personal access token\n\nTo submit your contribution, navigate to your forked repository GitHub page and make a pull request using the Compare &pull request green button. Make sure to select the base repository and its dev branch. Also select your forked repository as head repository and make sure compare shows your branch name. You can add your comments and press Create pull request green button. Our team will be notified and will review your suggested revisions.\n\nPlease submit a pull request early in the development phase, outlining the changes you intend to make or features you intend to add. This allows us to offer feedback early on, ensuring your contribution can be added to the repository before you invest a significant amount of time.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "href": "CONTRIBUTING.html#adding-new-notebooks-or-example-workflows",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "In the spirit of open science, we want to minimize barriers to sharing code and examples. We have added a community_contributed directory to the repository for anyone to share examples of their work in notebook or code form. Documentation and descriptions do not need to be as thorough as the examples we’ve created, but we ask that you provide as much as possible. Follow the instructions above, placing your new notebook or module in a suitably named directory within the community_contributed directory. Be sure to remove any large datasets and indicate where users can retrieve them.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "CONTRIBUTING.html#attribution",
    "href": "CONTRIBUTING.html#attribution",
    "title": "Contributing to this Repository",
    "section": "",
    "text": "These contributing guidelines are adapted from the NASA Transform to Open Science GitHub, available at https://github.com/nasa/Transform-to-Open-Science/blob/main/CONTRIBUTING.md.",
    "crumbs": [
      "Contributing",
      "Contributing to this Repository"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html",
    "href": "python/01_Finding_Concurrent_Data.html",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "",
    "text": "Summary\nBoth the ECOsystem Spaceborne Thermal Radiometer Experiment on Space Station (ECOSTRESS) and the Earth surface Mineral dust source InvesTigation (EMIT) instruments are located on the International Space Station (ISS). Their overlapping fields of view provide an unprecedented opportunity to demonstrate the compounded benefits of working with both datasets. In this notebook we will show how to utilize the earthaccess Python library to find concurrent ECOSTRESS and EMIT data.\nBackground\nThe ECOSTRESS instrument is a multispectral thermal imaging radiometer designed to answer three overarching science questions:\nThe ECOSTRESS mission is answering these questions by accurately measuring the temperature of plants. Plants regulate their temperature by releasing water through tiny pores on their leaves called stomata. If they have sufficient water they can maintain their temperature, but if there is insufficient water, their temperatures rise and this temperature rise can be measured with ECOSTRESS. The images acquired by ECOSTRESS are the most detailed temperature images of the surface ever acquired from space and can be used to measure the temperature of an individual farmers field.\nMore details about ECOSTRESS and its associated products can be found on the ECOSTRESS website and ECOSTRESS product pages hosted by the Land Processes Distributed Active Archive Center (LP DAAC).\nThe EMIT instrument is an imaging spectrometer that measures light in visible and infrared wavelengths. These measurements display unique spectral signatures that correspond to the composition on the Earth’s surface. The EMIT mission focuses specifically on mapping the composition of minerals to better understand the effects of mineral dust throughout the Earth system and human populations now and in the future. In addition, the EMIT instrument can be used in other applications, such as mapping of greenhouse gases, snow properties, and water resources.\nMore details about EMIT and its associated products can be found on the EMIT website and EMIT product pages hosted by the LP DAAC.\nRequirements\n- NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only Set up Python Environment - See setup_instructions.md in the /setup/ folder to set up a local compatible Python environment\nLearning Objectives\n- How to use earthaccess to find concurrent EMIT and ECOSTRESS data.\n- How to export a list of files and download them programmatically.\nTutorial Outline",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#setup",
    "href": "python/01_Finding_Concurrent_Data.html#setup",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "1. Setup",
    "text": "1. Setup\nImport the required Python libraries.\n\n# Import required libraries\nimport os\nimport folium\nimport earthaccess\nimport warnings\nimport folium.plugins\nimport pandas as pd\nimport geopandas as gpd\nimport math\n\nfrom branca.element import Figure\nfrom IPython.display import display\nfrom shapely import geometry\nfrom skimage import io\nfrom datetime import timedelta\nfrom shapely.geometry.polygon import orient\nfrom matplotlib import pyplot as plt\n\n\n1.2 NASA Earthdata Login Credentials\nTo download or stream NASA data you will need an Earthdata account, you can create one here. We will use the login function from the earthaccess library for authentication before downloading at the end of the notebook. This function can also be used to create a local .netrc file if it doesn’t exist or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. This step is not necessary to conduct searches but is needed to download or stream data.",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#search-for-ecostress-and-emit-data",
    "href": "python/01_Finding_Concurrent_Data.html#search-for-ecostress-and-emit-data",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "2. Search for ECOSTRESS and EMIT Data",
    "text": "2. Search for ECOSTRESS and EMIT Data\nBoth EMIT and ECOSTRESS products are hosted by the Land Processes Distributed Active Archive Center (LP DAAC). In this example we will use the cloud-hosted EMIT_L2A_RFL and ECOSTRESS_L2T_LSTE products available from the LP DAAC to find data. Any results we find for these products, should be available for other products within the EMIT and ECOSTRESS collections.\nTo find data we will use the earthaccess Python library. earthaccess searches NASA’s Common Metadata Repository (CMR), a metadata system that catalogs Earth Science data and associated metadata records. The results can then be used to download granules or generate lists of granule search result URLs.\nUsing earthaccess we can search based on the attributes of a granule, which can be thought of as a spatiotemporal scene from an instrument containing multiple assets (ex: Reflectance, Reflectance Uncertainty, Masks for the EMIT L2A Reflectance Collection). We can search using attributes such as collection, acquisition time, and spatial footprint. This process can also be used with other EMIT or ECOSTRESS products, other collections, or different data providers, as well as across multiple catalogs with some modification.\n\n2.1 Define Spatial Region of Interest\nFor this example, our spatial region of interest (ROI) will be the a region near Santa Barbara, CA that contains the Jack and Laura Dangermond Preserve and the Sedgwick Reserve.\nIn this example, we will create a rectangular ROI surrounding these two reserves as well as some of the agricultural region between. Even though the shape is rectangular we elect to search using a polygon rather than a standard bounding box in earthaccess because bounding boxes will typically have a larger spatial extent, capturing a lot of area we may not be interested in. This becomes more important for searches with larger ROIs than our example here. To search for intersections with a polygon using earthaccess, we need to format our ROI as a counterclockwise list of coordinate pairs.\nOpen the geojson file containing the Dangermond and Sedgwick boundaries as a geodataframe, and check the coordinate reference system (CRS) of the data.\n\npolygon = gpd.read_file('../data/agu_workshop_roi.geojson')\npolygon.crs\n\n&lt;Geographic 2D CRS: EPSG:4326&gt;\nName: WGS 84\nAxis Info [ellipsoidal]:\n- Lat[north]: Geodetic latitude (degree)\n- Lon[east]: Geodetic longitude (degree)\nArea of Use:\n- name: World.\n- bounds: (-180.0, -90.0, 180.0, 90.0)\nDatum: World Geodetic System 1984 ensemble\n- Ellipsoid: WGS 84\n- Prime Meridian: Greenwich\n\n\nThe CRS is EPSG:4326 (WGS84), which is also the CRS we want the data in to submit for our search.\nNext, let’s examine our polygon a bit closer.\n\npolygon\n\n\n\n\n\n\n\n\nName\nAcreage\ngeometry\n\n\n\n\n0\nSedgwick Reserve\n5874.210000\nPOLYGON ((-120.04041 34.74342, -120.04122 34.7...\n\n\n1\nDangermond Preserve\n24458.615397\nPOLYGON ((-120.47367 34.56987, -120.47427 34.5...\n\n\n\n\n\n\n\nWe can see this geodataframe consists of two polygons, that we want to include in our study site. We need to create an exterior boundary polygon containing these, and make sure the vertices are in counterclockwise order to submit them in our query. To do this, create a polygon consisting of all the geometries, then create a bounding rectangle. This will give us a simple exterior polygon around our two ROIs. After that, use the orient function to place our coordinate pairs in counterclockwise order.\n\n# Merge all Polygon geometries and create external boundary\nroi_poly = polygon.unary_union.envelope\n# Re-order vertices to counterclockwise\nroi_poly = orient(roi_poly, sign=1.0)\n\nMake a GeoDataFrame consisting of the bounding box geometry.\n\ndf = pd.DataFrame({\"Name\":[\"ROI Bounding Box\"]})\nagu_bbox = gpd.GeoDataFrame({\"Name\":[\"ROI Bounding Box\"], \"geometry\":[roi_poly]},crs=\"EPSG:4326\")\nagu_bbox\n\n\n\n\n\n\n\n\nName\ngeometry\n\n\n\n\n0\nROI Bounding Box\nPOLYGON ((-120.49929 34.44230, -120.01175 34.4...\n\n\n\n\n\n\n\nWe can write this bounding box to a file for use in future notebooks.\n\n#agu_bbox.to_file('../data/roi_bbox.geojson', driver='GeoJSON')\n\nWe can go ahead and visualize our region of interest and the exterior boundary polygon containing ROIs. First add a function to help reformat bounding box coordinates to work with leaflet notation.\n\n# Function to convert a bounding box for use in leaflet notation\n\ndef convert_bounds(bbox, invert_y=False):\n    \"\"\"\n    Helper method for changing bounding box representation to leaflet notation\n\n    ``(lon1, lat1, lon2, lat2) -&gt; ((lat1, lon1), (lat2, lon2))``\n    \"\"\"\n    x1, y1, x2, y2 = bbox\n    if invert_y:\n        y1, y2 = y2, y1\n    return ((y1, x1), (y2, x2))\n\n\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Add Convex Hull Polygon\nfolium.GeoJson(agu_bbox,\n                name='bounding_box',\n                ).add_to(map1)\n\n# Add roi geodataframe\npolygon.explore(\n    \"Name\",\n    popup=True,\n    categorical=True,\n    cmap='Set3',\n    style_kwds=dict(opacity=0.7, fillOpacity=0.4),\n    name=\"Regions of Interest\",\n    m=map1\n)\n\nmap1.add_child(folium.LayerControl())\nmap1.fit_bounds(bounds=convert_bounds(polygon.unary_union.bounds))\ndisplay(fig)\n\n\n\n\nAbove we can see our regions of interest (ROIs) and the exterior boundary polygon containing the ROIs that we opened. We can hover over different areas to see the name of each ROI.\nLastly, we need to convert our polygon to a list of coordinate pairs, so it will be accepted as a ‘polygon’ search parameter in earthaccess, as it expects a list of coordinate pairs in a counterclockwise order.\n\n# Set ROI as list of exterior polygon vertices as coordinate pairs\nroi = list(roi_poly.exterior.coords)\n\n\n\n2.2 Define Collections of Interest\nWe need to specify which products we want to search for. The best way to do this is using their concept-id. As mentioned above, we will conduct our search using the EMIT Level 2A Reflectance (EMITL2ARFL) and ECOSTRESS Level 2 Tiled Land Surface Temperature and Emissivity (ECO_L2T_LSTE). We can do some quick collection queries using earthaccess to retrieve the concept-id for each dataset.\n\nNote: Here we use the Tiled ECOSTRESS LSTE Product. This will also work with the gridded LSTE and the swath; however, the swath product does not have a browse image for the visualization in section 4 and will require additional processing for subsequent analysis.\n\n\n# EMIT Collection Query\nemit_collection_query = earthaccess.collection_query().keyword('EMIT L2A Reflectance')\nemit_collection_query.fields(['ShortName','EntryTitle','Version']).get()\n\n[{\n   \"meta\": {\n     \"concept-id\": \"C2408750690-LPCLOUD\",\n     \"granule-count\": 80543,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"EMITL2ARFL\",\n     \"EntryTitle\": \"EMIT L2A Estimated Surface Reflectance and Uncertainty and Masks 60 m V001\",\n     \"Version\": \"001\"\n   }\n }]\n\n\n\n# ECOSTRESS Collection Query\neco_collection_query = earthaccess.collection_query().keyword('ECOSTRESS L2 Tiled LSTE')\neco_collection_query.fields(['ShortName','EntryTitle','Version']).get()\n\n[{\n   \"meta\": {\n     \"concept-id\": \"C2076090826-LPCLOUD\",\n     \"granule-count\": 3133795,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"ECO_L2T_LSTE\",\n     \"EntryTitle\": \"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\n     \"Version\": \"002\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2204557047-LPDAAC_ECS\",\n     \"granule-count\": 1761265,\n     \"provider-id\": \"LPDAAC_ECS\"\n   },\n   \"umm\": {\n     \"ShortName\": \"ECO_L2T_LSTE\",\n     \"EntryTitle\": \"ECOSTRESS Tiled Land Surface Temperature and Emissivity Instantaneous L2 Global 70 m V002\",\n     \"Version\": \"002\"\n   }\n },\n {\n   \"meta\": {\n     \"concept-id\": \"C2090073749-LPCLOUD\",\n     \"granule-count\": 202814,\n     \"provider-id\": \"LPCLOUD\"\n   },\n   \"umm\": {\n     \"ShortName\": \"ECO_L2T_STARS\",\n     \"EntryTitle\": \"ECOSTRESS Tiled Ancillary NDVI and Albedo L2 Global 70 m V002\",\n     \"Version\": \"002\"\n   }\n }]\n\n\nIf your search returns multiple products, be sure to select the right concept-id For this example it will be the first one. We want to use the LPCLOUD ECOSTRESS Tiled Land Surface Temperature and Emissivity (concept-id: “C2076090826-LPCLOUD”). Create a list of these concept-ids for our data search.\n\n# Data Collections for our search\nconcept_ids = ['C2408750690-LPCLOUD', 'C2076090826-LPCLOUD']\n\n\n\n2.3 Define Date Range\nFor our date range, we’ll look at data collected between January and October 2023. The date_range can be specified as a pair of dates, start and end (up to, not including).\n\n# Define Date Range\ndate_range = ('2023-01-01','2023-11-01')\n\n\n\n2.4 Searching\nSubmit a query using earthaccess.\n\nresults = earthaccess.search_data(\n    concept_id=concept_ids,\n    polygon=roi,\n    temporal=date_range,\n    count=500\n)\n\nGranules found: 323",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#organizing-and-filtering-results",
    "href": "python/01_Finding_Concurrent_Data.html#organizing-and-filtering-results",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "3. Organizing and Filtering Results",
    "text": "3. Organizing and Filtering Results\nAs we can see from above, the results object contains a list of objects with metadata and links. We can convert this to a more readable format, a dataframe. In addition, we can make it a geodataframe by taking the spatial metadata and creating a shapely polygon representing the spatial coverage, and further customize which information we want to use from other metadata fields.\nFirst, we define some functions to help us create a shapely object for our geodataframe, and retrieve the specific browse image URLs that we want. By default, the browse image selected by earthaccess is the first one in the list, but the ECO_L2_LSTE has several browse images, and we want to make sure we retrieve the png file, which is a preview of the LSTE.\n\n# Function to create shapely polygon of spatial coverage\ndef get_shapely_object(result:earthaccess.results.DataGranule):\n    # Get Geometry Keys\n    geo = result['umm']['SpatialExtent']['HorizontalSpatialDomain']['Geometry']\n    keys = geo.keys()\n\n    if 'BoundingRectangles' in keys:\n        bounding_rectangle = geo['BoundingRectangles'][0]\n        # Create bbox tuple\n        bbox_coords = (bounding_rectangle['WestBoundingCoordinate'],bounding_rectangle['SouthBoundingCoordinate'],\n                    bounding_rectangle['EastBoundingCoordinate'],bounding_rectangle['NorthBoundingCoordinate'])\n        # Create shapely geometry from bbox\n        shape = geometry.box(*bbox_coords, ccw=True)\n    elif 'GPolygons' in keys:\n        points = geo['GPolygons'][0]['Boundary']['Points']\n        # Create shapely geometry from polygons\n        shape = geometry.Polygon([[p['Longitude'],p['Latitude']] for p in points])\n    else:\n         raise ValueError('Provided result does not contain bounding boxes/polygons or is incompatible.')\n    return(shape)\n\n# Retrieve png browse image if it exists or first jpg in list of urls\ndef get_png(result:earthaccess.results.DataGranule):\n    https_links = [link for link in result.dataviz_links() if 'https' in link]\n    if len(https_links) == 1:\n        browse = https_links[0]\n    elif len(https_links) == 0:\n        browse = 'no browse image'\n        warnings.warn(f\"There is no browse imagery for {result['umm']['GranuleUR']}.\")\n    else:\n        browse = [png for png in https_links if '.png' in png][0]\n    return(browse)\n\nNow that we have our functions we can create a dataframe, then calculate and add our shapely geometries to make a geodataframe. After that, add a column for our browse image urls and print the number of granules in our results, so we can monitor the quantity we are working with a we winnow down to the data we want.\n\n# Create Dataframe of Results Metadata\nresults_df = pd.json_normalize(results)\n# Create shapely polygons for result\ngeometries = [get_shapely_object(results[index]) for index in results_df.index.to_list()]\n# Convert to GeoDataframe\ngdf = gpd.GeoDataFrame(results_df, geometry=geometries, crs=\"EPSG:4326\")\n# Remove results df, no longer needed\ndel results_df\n# Add browse imagery links\ngdf['browse'] = [get_png(granule) for granule in results]\ngdf['shortname'] = [result['umm']['CollectionReference']['ShortName'] for result in results]\n# Preview GeoDataframe\nprint(f'{gdf.shape[0]} granules total')\n\n323 granules total\n\n\nPreview our geodataframe to get an idea what it looks like.\n\ngdf.head()\n\n\n\n\n\n\n\n\nsize\nmeta.concept-type\nmeta.concept-id\nmeta.revision-id\nmeta.native-id\nmeta.collection-concept-id\nmeta.provider-id\nmeta.format\nmeta.revision-date\numm.TemporalExtent.RangeDateTime.BeginningDateTime\n...\numm.Platforms\numm.MetadataSpecification.URL\numm.MetadataSpecification.Name\numm.MetadataSpecification.Version\numm.SpatialExtent.HorizontalSpatialDomain.Geometry.GPolygons\numm.PGEVersionClass.PGEName\numm.CloudCover\ngeometry\nbrowse\nshortname\n\n\n\n\n0\n2.496260\ngranule\nG2581836170-LPCLOUD\n2\nECOv002_L2T_LSTE_25460_016_11SKU_20230101T1552...\nC2076090826-LPCLOUD\nLPCLOUD\napplication/echo10+xml\n2024-01-20T15:57:11.266Z\n2023-01-01T15:52:48.650Z\n...\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nNaN\nNaN\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n1\n0.292531\ngranule\nG2586136519-LPCLOUD\n2\nECOv002_L2T_LSTE_25486_005_10SGD_20230103T0743...\nC2076090826-LPCLOUD\nLPCLOUD\napplication/echo10+xml\n2024-01-20T17:35:56.995Z\n2023-01-03T07:43:30.510Z\n...\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nNaN\nNaN\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n2\n8.495430\ngranule\nG2586136993-LPCLOUD\n2\nECOv002_L2T_LSTE_25486_006_11SKU_20230103T0744...\nC2076090826-LPCLOUD\nLPCLOUD\napplication/echo10+xml\n2024-01-20T17:36:01.364Z\n2023-01-03T07:44:22.480Z\n...\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nNaN\nNaN\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n3\n5.103070\ngranule\nG2586137000-LPCLOUD\n2\nECOv002_L2T_LSTE_25486_006_10SGD_20230103T0744...\nC2076090826-LPCLOUD\nLPCLOUD\napplication/echo10+xml\n2024-01-20T17:36:08.168Z\n2023-01-03T07:44:22.480Z\n...\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nNaN\nNaN\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n4\n1.160680\ngranule\nG2591892077-LPCLOUD\n2\nECOv002_L2T_LSTE_25547_005_11SKU_20230107T0607...\nC2076090826-LPCLOUD\nLPCLOUD\napplication/echo10+xml\n2024-01-17T16:14:22.254Z\n2023-01-07T06:07:21.560Z\n...\n[{'ShortName': 'ISS', 'Instruments': [{'ShortN...\nhttps://cdn.earthdata.nasa.gov/umm/granule/v1.6.6\nUMM-G\n1.6.6\nNaN\nNaN\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n\n\n5 rows × 35 columns\n\n\n\nThere are a lot of columns with data that is not relevant to our goal, so we can drop those. To do that, list the names of columns.\n\n# List Column Names\ngdf.columns\n\nIndex(['size', 'meta.concept-type', 'meta.concept-id', 'meta.revision-id',\n       'meta.native-id', 'meta.collection-concept-id', 'meta.provider-id',\n       'meta.format', 'meta.revision-date',\n       'umm.TemporalExtent.RangeDateTime.BeginningDateTime',\n       'umm.TemporalExtent.RangeDateTime.EndingDateTime',\n       'umm.OrbitCalculatedSpatialDomains', 'umm.GranuleUR',\n       'umm.AdditionalAttributes', 'umm.MeasuredParameters',\n       'umm.SpatialExtent.HorizontalSpatialDomain.Geometry.BoundingRectangles',\n       'umm.ProviderDates', 'umm.CollectionReference.ShortName',\n       'umm.CollectionReference.Version', 'umm.PGEVersionClass.PGEVersion',\n       'umm.RelatedUrls', 'umm.DataGranule.DayNightFlag',\n       'umm.DataGranule.Identifiers', 'umm.DataGranule.ProductionDateTime',\n       'umm.DataGranule.ArchiveAndDistributionInformation', 'umm.Platforms',\n       'umm.MetadataSpecification.URL', 'umm.MetadataSpecification.Name',\n       'umm.MetadataSpecification.Version',\n       'umm.SpatialExtent.HorizontalSpatialDomain.Geometry.GPolygons',\n       'umm.PGEVersionClass.PGEName', 'umm.CloudCover', 'geometry', 'browse',\n       'shortname'],\n      dtype='object')\n\n\nNow create a list of columns to keep and use it to filter the dataframe.\n\n# Create a list of columns to keep\nkeep_cols = ['meta.concept-id','meta.native-id', 'umm.TemporalExtent.RangeDateTime.BeginningDateTime','umm.TemporalExtent.RangeDateTime.EndingDateTime','umm.CloudCover','umm.DataGranule.DayNightFlag','geometry','browse', 'shortname']\n# Remove unneeded columns\ngdf = gdf[gdf.columns.intersection(keep_cols)]\ngdf.head()\n\n\n\n\n\n\n\n\nmeta.concept-id\nmeta.native-id\numm.TemporalExtent.RangeDateTime.BeginningDateTime\numm.TemporalExtent.RangeDateTime.EndingDateTime\numm.DataGranule.DayNightFlag\numm.CloudCover\ngeometry\nbrowse\nshortname\n\n\n\n\n0\nG2581836170-LPCLOUD\nECOv002_L2T_LSTE_25460_016_11SKU_20230101T1552...\n2023-01-01T15:52:48.650Z\n2023-01-01T15:53:40.620Z\nDay\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n1\nG2586136519-LPCLOUD\nECOv002_L2T_LSTE_25486_005_10SGD_20230103T0743...\n2023-01-03T07:43:30.510Z\n2023-01-03T07:44:22.480Z\nNight\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n2\nG2586136993-LPCLOUD\nECOv002_L2T_LSTE_25486_006_11SKU_20230103T0744...\n2023-01-03T07:44:22.480Z\n2023-01-03T07:45:14.450Z\nNight\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n3\nG2586137000-LPCLOUD\nECOv002_L2T_LSTE_25486_006_10SGD_20230103T0744...\n2023-01-03T07:44:22.480Z\n2023-01-03T07:45:14.450Z\nNight\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n4\nG2591892077-LPCLOUD\nECOv002_L2T_LSTE_25547_005_11SKU_20230107T0607...\n2023-01-07T06:07:21.560Z\n2023-01-07T06:08:13.530Z\nNight\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n\n\n\n\n\nThis is looking better, but we can make it more readable by renaming our columns.\n\n# Rename some Columns\ngdf.rename(columns = {'meta.concept-id':'concept_id','meta.native-id':'granule',\n                       'umm.TemporalExtent.RangeDateTime.BeginningDateTime':'start_datetime',\n                      'umm.TemporalExtent.RangeDateTime.EndingDateTime':'end_datetime',\n                      'umm.CloudCover':'cloud_cover',\n                      'umm.DataGranule.DayNightFlag':'day_night'}, inplace=True)\ngdf.head()\n\n\n\n\n\n\n\n\nconcept_id\ngranule\nstart_datetime\nend_datetime\nday_night\ncloud_cover\ngeometry\nbrowse\nshortname\n\n\n\n\n0\nG2581836170-LPCLOUD\nECOv002_L2T_LSTE_25460_016_11SKU_20230101T1552...\n2023-01-01T15:52:48.650Z\n2023-01-01T15:53:40.620Z\nDay\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n1\nG2586136519-LPCLOUD\nECOv002_L2T_LSTE_25486_005_10SGD_20230103T0743...\n2023-01-03T07:43:30.510Z\n2023-01-03T07:44:22.480Z\nNight\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n2\nG2586136993-LPCLOUD\nECOv002_L2T_LSTE_25486_006_11SKU_20230103T0744...\n2023-01-03T07:44:22.480Z\n2023-01-03T07:45:14.450Z\nNight\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n3\nG2586137000-LPCLOUD\nECOv002_L2T_LSTE_25486_006_10SGD_20230103T0744...\n2023-01-03T07:44:22.480Z\n2023-01-03T07:45:14.450Z\nNight\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n4\nG2591892077-LPCLOUD\nECOv002_L2T_LSTE_25547_005_11SKU_20230107T0607...\n2023-01-07T06:07:21.560Z\n2023-01-07T06:08:13.530Z\nNight\nNaN\nPOLYGON ((-119.06582 34.21003, -119.06582 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n\n\n\n\n\n\n\n\nNote: If querying on-premises (not cloud) LP DAAC datasets, the meta.concept-id will not show as xxxxxx-LPCLOUD. For these datasets, the granule name can be retrieved from the umm.DataGranule.Identifiers column.\n\nWe can filter using the day/night flag as well, but this step will be unnecessary as we check to ensure all results from ECOSTRESS fall within an hour of resulting EMIT granules.\n\n# gdf = gdf[gdf['day_night'].str.contains('Day')]\n\nOur first step toward filtering the datasets will be to add a column with a datetime.\n\nYou may have noticed that the date format is similar for ECOSTRESS and EMIT, but the ECOSTRESS data also includes fractional seconds. If using an different version of pandas, you may need to drop the format='ISO8601'argument to the to_datetime function, as shown in the commented-out line.\n\n\n# gdf['datetime_obj'] = pd.to_datetime(gdf['start_datetime']) # different pandas version\ngdf['datetime_obj'] = pd.to_datetime(gdf['start_datetime'], format='ISO8601')\n\nWe can roughly visualize the quantity of results by month at our location using a histogram with 8 bins (Jan - Oct).\n\ngdf.hist(column='datetime_obj', by='shortname', bins=10, color='green', edgecolor='black', linewidth=1, sharey=True)\n\narray([&lt;Axes: title={'center': 'ECO_L2T_LSTE'}&gt;,\n       &lt;Axes: title={'center': 'EMITL2ARFL'}&gt;], dtype=object)\n\n\n\n\n\n\n\n\n\nNow we will separate the results into two dataframes, one for ECOTRESS and one for EMIT, and print the number of results for each so we can monitor how many granules we’re filtering.\n\n# Suppress Setting with Copy Warning - not applicable in this use case\npd.options.mode.chained_assignment = None  # default='warn'\n\n# Split into two dataframes - ECO and EMIT\neco_gdf = gdf[gdf['granule'].str.contains('ECO')]\nemit_gdf = gdf[gdf['granule'].str.contains('EMIT')]\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 288 \n EMIT Granules: 35\n\n\n\nemit_gdf.head()\n\n\n\n\n\n\n\n\nconcept_id\ngranule\nstart_datetime\nend_datetime\nday_night\ncloud_cover\ngeometry\nbrowse\nshortname\ndatetime_obj\n\n\n\n\n31\nG2623634492-LPCLOUD\nEMIT_L2A_RFL_001_20230129T211308_2302914_003\n2023-01-29T21:13:08Z\n2023-01-29T21:13:20Z\nDay\n96.0\nPOLYGON ((-120.29004 35.49324, -121.08087 34.8...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-01-29 21:13:08+00:00\n\n\n53\nG2631040813-LPCLOUD\nEMIT_L2A_RFL_001_20230219T202939_2305013_002\n2023-02-19T20:29:39Z\n2023-02-19T20:29:51Z\nDay\n47.0\nPOLYGON ((-120.70791 35.54348, -121.19894 34.9...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-19 20:29:39+00:00\n\n\n56\nG2631045418-LPCLOUD\nEMIT_L2A_RFL_001_20230219T202951_2305013_003\n2023-02-19T20:29:51Z\n2023-02-19T20:30:14Z\nDay\n67.0\nPOLYGON ((-120.04838 35.03646, -120.54523 34.4...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-19 20:29:51+00:00\n\n\n65\nG2631457332-LPCLOUD\nEMIT_L2A_RFL_001_20230223T185536_2305412_003\n2023-02-23T18:55:36Z\n2023-02-23T18:55:48Z\nDay\n98.0\nPOLYGON ((-120.61175 35.58776, -121.10041 34.9...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-23 18:55:36+00:00\n\n\n66\nG2631458885-LPCLOUD\nEMIT_L2A_RFL_001_20230223T185548_2305412_004\n2023-02-23T18:55:48Z\n2023-02-23T18:56:00Z\nDay\n96.0\nPOLYGON ((-119.95147 35.08163, -120.44685 34.4...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-23 18:55:48+00:00\n\n\n\n\n\n\n\nWe still haven’t filtered the locations where EMIT and ECOSTRESS have data at the same spatial location and timeframe. The EMIT acquisition mask has been added to ECOSTRESS, so in most cases if EMIT is collecting data, so will ECOSTRESS, but there are edge cases where this is not true. To do this we’ll use two filters to catch the edge-cases and provide an example that can be used with other datasets.\nFirst, since EMIT has a smaller swath width, we can use a unary union of the spatial coverage present in our geodataframe to filter out ECOSTRESS granules that do not overlap with it.\n\n# Subset ECOSTRESS Granules in Geodataframe by intersection with EMIT granules\n## Create new column based on intersection with union of EMIT polygons.\neco_gdf['intersects'] = eco_gdf.intersects(emit_gdf.unary_union)\n## Apply subsetting\neco_gdf = eco_gdf[eco_gdf['intersects'] == True]\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 288 \n EMIT Granules: 35\n\n\nIn this instance, our results aren’t narrowed because our region of interest is smaller than a single EMIT scene. If the spatial ROI was very large, this would be much more unlikely.\nAdditionally, we want to make sure that data in our results are collected at the same time. For EMIT and ECOSTRESS, the EMIT acquisition mask has been added to the ECOSTRESS mask, meaning that if there is an EMIT scene, there should also be an ECOSTRESS scene acquired at the same time. In practice, however, the timestamps on the scenes can vary slightly. In order to capture this slight variability, we need to use a range instead of a single timestamp to capture concurrent data. To do this, we’ll ensure all ECOSTRESS granule start times fall within 10 minutes of any of the EMIT granules in our results, and vice-versa.\nWrite a function to evaluate whether these datetime objects fall within 10 minutes of one another using the timedelta function.\n\n# Function to Filter timestamps that do not fall within a time_delta of timestamps from the other acquisition time\ndef concurrent_match(gdf_a:pd.DataFrame, gdf_b:pd.DataFrame, col_name:str, time_delta:timedelta):\n    \"\"\"\n    Cross references dataframes containing a datetime object column and keeps rows in \n    each that fall within the provided timedelta of the other. Acceptable time_delta examples:\n    \n    months=1\n    days=1\n    hours=1\n    minutes=1\n    seconds=1 \n\n    \"\"\"\n    # Match Timestamps from Dataframe A with Time-range of entries in Dataframe B\n    # Create empty list\n    a_list = []\n    # Iterate results for product a based on index values\n    for _n in gdf_b.index.to_list():\n        # Find where product b is within the window of each product a result\n        a_matches = (gdf_a[col_name] &gt; gdf_b[col_name][_n]-time_delta) & (gdf_a[col_name] &lt; gdf_b[col_name][_n]+time_delta)\n        # Append list with values\n        a_list.append(a_matches)\n    # Match Timestamps from Dataframe B with Time-range of entries in Dataframe A\n    # Create empty list\n    b_list =[]\n    for _m in gdf_a.index.to_list():\n        # Find where product a is within the window of each product b result\n        b_matches = (gdf_b[col_name] &gt; gdf_a[col_name][_m]-time_delta) &  (gdf_b[col_name] &lt; gdf_a[col_name][_m]+time_delta)\n        # Append list with values\n        b_list.append(b_matches)\n    # Filter Original Dataframes by summing list of bools, 0 = outside of all time-ranges\n    a_filtered = gdf_a.loc[sum(a_list) &gt; 0]\n    b_filtered = gdf_b.loc[sum(b_list) &gt; 0]\n    return(a_filtered, b_filtered)\n\nNow run our function.\n\neco_gdf, emit_gdf = concurrent_match(eco_gdf,emit_gdf, col_name='datetime_obj',time_delta=timedelta(minutes=10))\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 32 \n EMIT Granules: 22",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#visualizing-intersecting-coverage",
    "href": "python/01_Finding_Concurrent_Data.html#visualizing-intersecting-coverage",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "4. Visualizing Intersecting Coverage",
    "text": "4. Visualizing Intersecting Coverage\nNow that we have geodataframes containing some concurrent data, we can visualize them on a map using folium. It’s often difficult to visualize a large time-series of scenes, so we’ve included an example in Appendix A1 on how to filter to a single day.\n\n# Plot Using Folium\n\n# Create Figure and Select Background Tiles\nfig = Figure(width=\"750px\", height=\"375px\")\nmap1 = folium.Map(tiles='https://mt1.google.com/vt/lyrs=y&x={x}&y={y}&z={z}', attr='Google')\nfig.add_child(map1)\n\n# Plot STAC ECOSTRESS Results - note we must drop the datetime_obj columns for this to work\neco_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"ECOSTRESS\",\n    m=map1,\n    legend=False\n)\n\n# Plot STAC EMITL2ARFL Results - note we must drop the datetime_obj columns for this to work\nemit_gdf.drop(columns=['datetime_obj']).explore(\n    \"granule\",\n    categorical=True,\n    tooltip=[\n        \"granule\",\n        \"start_datetime\",\n        \"cloud_cover\",\n    ],\n    popup=True,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"EMIT\",\n    m=map1,\n    legend=False\n)\n\n# ECOSTRESS Browse Images - Comment out to remove\nfor _n in eco_gdf.index.to_list():\n    folium.raster_layers.ImageOverlay(\n        image=eco_gdf['browse'][_n],\n        name=eco_gdf['granule'][_n],\n        bounds=[[eco_gdf.bounds['miny'][_n], eco_gdf.bounds['minx'][_n]], [eco_gdf.bounds['maxy'][_n], eco_gdf.bounds['maxx'][_n]]],\n        interactive=False,\n        cross_origin=False,\n        opacity=0.75,\n        zindex=1,\n        ).add_to(map1)\n\n# Plot Region of Interest\npolygon.explore(\n    popup=False,\n    style_kwds=dict(fillOpacity=0.1, width=2),\n    name=\"Region of Interest\",\n    m=map1\n)\n\nfolium.GeoJson(roi_poly,\n                name='bounding_box',\n                ).add_to(map1)\n\nmap1.fit_bounds(bounds=convert_bounds(gdf.unary_union.bounds))\nmap1.add_child(folium.LayerControl())\ndisplay(fig)\n\n\n\n\nIn the figure above, you can zoom in and out, and add or remove layers using the layer control in the top right. Notice that since we’re using the tiled ECOSTRESS product, we have 2 overlapping tiles at our ROI. You can visualize the tiles by adding or removing the layers.\nFrom this figure, we can see there are two ECOSTRESS tiles: 10SGD and 10SKU, which both intersect with our area. Since both fall within tile 10SGD, we can keep results using 10SGD as a filter.\nThere is a lot going on in the above visualization. After doing some additional filtering below, we can re-run the above cell to visualize our filtered scenes.\n\neco_gdf = eco_gdf[eco_gdf['granule'].str.contains(\"10SGD\")]\n\n\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 17 \n EMIT Granules: 22\n\n\n\n4.2 Previewing EMIT Browse Imagery\nThe EMIT browse imagery is not orthorectified, so it can’t be visualized on a plot like the ECOSTRESS browse imagery. To get an idea what scenes look like we can plot them in a grid using matplotlib.\n\nNote: The black space is indicative of onboard cloud masking that occurs before data is downlinked from the ISS.\n\n\ncols = 3\nrows = math.ceil(len(emit_gdf)/cols)\nfig, ax = plt.subplots(rows, cols, figsize=(20,20))\nax = ax.flatten()\n\nfor _n, index in enumerate(emit_gdf.index.to_list()):\n    img = io.imread(emit_gdf['browse'][index])\n    ax[_n].imshow(img)\n    ax[_n].set_title(f\"Index: {index} - {emit_gdf['granule'][index]}\")\n    ax[_n].axis('off')\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n4.3 Further Filtering\nWe can see that some of these granules likely won’t work because of the large amount of cloud cover, we can use a list of these to filter them out. Make a list of indexes to filter out. Note that these indices can change since they are dependent on the order scenes are returned from CMR. Please doublecheck them before running this cell.\n\n# Bad granule list\nbad_granules = [31,82,115,116,176,182]\n\nFilter out the bad granules.\n\nemit_gdf = emit_gdf[~emit_gdf.index.isin(bad_granules)]\n\nNow that we’ve narrowed our EMIT results we can again filter the ECOSTRESS granules based on their concurrency with our filtered EMIT granules.\n\neco_gdf, emit_gdf = concurrent_match(eco_gdf,emit_gdf, col_name='datetime_obj',time_delta=timedelta(hours=1))\nprint(f' ECOSTRESS Granules: {eco_gdf.shape[0]} \\n EMIT Granules: {emit_gdf.shape[0]}')\n\n ECOSTRESS Granules: 15 \n EMIT Granules: 16\n\n\n\neco_gdf\n\n\n\n\n\n\n\n\nconcept_id\ngranule\nstart_datetime\nend_datetime\nday_night\ncloud_cover\ngeometry\nbrowse\nshortname\ndatetime_obj\nintersects\n\n\n\n\n52\nG2627756987-LPCLOUD\nECOv002_L2T_LSTE_26223_011_10SGD_20230219T2028...\n2023-02-19T20:28:51.310Z\n2023-02-19T20:29:43.280Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-02-19 20:28:51.310000+00:00\nTrue\n\n\n54\nG2627758024-LPCLOUD\nECOv002_L2T_LSTE_26223_012_10SGD_20230219T2029...\n2023-02-19T20:29:43.280Z\n2023-02-19T20:30:35.250Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-02-19 20:29:43.280000+00:00\nTrue\n\n\n75\nG2630582431-LPCLOUD\nECOv002_L2T_LSTE_26345_013_10SGD_20230227T1720...\n2023-02-27T17:20:29.100Z\n2023-02-27T17:21:21.070Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-02-27 17:20:29.100000+00:00\nTrue\n\n\n76\nG3015778622-LPCLOUD\nECOv002_L2T_LSTE_26345_013_10SGD_20230227T1720...\n2023-02-27T17:20:29.104Z\n2023-02-27T17:21:21.074Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-02-27 17:20:29.104000+00:00\nTrue\n\n\n79\nG2630583457-LPCLOUD\nECOv002_L2T_LSTE_26345_014_10SGD_20230227T1721...\n2023-02-27T17:21:21.070Z\n2023-02-27T17:22:13.040Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-02-27 17:21:21.070000+00:00\nTrue\n\n\n81\nG3015772990-LPCLOUD\nECOv002_L2T_LSTE_26345_014_10SGD_20230227T1721...\n2023-02-27T17:21:21.074Z\n2023-02-27T17:22:13.044Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-02-27 17:21:21.074000+00:00\nTrue\n\n\n119\nG2650963815-LPCLOUD\nECOv002_L2T_LSTE_26860_001_10SGD_20230401T2037...\n2023-04-01T20:37:33.030Z\n2023-04-01T20:38:25.000Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-04-01 20:37:33.030000+00:00\nTrue\n\n\n124\nG2655983115-LPCLOUD\nECOv002_L2T_LSTE_26921_001_10SGD_20230405T1902...\n2023-04-05T19:02:58.950Z\n2023-04-05T19:03:50.920Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-04-05 19:02:58.950000+00:00\nTrue\n\n\n153\nG2710071138-LPCLOUD\nECOv002_L2T_LSTE_27185_010_10SGD_20230422T1958...\n2023-04-22T19:58:36.740Z\n2023-04-22T19:59:28.710Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-04-22 19:58:36.740000+00:00\nTrue\n\n\n174\nG2715826603-LPCLOUD\nECOv002_L2T_LSTE_27745_005_10SGD_20230528T2204...\n2023-05-28T22:04:01.690Z\n2023-05-28T22:04:53.660Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-05-28 22:04:01.690000+00:00\nTrue\n\n\n180\nG2715906073-LPCLOUD\nECOv002_L2T_LSTE_27806_005_10SGD_20230601T2026...\n2023-06-01T20:26:19.770Z\n2023-06-01T20:27:11.740Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-06-01 20:26:19.770000+00:00\nTrue\n\n\n205\nG2729123248-LPCLOUD\nECOv002_L2T_LSTE_28238_012_10SGD_20230629T1704...\n2023-06-29T17:04:16.344Z\n2023-06-29T17:05:08.313Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-06-29 17:04:16.344000+00:00\nTrue\n\n\n293\nG2772539755-LPCLOUD\nECOv002_L2T_LSTE_29576_005_10SGD_20230923T2321...\n2023-09-23T23:21:04.439Z\n2023-09-23T23:21:56.408Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-09-23 23:21:04.439000+00:00\nTrue\n\n\n307\nG2792244801-LPCLOUD\nECOv002_L2T_LSTE_29901_007_10SGD_20231014T2239...\n2023-10-14T22:39:36.895Z\n2023-10-14T22:40:28.865Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-10-14 22:39:36.895000+00:00\nTrue\n\n\n316\nG2790664680-LPCLOUD\nECOv002_L2T_LSTE_29962_012_10SGD_20231018T2102...\n2023-10-18T21:02:08.141Z\n2023-10-18T21:03:00.111Z\nDay\nNaN\nPOLYGON ((-119.59844 34.20719, -119.59844 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nECO_L2T_LSTE\n2023-10-18 21:02:08.141000+00:00\nTrue\n\n\n\n\n\n\n\n\nemit_gdf\n\n\n\n\n\n\n\n\nconcept_id\ngranule\nstart_datetime\nend_datetime\nday_night\ncloud_cover\ngeometry\nbrowse\nshortname\ndatetime_obj\n\n\n\n\n53\nG2631040813-LPCLOUD\nEMIT_L2A_RFL_001_20230219T202939_2305013_002\n2023-02-19T20:29:39Z\n2023-02-19T20:29:51Z\nDay\n47.0\nPOLYGON ((-120.70791 35.54348, -121.19894 34.9...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-19 20:29:39+00:00\n\n\n56\nG2631045418-LPCLOUD\nEMIT_L2A_RFL_001_20230219T202951_2305013_003\n2023-02-19T20:29:51Z\n2023-02-19T20:30:14Z\nDay\n67.0\nPOLYGON ((-120.04838 35.03646, -120.54523 34.4...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-19 20:29:51+00:00\n\n\n77\nG2631818088-LPCLOUD\nEMIT_L2A_RFL_001_20230227T172116_2305811_004\n2023-02-27T17:21:16Z\n2023-02-27T17:21:28Z\nDay\n99.0\nPOLYGON ((-120.73262 35.83169, -121.21741 35.1...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-02-27 17:21:16+00:00\n\n\n121\nG2648965598-LPCLOUD\nEMIT_L2A_RFL_001_20230401T203751_2309114_002\n2023-04-01T20:37:51Z\n2023-04-01T20:38:03Z\nDay\n80.0\nPOLYGON ((-120.21542 34.90659, -120.99248 34.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-04-01 20:37:51+00:00\n\n\n122\nG2648966116-LPCLOUD\nEMIT_L2A_RFL_001_20230401T203803_2309114_003\n2023-04-01T20:38:03Z\n2023-04-01T20:38:15Z\nDay\n18.0\nPOLYGON ((-119.55793 35.41507, -120.34872 34.7...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-04-01 20:38:03+00:00\n\n\n125\nG2667368816-LPCLOUD\nEMIT_L2A_RFL_001_20230405T190311_2309513_002\n2023-04-05T19:03:11Z\n2023-04-05T19:03:23Z\nDay\n71.0\nPOLYGON ((-119.87804 34.90691, -120.65480 34.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-04-05 19:03:11+00:00\n\n\n154\nG2667503312-LPCLOUD\nEMIT_L2A_RFL_001_20230422T195924_2311213_002\n2023-04-22T19:59:24Z\n2023-04-22T19:59:36Z\nDay\n40.0\nPOLYGON ((-120.49827 35.12071, -120.99520 34.4...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-04-22 19:59:24+00:00\n\n\n177\nG2699997381-LPCLOUD\nEMIT_L2A_RFL_001_20230528T220420_2314815_004\n2023-05-28T22:04:20Z\n2023-05-28T22:04:32Z\nDay\n52.0\nPOLYGON ((-119.96105 35.44150, -120.75292 34.8...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-05-28 22:04:20+00:00\n\n\n179\nG2720810376-LPCLOUD\nEMIT_L2A_RFL_001_20230601T202611_2315214_002\n2023-06-01T20:26:11Z\n2023-06-01T20:26:23Z\nDay\n15.0\nPOLYGON ((-120.29042 34.51665, -120.82317 33.9...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-06-01 20:26:11+00:00\n\n\n207\nG2735424106-LPCLOUD\nEMIT_L2A_RFL_001_20230629T170449_2318011_002\n2023-06-29T17:04:49Z\n2023-06-29T17:05:01Z\nDay\n48.0\nPOLYGON ((-120.35333 35.27559, -120.83881 34.6...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-06-29 17:04:49+00:00\n\n\n292\nG2772438951-LPCLOUD\nEMIT_L2A_RFL_001_20230923T232101_2326615_002\n2023-09-23T23:21:01Z\n2023-09-23T23:21:13Z\nDay\n73.0\nPOLYGON ((-120.52930 35.29765, -121.32658 34.6...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-09-23 23:21:01+00:00\n\n\n295\nG2772444304-LPCLOUD\nEMIT_L2A_RFL_001_20230923T232113_2326615_003\n2023-09-23T23:21:13Z\n2023-09-23T23:21:31Z\nDay\n10.0\nPOLYGON ((-119.51744 36.06493, -120.67430 35.1...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-09-23 23:21:13+00:00\n\n\n309\nG2786272677-LPCLOUD\nEMIT_L2A_RFL_001_20231014T224006_2328715_002\n2023-10-14T22:40:06Z\n2023-10-14T22:40:18Z\nDay\n66.0\nPOLYGON ((-120.79352 35.15521, -121.28294 34.5...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-10-14 22:40:06+00:00\n\n\n310\nG2786302498-LPCLOUD\nEMIT_L2A_RFL_001_20231014T224018_2328715_003\n2023-10-14T22:40:18Z\n2023-10-14T22:40:36Z\nDay\n93.0\nPOLYGON ((-120.14165 34.64471, -120.63443 34.0...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-10-14 22:40:18+00:00\n\n\n315\nG2786557891-LPCLOUD\nEMIT_L2A_RFL_001_20231018T210205_2329114_005\n2023-10-18T21:02:05Z\n2023-10-18T21:02:17Z\nDay\n11.0\nPOLYGON ((-120.72254 35.89691, -121.20394 35.2...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-10-18 21:02:05+00:00\n\n\n318\nG2786556493-LPCLOUD\nEMIT_L2A_RFL_001_20231018T210217_2329114_006\n2023-10-18T21:02:17Z\n2023-10-18T21:02:29Z\nDay\n9.0\nPOLYGON ((-120.05733 35.39170, -120.54254 34.7...\nhttps://data.lpdaac.earthdatacloud.nasa.gov/lp...\nEMITL2ARFL\n2023-10-18 21:02:17+00:00\n\n\n\n\n\n\n\nWe can now go back to our folium plot above and re-run the cell to update it based on our filtering.",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#generating-a-list-of-urls",
    "href": "python/01_Finding_Concurrent_Data.html#generating-a-list-of-urls",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "5. Generating a list of URLs",
    "text": "5. Generating a list of URLs\nWith our filtered geodataframe we can go back to our earthaccess search results and filter them.\nCreate a list of the geodataframe indices to use in filtering our search results.\n\nkeep_granules = eco_gdf.index.to_list()+emit_gdf.index.to_list()\nkeep_granules.sort()\n\nFilter the results list.\n\nfiltered_results = [result for i, result in enumerate(results) if i in keep_granules]\n\nAfter filtering our granules, also want to filter which assets we want from each granule. An asset is an individual file associated with a granule.\nGranules often have several assets associated with them, for example, ECO_L2T_LSTE has several assets: - Water Mask (water) - Cloud Mask (cloud) - Quality (QC) - Land Surface Temperature (LST) - Land Surface Temperature Error (LST_err) - Wide Band Emissivity (EmisWB) - Height (height)\nCreate a list of results urls from our filtered_results list and print the first 5 results.\n\n# Retrieve URLS for Assets\nresults_urls = [granule.data_links() for granule in filtered_results]\n\n\nresults_urls[:5]\n\n[['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_water.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_cloud.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_view_zenith.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_height.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_QC.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_LST.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_LST_err.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_EmisWB.tif'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_RFL_001_20230219T202939_2305013_002.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_RFLUNCERT_001_20230219T202939_2305013_002.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_MASK_001_20230219T202939_2305013_002.nc'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_water.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_cloud.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_view_zenith.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_height.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_QC.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST_err.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_EmisWB.tif'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_RFL_001_20230219T202951_2305013_003.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_RFLUNCERT_001_20230219T202951_2305013_003.nc',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_MASK_001_20230219T202951_2305013_003.nc'],\n ['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_water.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_cloud.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_view_zenith.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_height.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_QC.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_LST.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_LST_err.tif',\n  'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01/ECOv002_L2T_LSTE_26345_013_10SGD_20230227T172029_0710_01_EmisWB.tif']]\n\n\nThe results_urls list we just generated contains URLs for all of these assets nested by granule. We can further filter our these to remove unwanted assets by using string matching.\nCreate a list of strings and enumerate through our results_url list to filter out unwanted assets and remove the nesting.\n\nfiltered_asset_links = []\n# Pick Desired Assets (leave _ on RFL to distinguish from RFLUNC, LST. to distinguish from LST_err)\ndesired_assets = ['_RFL_','_MASK_', 'LST.'] # Add more or do individually for reflectance, reflectance uncertainty, or mask\n# Step through each sublist (granule) and filter based on desired assets.\nfor n, granule in enumerate(results_urls):\n    for url in granule: \n        asset_name = url.split('/')[-1]\n        if any(asset in asset_name for asset in desired_assets):\n            filtered_asset_links.append(url)\n# Show first 5 asset links           \nfiltered_asset_links[:5]\n\n['https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01/ECOv002_L2T_LSTE_26223_011_10SGD_20230219T202851_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_RFL_001_20230219T202939_2305013_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202939_2305013_002/EMIT_L2A_MASK_001_20230219T202939_2305013_002.nc',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/ECO_L2T_LSTE.002/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01/ECOv002_L2T_LSTE_26223_012_10SGD_20230219T202943_0710_01_LST.tif',\n 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230219T202951_2305013_003/EMIT_L2A_RFL_001_20230219T202951_2305013_003.nc']\n\n\nWrite this list of files to a text file to have a record of the search, or to use for streaming or downloading the data.\n\nwith open('../data/search_results.txt', 'w') as f:\n    for line in filtered_asset_links:\n        f.write(f\"{line}\\n\")",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#streaming-or-downloading-data",
    "href": "python/01_Finding_Concurrent_Data.html#streaming-or-downloading-data",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "6. Streaming or Downloading Data",
    "text": "6. Streaming or Downloading Data\nFor the workshop, we will stream the data, but either method can be used, and each has trade-offs based on the internet speed, storage space, or use case. The EMIT files are very large due to the number of bands, so operations can take some time if streaming with a slower internet connection. Since the workshop is hosted in a Cloud workspace, we can stream the data directly to the workspace.\n\n6.1 Streaming Data Workflow\nFor an example of streaming both netCDF and Cloud Optimized GeoTIFF (COG) data please see notebook 2, Working With EMIT Reflectance and ECOSTRESS LST.\nIf you plan to stream the data, you can stop here and move to the next notebook.\n\n\n6.2 Downloading Data Workflow\nTo download the scenes, we can use the earthaccess library to authenticate then download the files.\nFirst, log into Earthdata using the login function from the earthaccess library. The persist=True argument will create a local .netrc file if it doesn’t exist, or add your login info to an existing .netrc file. If no Earthdata Login credentials are found in the .netrc you’ll be prompted for them. As mentioned in section 1.2, this step is not necessary to conduct searches, but is needed to download or stream data.\nWe’ve included the canopy water content files in the repository to simplify the notebooks so users don’t need to perform that operation for the examples in the repository. This means that only 3 granules from our list are required to execute the notebooks and walk through the notebooks in the repository. These are included in a separate text file, required_granules.txt.\nThese can be downloading by uncommenting and running the following cells.\n\nNote: If interested users can download all of the files using the cell below and recreate all of the canopy water content files following a workflow similar to the example in notebooks 2 and 3 for all of the necessary scenes. To do this, uncomment the file_list object with the search_results.txt filepath to download all of the results rather than just what is required.\n\n\n# Authenticate using earthaccess\nearthaccess.login(persist=True)\n\n&lt;earthaccess.auth.Auth at 0x7f02746b41c0&gt;\n\n\nOpen the text file containing the URLs you wish to download.\n\n# # Open Text File and Read Lines\n# file_list = '../data/required_granules.txt'\n# # file_list = '../data/search_results.txt'\n# with open(file_list) as f:\n#     urls = [line.rstrip('\\n') for line in f]\n\nDownload the required granules.\n\n# # Get requests https Session using Earthdata Login Info\n# fs = earthaccess.get_requests_https_session()\n# # Retrieve granule asset ID from URL (to maintain existing naming convention)\n# for url in urls:\n#     granule_asset_id = url.split('/')[-1]\n#     # Define Local Filepath\n#     fp = f'../data/{granule_asset_id}'\n#     # Download the Granule Asset if it doesn't exist\n#     if not os.path.isfile(fp):\n#         with fs.get(url,stream=True) as src:\n#             with open(fp,'wb') as dst:\n#                 for chunk in src.iter_content(chunk_size=64*1024*1024):\n#                     dst.write(chunk)\n\nCongratulations, now you have downloaded concurrent data from the ECOSTRESS and EMIT instruments on the ISS.",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/01_Finding_Concurrent_Data.html#contact-info",
    "href": "python/01_Finding_Concurrent_Data.html#contact-info",
    "title": "1 Finding Concurrent ECOSTRESS and EMIT Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-17-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Notebooks",
      "1 Finding Concurrent Data"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html",
    "href": "python/03_EMIT_CWC_from_Reflectance.html",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "",
    "text": "Summary\nIn this notebook we will explore how Equivalent Water Thickness (EWT) or Canopy Water Content (CWC) can be calculated from the Earth Surface Mineral Dust Source Investigation (EMIT) L2A Reflectance Product, then we will apply this knowledge to calculate CWC over the Jack and Laura Dangermond Preserve located near Santa Barbara, CA.\nBackground\nEquivalent Water Thickness (EWT) is the predicted thickness or absorption path length in centimeters (cm) of water that would be required to yield an observed spectra. In the context of vegetation, this is equivalent to canopy water content (CWC) in g/cm^2 because a cm^3 of water has a mass of 1g.\nCWC can be derived from surface reflectance spectra because they provide information about the composition of the target, including water content. Reflectance is the fraction of incoming solar radiation reflected by Earth’s surface. Different materials reflect varying proportions of radiation based upon their chemical composition and physical properties, giving materials their own unique spectral signature or fingerprint. In particular, liquid water causes characteristic absorption features to appear in the near-infrared wavelengths of the solar spectrum, which enables an estimation of its content.\nCWC correlates with vegetation type and health, as well as wildfire risk. The methods used here to calculate CWC are based on the ISOFIT python package. The Beer-Lambert physical model used to calculate CWC is described in Green et al. (2006) and Bohn et al. (2020). It uses wavelength-dependent absorption coefficients of liquid water to determine the absorption path length as a function of absorption feature depth. Of note, this model does not account for multiple scattering effects within the canopy and may result in overestimation of CWC (Bohn et al., 2020).\nThe Jack and Laura Dangermond Preserve and its surrounding lands are one of the last “wild coastal” regions in Southern California. The preserve is over 24,000 acres and consists of several ecosystem types and is home to over 600 plant species and over 200 wildlife species.\nMore about the EMIT mission and EMIT products.\nReferences\nRequirements - NASA Earthdata Account\n- No Python setup requirements if connected to the workshop cloud instance!\n- Local Only Set up Python Environment - See setup_instructions.md in the /setup/ folder to set up a local compatible Python environment\n- Downloaded necessary files. This is done at the end of the 01_Finding_Concurrent_Data notebook.\nLearning Objectives\n- Calculate CWC of a single pixel\n- Calculate CWC of an ROI\nTutorial Outline\n3.1 Setup\n3.2 Opening EMIT Data\n3.3 Extracting Reflectance of a Pixel\n3.4 Calculating CWC\n3.4.1 Single Point\n3.4.2 DataFrame of Points\n3.5 Applying Inversion in Parallel Across an ROI",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html#setup",
    "href": "python/03_EMIT_CWC_from_Reflectance.html#setup",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "3.1 Setup",
    "text": "3.1 Setup\nUncomment the below cell and install the ray python library. This is only necessary if using the 2i2c Cloud Instance.\n\n# !pip install \"ray[default]\"\n\nImport the required libraries.\n\n# Import Packages\nimport os\n# Some cells may generate warnings that we can ignore. Comment below lines to see.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport xarray as xr\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nfrom matplotlib import pyplot as plt\nimport hvplot.xarray\nimport hvplot.pandas\nimport pandas as pd\nimport earthaccess\n\nfrom modules.emit_tools import emit_xarray\nfrom modules.ewt_calc import calc_ewt\nfrom scipy.optimize import least_squares",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html#open-an-emit-file",
    "href": "python/03_EMIT_CWC_from_Reflectance.html#open-an-emit-file",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "3.2 Open an EMIT File",
    "text": "3.2 Open an EMIT File\nEMIT L2A Reflectance Data are distributed in a non-orthocorrected spatially raw NetCDF4 (.nc) format consisting of the data and its associated metadata. To work with this data, we will use the emit_xarray function from the emit_tools.py module included in the repository.\n\n3.2.1 Streaming Data\nAs we did in the previous notebook, to stream the data, log in using earthaccess, then start and https session and open the url for the scene.\n\n# Login to NASA Earthdata\nearthaccess.login(persist=True)\n\n# Get Https Session using Earthdata Login Info\nfs = earthaccess.get_fsspec_https_session()\n\n# Define Local Filepath\nfp = fs.open('https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203751_2309114_002/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc')\n\n\n\n3.2.2 Downloading Data\nIf you have downloaded the data, you can just set the filepath.\n\n# fp = '../data/EMIT_L2A_RFL_001_20230401T203751_2309114_002.nc'\n\nOpen the file using the emit_xarray function.\n\nds = emit_xarray(fp,ortho=True).load()\n\nSet the fill values equal to np.nan improved visualization.\n\nds.reflectance.data[ds.reflectance.data == -9999] = np.nan\n\nSimilarly to what we did in the previous notebook, we can plot a single band to get an idea of where our scene is. This is the same scene we processed earlier.\n\nemit_layer = ds.sel(wavelengths=850,method='nearest')\nemit_layer.hvplot.image(cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{emit_layer.wavelengths:.3f} {emit_layer.wavelengths.units}\", xlabel='Longitude',ylabel='Latitude')",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html#extracting-reflectance-of-a-single-pixel",
    "href": "python/03_EMIT_CWC_from_Reflectance.html#extracting-reflectance-of-a-single-pixel",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "3.3 Extracting Reflectance of a Single Pixel",
    "text": "3.3 Extracting Reflectance of a Single Pixel\nWe can mask out the -.01 values used to represent the region of the spectra with strong atmospheric water vapor absorption features.\n\nds['reflectance'].data[:,:,ds['good_wavelengths'].data==0] = np.nan\n\nRetrieve the spectra from a sample point by providing a latitude and longitude along with a method using the sel function. This will select the pixel closest to the provided coordinates.\n\npoint = ds.sel(latitude=34.5399,longitude=-120.3529, method='nearest')\npoint\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 5kB\nDimensions:           (wavelengths: 285)\nCoordinates:\n  * wavelengths       (wavelengths) float32 1kB 381.0 388.4 ... 2.493e+03\n    fwhm              (wavelengths) float32 1kB 8.415 8.415 ... 8.807 8.809\n    good_wavelengths  (wavelengths) float32 1kB 1.0 1.0 1.0 1.0 ... 1.0 1.0 1.0\n    latitude          float64 8B 34.54\n    longitude         float64 8B -120.4\n    elev              float32 4B 124.2\n    spatial_ref       int64 8B 0\nData variables:\n    reflectance       (wavelengths) float32 1kB 0.01718 0.01754 ... 0.02455\nAttributes: (12/40)\n    ncei_template_version:             NCEI_NetCDF_Swath_Template_v2.0\n    summary:                           The Earth Surface Mineral Dust Source ...\n    keywords:                          Imaging Spectroscopy, minerals, EMIT, ...\n    Conventions:                       CF-1.63\n    sensor:                            EMIT (Earth Surface Mineral Dust Sourc...\n    instrument:                        EMIT\n    ...                                ...\n    spatial_ref:                       GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHER...\n    geotransform:                      [-1.20992414e+02  5.42232520e-04 -0.00...\n    day_night_flag:                    Day\n    title:                             EMIT L2A Estimated Surface Reflectance...\n    granule_id:                        EMIT_L2A_RFL_001_20230401T203751_23091...\n    Orthorectified:                    Truexarray.DatasetDimensions:wavelengths: 285Coordinates: (7)wavelengths(wavelengths)float32381.0 388.4 ... 2.486e+03 2.493e+03long_name :Wavelength Centersunits :nmarray([ 381.00558,  388.4092 ,  395.81583, ..., 2478.153  , 2485.5386 ,\n       2492.9238 ], dtype=float32)fwhm(wavelengths)float328.415 8.415 8.415 ... 8.807 8.809long_name :Full Width at Half Maxunits :nmarray([8.415, 8.415, 8.415, 8.415, 8.417, 8.418, 8.419, 8.421, 8.422,\n       8.424, 8.425, 8.426, 8.428, 8.429, 8.431, 8.432, 8.433, 8.435,\n       8.436, 8.438, 8.439, 8.44 , 8.442, 8.443, 8.445, 8.446, 8.447,\n       8.449, 8.45 , 8.452, 8.453, 8.454, 8.456, 8.457, 8.459, 8.46 ,\n       8.461, 8.463, 8.464, 8.466, 8.467, 8.468, 8.47 , 8.471, 8.473,\n       8.474, 8.475, 8.477, 8.478, 8.48 , 8.481, 8.482, 8.484, 8.485,\n       8.487, 8.488, 8.489, 8.491, 8.492, 8.494, 8.495, 8.496, 8.498,\n       8.499, 8.501, 8.502, 8.503, 8.505, 8.506, 8.508, 8.509, 8.51 ,\n       8.512, 8.513, 8.515, 8.516, 8.517, 8.519, 8.52 , 8.522, 8.523,\n       8.524, 8.526, 8.527, 8.529, 8.53 , 8.531, 8.533, 8.534, 8.536,\n       8.537, 8.538, 8.54 , 8.541, 8.543, 8.544, 8.545, 8.547, 8.548,\n       8.55 , 8.551, 8.552, 8.554, 8.555, 8.557, 8.558, 8.559, 8.561,\n       8.562, 8.564, 8.565, 8.566, 8.568, 8.569, 8.571, 8.572, 8.573,\n       8.575, 8.576, 8.578, 8.579, 8.58 , 8.582, 8.583, 8.585, 8.586,\n       8.587, 8.589, 8.59 , 8.592, 8.593, 8.594, 8.596, 8.597, 8.599,\n       8.6  , 8.601, 8.603, 8.604, 8.606, 8.607, 8.608, 8.61 , 8.611,\n       8.613, 8.614, 8.615, 8.617, 8.618, 8.62 , 8.621, 8.622, 8.624,\n       8.625, 8.627, 8.628, 8.629, 8.631, 8.632, 8.634, 8.635, 8.636,\n       8.638, 8.639, 8.641, 8.642, 8.643, 8.645, 8.646, 8.648, 8.649,\n       8.65 , 8.652, 8.653, 8.655, 8.656, 8.657, 8.659, 8.66 , 8.662,\n       8.663, 8.664, 8.666, 8.667, 8.669, 8.67 , 8.671, 8.673, 8.674,\n       8.676, 8.677, 8.678, 8.68 , 8.681, 8.683, 8.684, 8.685, 8.687,\n       8.688, 8.69 , 8.691, 8.692, 8.694, 8.695, 8.697, 8.698, 8.699,\n       8.701, 8.702, 8.704, 8.705, 8.706, 8.708, 8.709, 8.711, 8.712,\n       8.714, 8.715, 8.716, 8.718, 8.719, 8.721, 8.722, 8.723, 8.725,\n       8.726, 8.727, 8.729, 8.73 , 8.732, 8.733, 8.734, 8.736, 8.737,\n       8.739, 8.74 , 8.741, 8.743, 8.744, 8.746, 8.747, 8.748, 8.75 ,\n       8.751, 8.753, 8.754, 8.755, 8.757, 8.758, 8.76 , 8.761, 8.763,\n       8.764, 8.765, 8.767, 8.768, 8.77 , 8.771, 8.772, 8.774, 8.775,\n       8.777, 8.778, 8.779, 8.781, 8.782, 8.783, 8.785, 8.786, 8.788,\n       8.789, 8.79 , 8.792, 8.793, 8.795, 8.796, 8.797, 8.799, 8.8  ,\n       8.802, 8.803, 8.804, 8.806, 8.807, 8.809], dtype=float32)good_wavelengths(wavelengths)float321.0 1.0 1.0 1.0 ... 1.0 1.0 1.0 1.0long_name :Wavelengths where reflectance is useable: 1 = good data, 0 = bad dataunits :unitlessarray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], dtype=float32)latitude()float6434.54long_name :Latitude (WGS-84)units :degrees northarray(34.5401589)longitude()float64-120.4long_name :Longitude (WGS-84)units :degrees eastarray(-120.35285082)elev()float32124.2long_name :Surface Elevationunits :marray(124.24713, dtype=float32)spatial_ref()int640crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]array(0)Data variables: (1)reflectance(wavelengths)float320.01718 0.01754 ... 0.02635 0.02455long_name :Surface Reflectanceunits :unitlessarray([0.01718401, 0.01754001, 0.01789793, 0.01825578, 0.01887722,\n       0.01977932, 0.02089375, 0.02213017, 0.02327097, 0.02443924,\n       0.02553891, 0.02647934, 0.02731651, 0.02806388, 0.03012716,\n       0.0303051 , 0.03147712, 0.03389502, 0.03716784, 0.04125404,\n       0.04527088, 0.04904396, 0.0513751 , 0.05365279, 0.05416293,\n       0.05240294, 0.05057795, 0.04905905, 0.04850443, 0.04918665,\n       0.04959129, 0.0485189 , 0.04756864, 0.04709126, 0.04778447,\n       0.04722033, 0.04627838, 0.04591437, 0.04385856, 0.04450385,\n       0.04487438, 0.04721367, 0.05855059, 0.07728622, 0.09845995,\n       0.11936122, 0.14146721, 0.16280495, 0.17837006, 0.1891758 ,\n       0.19495177, 0.19879304, 0.20209542, 0.20503408, 0.20749208,\n       0.20980741, 0.21209788, 0.21429807, 0.2161592 , 0.21820003,\n       0.2202218 , 0.2222354 , 0.22431302, 0.22625907, 0.22809783,\n       0.2298785 , 0.23154566, 0.23314181, 0.23455934, 0.2356244 ,\n       0.23671815, 0.2378939 , 0.2389565 , 0.23990753, 0.23933375,\n       0.238821  , 0.23855045, 0.23721987, 0.23602907, 0.23627467,\n       0.23715375, 0.2386118 , 0.2409895 , 0.24408886, 0.24704838,\n       0.25001273, 0.25301102, 0.25599742, 0.258825  , 0.26146266,\n       0.26378876, 0.2659258 , 0.2676973 , 0.26931012, 0.27055526,\n       0.27149647, 0.2718703 , 0.27186936, 0.2714865 , 0.2693018 ,\n...\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan,        nan,        nan,\n              nan,        nan,        nan, 0.05113549, 0.05279826,\n       0.05452276, 0.05609974, 0.05744791, 0.05934713, 0.06077929,\n       0.06164304, 0.06214674, 0.06242346, 0.06241743, 0.06247758,\n       0.06254932, 0.062612  , 0.06495836, 0.06524801, 0.06746189,\n       0.06759153, 0.06831566, 0.06865513, 0.06930527, 0.07104894,\n       0.07243683, 0.07320293, 0.07459541, 0.07555467, 0.07578479,\n       0.07651836, 0.07675724, 0.07744075, 0.07785628, 0.0794607 ,\n       0.07826924, 0.0792022 , 0.07868779, 0.07894044, 0.07777485,\n       0.07542186, 0.07294796, 0.07028439, 0.06905952, 0.06607912,\n       0.06531868, 0.06337301, 0.0620889 , 0.05987899, 0.05752026,\n       0.05735842, 0.05715922, 0.05624693, 0.05384592, 0.05267731,\n       0.05235471, 0.05194782, 0.05061792, 0.05089531, 0.05007195,\n       0.04892436, 0.04694235, 0.04603988, 0.0461389 , 0.04569674,\n       0.0415682 , 0.04030755, 0.03803281, 0.03684125, 0.03320232,\n       0.03311669, 0.03205504, 0.02982526, 0.0263453 , 0.02454741],\n      dtype=float32)Indexes: (1)wavelengthsPandasIndexPandasIndex(Index([ 381.0055847167969,  388.4092102050781,  395.8158264160156,\n       403.22540283203125, 410.63800048828125,  418.0535888671875,\n        425.4721374511719,  432.8927001953125,  440.3172607421875,\n        447.7427978515625,\n       ...\n        2426.440185546875,  2433.830322265625,   2441.21826171875,\n          2448.6064453125,  2455.994384765625,  2463.381591796875,\n        2470.767822265625,  2478.153076171875,   2485.53857421875,\n           2492.923828125],\n      dtype='float32', name='wavelengths', length=285))Attributes: (40)ncei_template_version :NCEI_NetCDF_Swath_Template_v2.0summary :The Earth Surface Mineral Dust Source Investigation (EMIT) is an Earth Ventures-Instrument (EVI-4) Mission that maps the surface mineralogy of arid dust source regions via imaging spectroscopy in the visible and short-wave infrared (VSWIR). Installed on the International Space Station (ISS), the EMIT instrument is a Dyson imaging spectrometer that uses contiguous spectroscopic measurements from 410 to 2450 nm to resolve absoprtion features of iron oxides, clays, sulfates, carbonates, and other dust-forming minerals. During its one-year mission, EMIT will observe the sunlit Earth's dust source regions that occur within +/-52° latitude and produce maps of the source regions that can be used to improve forecasts of the role of mineral dust in the radiative forcing (warming or cooling) of the atmosphere.\\n\\nThis file contains L2A estimated surface reflectances and geolocation data. Reflectance estimates are created using an Optimal Estimation technique - see ATBD for details. Reflectance values are reported as fractions (relative to 1). Geolocation data (latitude, longitude, height) and a lookup table to project the data are also included.keywords :Imaging Spectroscopy, minerals, EMIT, dust, radiative forcingConventions :CF-1.63sensor :EMIT (Earth Surface Mineral Dust Source Investigation)instrument :EMITplatform :ISSinstitution :NASA Jet Propulsion Laboratory/California Institute of Technologylicense :https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/naming_authority :LPDAACdate_created :2023-04-06T02:26:24Zkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstdname_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventioncreator_name :Jet Propulsion Laboratory/California Institute of Technologycreator_url :https://earth.jpl.nasa.gov/emit/project :Earth Surface Mineral Dust Source Investigationproject_url :https://emit.jpl.nasa.gov/publisher_name :NASA LPDAACpublisher_url :https://lpdaac.usgs.govpublisher_email :lpdaac@usgs.govidentifier_product_doi_authority :https://doi.orgflight_line :emit20230401t203751_o09114_s000time_coverage_start :2023-04-01T20:37:51+0000time_coverage_end :2023-04-01T20:38:03+0000software_build_version :010610software_delivery_version :010610product_version :V001history :PGE Run Command: {python /beegfs/store/emit/ops/repos/emit-sds-l2a/spectrum_quality.py /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl_quality.txt}, PGE Input Files: {radiance_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_rdn_b0106_v01.img, pixel_locations_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_loc_b0106_v01.img, observation_parameters_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_obs_b0106_v01.img, surface_model_config=/beegfs/store/emit/ops/repos/emit-sds-l2a/surface/surface_20221020.json}crosstrack_orientation :as seen on groundeasternmost_longitude :-119.71545648976226northernmost_latitude :34.7990749308955westernmost_longitude :-120.992414074966southernmost_latitude :33.77100207248943spatialResolution :0.000542232520256367spatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]geotransform :[-1.20992414e+02  5.42232520e-04 -0.00000000e+00  3.47990749e+01\n -0.00000000e+00 -5.42232520e-04]day_night_flag :Daytitle :EMIT L2A Estimated Surface Reflectance 60 m V001granule_id :EMIT_L2A_RFL_001_20230401T203751_2309114_002Orthorectified :True\n\n\nWe can plot this information to see the spectra.\n\npoint.hvplot.line(x='wavelengths',y='reflectance',color='black').opts(title=f\"Latitude: {point.latitude.values:.3f} Longitude: {point.longitude.values:.3f}\")",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html#calculating-cwc",
    "href": "python/03_EMIT_CWC_from_Reflectance.html#calculating-cwc",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "3.4 Calculating CWC",
    "text": "3.4 Calculating CWC\nAs mentioned in the background we use the surface reflectance to estimate CWC. The unique spectral signatures allow identification and quantification based upon the wavelength-dependent absorption coefficients of liquid water. The EMIT mission has applied similar approaches to identify dust source minerals as well as methane point source emissions. The path length of liquid water absorption can be estimated by utilizing a least squares inversion to minimize the residuals between the EMIT reflectance and the Beer-Lambert model (Green et al.,2006), which relates the wavelength-dependent absorption to the path length the photons are traveling through the material. During the inversion, the path lengths are iteratively adjusted to match the modeled spectra to the EMIT reflectance within the water absorption feature region from 850 to 1100 nm.\nFirst, define a Beer-Lambert Model function that returns the vector of residuals between measured and modeled surface reflectance.\n\n# https://github.com/isofit/isofit/blob/main/isofit/inversion/inverse_simple.py#L514C1-L532C17\ndef beer_lambert_model(x, y, wl, alpha_lw):\n    \"\"\"Function, which computes the vector of residuals between measured and modeled surface reflectance optimizing\n    for path length of surface liquid water based on the Beer-Lambert attenuation law.\n\n    Args:\n        x:        state vector (liquid water path length, intercept, slope)\n        y:        measurement (surface reflectance spectrum)\n        wl:       instrument wavelengths\n        alpha_lw: wavelength dependent absorption coefficients of liquid water\n\n    Returns:\n        resid: residual between modeled and measured surface reflectance\n    \"\"\"\n\n    attenuation = np.exp(-x[0] * 1e7 * alpha_lw)\n    rho = (x[1] + x[2] * wl) * attenuation\n    resid = rho - y\n\n    return resid\n\nWe need some lab measurements of the complex refractive index of liquid water to obtain the wavelength-dependent absorption coefficients. They are calculated by taking four times the product of Pi and the imaginary part of the refractive index, divided by wavelength. The refractive index of liquid water per wavelength is provided by the k_liquid_water_ice.csv in the data folder. We can also preview this data to get a better understanding of the information we are using.\n\nwp_fp = '../data/k_liquid_water_ice.csv'\nk_wi = pd.read_csv(wp_fp)\nk_wi.head()\n\n\n\n\n\n\n\n\nwvl_1\nT = 22°C\nwvl_2\nT = -8°C\nwvl_3\nT = -25°C\nwvl_4\nT = -7°C\nwvl_5\nT = 25°C (H)\nwvl_6\nT = 20°C\nwvl_7\nT = 25°C (S)\nIndex\n\n\n\n\n0\n666.7\n2.470000e-08\nNaN\nNaN\nNaN\nNaN\n660.0\n1.660000e-08\n650.0\n1.640000e-08\n650.0\n1.870000e-08\n650.12971\n1.674130e-08\n0\n\n\n1\n667.6\n2.480000e-08\nNaN\nNaN\nNaN\nNaN\n670.0\n1.890000e-08\n675.0\n2.230000e-08\n651.0\n1.890000e-08\n654.63616\n1.777420e-08\n1\n\n\n2\n668.4\n2.480000e-08\nNaN\nNaN\nNaN\nNaN\n680.0\n2.090000e-08\n700.0\n3.350000e-08\n652.0\n1.910000e-08\n660.69347\n1.939950e-08\n2\n\n\n3\n669.3\n2.520000e-08\nNaN\nNaN\nNaN\nNaN\n690.0\n2.400000e-08\n725.0\n9.150000e-08\n653.0\n1.940000e-08\n665.27314\n2.031380e-08\n3\n\n\n4\n670.2\n2.530000e-08\nNaN\nNaN\nNaN\nNaN\n700.0\n2.900000e-08\n750.0\n1.560000e-07\n654.0\n1.970000e-08\n669.88461\n2.097930e-08\n4\n\n\n\n\n\n\n\n\nfig, axs = plt.subplots(2,4, figsize=(15, 6),  sharex=True, sharey=True, constrained_layout=True)\naxs = axs.ravel()\ncol_n = 0\nfor i in range(0, 7):\n    x = k_wi.iloc[:, col_n+i]\n    y = k_wi.iloc[:, col_n+i+1]\n    axs[i].scatter(x, y)\n    axs[i].set_title(y.name)\n    col_n+=1\nfig.supylabel('imaginary parts of refractive index')\nfig.supxlabel('wavelength')\nplt.show()\n\n\n\n\n\n\n\n\nNow define a function to get the desired data from the csv file.\n\n# https://github.com/isofit/isofit/blob/dev/isofit/core/common.py#L461C1-L488C26\ndef get_refractive_index(k_wi, a, b, col_wvl, col_k):\n    \"\"\"Convert refractive index table entries to numpy array.\n\n    Args:\n        k_wi:    variable\n        a:       start line\n        b:       end line\n        col_wvl: wavelength column in pandas table\n        col_k:   k column in pandas table\n\n    Returns:\n        wvl_arr: array of wavelengths\n        k_arr:   array of imaginary parts of refractive index\n    \"\"\"\n\n    wvl_ = []\n    k_ = []\n\n    for ii in range(a, b):\n        wvl = k_wi.at[ii, col_wvl]\n        k = k_wi.at[ii, col_k]\n        wvl_.append(wvl)\n        k_.append(k)\n\n    wvl_arr = np.asarray(wvl_)\n    k_arr = np.asarray(k_)\n\n    return wvl_arr, k_arr\n\nLastly, to calculate CWC we define a function that uses least squares optimization to minimize the residuals of our Beer-Lambert Model and find a likely path length of liquid water.\n\n# https://github.com/isofit/isofit/blob/main/isofit/inversion/inverse_simple.py#L443C1-L511C24\ndef invert_liquid_water(\n    rfl_meas: np.array,\n    wl: np.array,\n    l_shoulder: float = 850,\n    r_shoulder: float = 1100,\n    lw_init: tuple = (0.02, 0.3, 0.0002),\n    lw_bounds: tuple = ([0, 0.5], [0, 1.0], [-0.0004, 0.0004]),\n    ewt_detection_limit: float = 0.5,\n    return_abs_co: bool = False,\n):\n    \"\"\"Given a reflectance estimate, fit a state vector including liquid water path length\n    based on a simple Beer-Lambert surface model.\n\n    Args:\n        rfl_meas:            surface reflectance spectrum\n        wl:                  instrument wavelengths, must be same size as rfl_meas\n        l_shoulder:          wavelength of left absorption feature shoulder\n        r_shoulder:          wavelength of right absorption feature shoulder\n        lw_init:             initial guess for liquid water path length, intercept, and slope\n        lw_bounds:           lower and upper bounds for liquid water path length, intercept, and slope\n        ewt_detection_limit: upper detection limit for ewt\n        return_abs_co:       if True, returns absorption coefficients of liquid water\n\n    Returns:\n        solution: estimated liquid water path length, intercept, and slope based on a given surface reflectance\n    \"\"\"\n    \n    # Ensure least squares is done with float64 datatype (added)\n    wl = np.float64(wl)\n    \n    # params needed for liquid water fitting\n    lw_feature_left = np.argmin(abs(l_shoulder - wl))\n    lw_feature_right = np.argmin(abs(r_shoulder - wl))\n    wl_sel = wl[lw_feature_left : lw_feature_right + 1]\n\n    # adjust upper detection limit for ewt if specified\n    if ewt_detection_limit != 0.5:\n        lw_bounds[0][1] = ewt_detection_limit\n\n    # load imaginary part of liquid water refractive index and calculate wavelength dependent absorption coefficient\n    # __file__ should live at isofit/isofit/inversion/\n    \n    \n    data_dir_path = \"../data/\"\n    path_k = os.path.join(data_dir_path,\"k_liquid_water_ice.csv\")\n    \n    #isofit_path = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))\n    #path_k = os.path.join(isofit_path, \"data\", \"iop\", \"k_liquid_water_ice.xlsx\")\n\n    # k_wi = pd.read_excel(io=path_k, sheet_name=\"Sheet1\", engine=\"openpyxl\")\n    # wl_water, k_water = get_refractive_index(\n    #     k_wi=k_wi, a=0, b=982, col_wvl=\"wvl_6\", col_k=\"T = 20°C\"\n    # )\n    k_wi = pd.read_csv(path_k)\n    wl_water, k_water = get_refractive_index(\n        k_wi=k_wi, a=0, b=982, col_wvl=\"wvl_6\", col_k=\"T = 20°C\"\n    )\n    kw = np.interp(x=wl_sel, xp=wl_water, fp=k_water)\n    abs_co_w = 4 * np.pi * kw / wl_sel\n\n    rfl_meas_sel = rfl_meas[lw_feature_left : lw_feature_right + 1]\n\n    x_opt = least_squares(\n        fun=beer_lambert_model,\n        x0=lw_init,\n        jac=\"2-point\",\n        method=\"trf\",\n        bounds=(\n            np.array([lw_bounds[ii][0] for ii in range(3)]),\n            np.array([lw_bounds[ii][1] for ii in range(3)]),\n        ),\n        max_nfev=15,\n        args=(rfl_meas_sel, wl_sel, abs_co_w),\n    )\n\n    solution = x_opt.x\n\n    if return_abs_co:\n        return solution, abs_co_w\n    else:\n        return solution\n\n\n3.4.1 CWC of a Single Point\nNow that we have all of the pieces, we can estimate the CWC of a single pixel using our invert_liquid_water function. By default there is a detection limit of 0.5, if we are hitting this threshold we can adjust by changing the ewt_detection_limit argument.\n\newt = invert_liquid_water(point.reflectance.values,point.wavelengths.values)\nprint(f\"EWT for ({point.longitude.values:.3f},{point.latitude.values:.3f}): {ewt[0]:.3f} cm\")\n\nEWT for (-120.353,34.540): 0.149 cm\n\n\n\n\n3.4.2 CWC of a DataFrame of Points\nWe can also apply this function to the point data we selected in the previous notebook with the interactive plot.\nRead in the csv file we created.\n\npoints_df = pd.read_csv(\"../data/emit_click_data.csv\")\npoints_df\n\n\n\n\n\n\n\n\nid\nx\ny\n381.00558\n388.4092\n395.81583\n403.2254\n410.638\n418.0536\n425.47214\n...\n2426.4402\n2433.8303\n2441.2183\n2448.6064\n2455.9944\n2463.3816\n2470.7678\n2478.153\n2485.5386\n2492.9238\n\n\n\n\n0\n0\n-120.569492\n34.574552\n0.018037\n0.017798\n0.017567\n0.017348\n0.017523\n0.018065\n0.018957\n...\n0.034822\n0.034534\n0.030492\n0.031268\n0.027023\n0.026327\n0.024637\n0.023704\n0.015793\n0.013392\n\n\n1\n1\n-120.568286\n34.591054\n0.012029\n0.011957\n0.011888\n0.011826\n0.012031\n0.012486\n0.013151\n...\n0.020698\n0.022204\n0.020115\n0.019911\n0.016761\n0.017437\n0.015805\n0.016146\n0.014353\n0.012923\n\n\n2\n2\n-120.371752\n34.484274\n0.020833\n0.021323\n0.021819\n0.022317\n0.023065\n0.024095\n0.025361\n...\n0.059581\n0.058129\n0.053620\n0.053622\n0.048783\n0.044683\n0.044064\n0.041562\n0.034080\n0.028254\n\n\n3\n3\n-120.181848\n34.483303\n0.024848\n0.028178\n0.034840\n0.038940\n0.043875\n0.047232\n0.050809\n...\n0.131800\n0.135400\n0.122960\n0.118844\n0.108201\n0.112605\n0.106271\n0.101222\n0.070507\n0.059903\n\n\n4\n4\n-120.407924\n34.591054\n0.040804\n0.044085\n0.048493\n0.054733\n0.059349\n0.064003\n0.067749\n...\n0.266081\n0.263995\n0.258724\n0.252158\n0.228920\n0.226200\n0.212124\n0.181446\n0.130708\n0.111522\n\n\n5\n5\n-120.438851\n34.712298\n0.015452\n0.016034\n0.016616\n0.017204\n0.017848\n0.018621\n0.019477\n...\n0.046427\n0.046706\n0.043317\n0.044288\n0.038928\n0.037683\n0.038662\n0.035936\n0.034616\n0.033542\n\n\n6\n6\n-120.512401\n34.648230\n0.044019\n0.039991\n0.039511\n0.045176\n0.045753\n0.047011\n0.048643\n...\n0.081432\n0.080438\n0.076009\n0.076724\n0.069166\n0.069164\n0.066233\n0.063523\n0.059582\n0.056769\n\n\n7\n7\n-120.350229\n34.626389\n0.018255\n0.018679\n0.019107\n0.019550\n0.020291\n0.021369\n0.022694\n...\n0.050578\n0.047565\n0.046092\n0.046638\n0.039017\n0.036911\n0.037903\n0.033778\n0.028663\n0.028218\n\n\n8\n8\n-120.355052\n34.713269\n0.017943\n0.018290\n0.018641\n0.019003\n0.019586\n0.020435\n0.021492\n...\n0.047776\n0.048551\n0.045117\n0.045340\n0.039024\n0.038904\n0.039077\n0.035324\n0.030073\n0.029010\n\n\n9\n9\n-120.243522\n34.645318\n0.021041\n0.021242\n0.021448\n0.021661\n0.022252\n0.023214\n0.024442\n...\n0.049286\n0.047242\n0.042362\n0.045277\n0.039692\n0.040241\n0.037825\n0.034632\n0.029134\n0.029248\n\n\n\n\n10 rows × 288 columns\n\n\n\nNow create an array of wavelengths from the column names starting with the first wavelength value (column 3).\n\n# Get wavelength values\nwavelength_values = points_df.columns[3::].to_numpy()\n\nIterate by row through our dataframe, selecting the reflectance values and providing them to the invert_liquid_water function. Afterwards, add an CWC column to our dataframe.\n\n# Create empty list\newt_values = []\n# Iterate through rows\nfor _i in points_df.index.to_list():\n    # Get reflectance array to pass to function\n    rfl_values = points_df.iloc[_i,3::]\n    # Use invert liquid water function and append results to list\n    ewt_values.append(invert_liquid_water(rfl_values,wavelength_values)[0])    \n# Add to our existing dataframe at Column Index 3\npoints_df.insert(3, \"ewt\", ewt_values)\n\n\npoints_df\n\n\n\n\n\n\n\n\nid\nx\ny\newt\n381.00558\n388.4092\n395.81583\n403.2254\n410.638\n418.0536\n...\n2426.4402\n2433.8303\n2441.2183\n2448.6064\n2455.9944\n2463.3816\n2470.7678\n2478.153\n2485.5386\n2492.9238\n\n\n\n\n0\n0\n-120.569492\n34.574552\n2.773249e-01\n0.018037\n0.017798\n0.017567\n0.017348\n0.017523\n0.018065\n...\n0.034822\n0.034534\n0.030492\n0.031268\n0.027023\n0.026327\n0.024637\n0.023704\n0.015793\n0.013392\n\n\n1\n1\n-120.568286\n34.591054\n2.495512e-01\n0.012029\n0.011957\n0.011888\n0.011826\n0.012031\n0.012486\n...\n0.020698\n0.022204\n0.020115\n0.019911\n0.016761\n0.017437\n0.015805\n0.016146\n0.014353\n0.012923\n\n\n2\n2\n-120.371752\n34.484274\n1.639785e-01\n0.020833\n0.021323\n0.021819\n0.022317\n0.023065\n0.024095\n...\n0.059581\n0.058129\n0.053620\n0.053622\n0.048783\n0.044683\n0.044064\n0.041562\n0.034080\n0.028254\n\n\n3\n3\n-120.181848\n34.483303\n5.938623e-02\n0.024848\n0.028178\n0.034840\n0.038940\n0.043875\n0.047232\n...\n0.131800\n0.135400\n0.122960\n0.118844\n0.108201\n0.112605\n0.106271\n0.101222\n0.070507\n0.059903\n\n\n4\n4\n-120.407924\n34.591054\n4.001294e-11\n0.040804\n0.044085\n0.048493\n0.054733\n0.059349\n0.064003\n...\n0.266081\n0.263995\n0.258724\n0.252158\n0.228920\n0.226200\n0.212124\n0.181446\n0.130708\n0.111522\n\n\n5\n5\n-120.438851\n34.712298\n6.409762e-02\n0.015452\n0.016034\n0.016616\n0.017204\n0.017848\n0.018621\n...\n0.046427\n0.046706\n0.043317\n0.044288\n0.038928\n0.037683\n0.038662\n0.035936\n0.034616\n0.033542\n\n\n6\n6\n-120.512401\n34.648230\n2.607293e-10\n0.044019\n0.039991\n0.039511\n0.045176\n0.045753\n0.047011\n...\n0.081432\n0.080438\n0.076009\n0.076724\n0.069166\n0.069164\n0.066233\n0.063523\n0.059582\n0.056769\n\n\n7\n7\n-120.350229\n34.626389\n1.505599e-01\n0.018255\n0.018679\n0.019107\n0.019550\n0.020291\n0.021369\n...\n0.050578\n0.047565\n0.046092\n0.046638\n0.039017\n0.036911\n0.037903\n0.033778\n0.028663\n0.028218\n\n\n8\n8\n-120.355052\n34.713269\n1.379378e-01\n0.017943\n0.018290\n0.018641\n0.019003\n0.019586\n0.020435\n...\n0.047776\n0.048551\n0.045117\n0.045340\n0.039024\n0.038904\n0.039077\n0.035324\n0.030073\n0.029010\n\n\n9\n9\n-120.243522\n34.645318\n1.954748e-01\n0.021041\n0.021242\n0.021448\n0.021661\n0.022252\n0.023214\n...\n0.049286\n0.047242\n0.042362\n0.045277\n0.039692\n0.040241\n0.037825\n0.034632\n0.029134\n0.029248\n\n\n\n\n10 rows × 289 columns\n\n\n\nFor larger sets of point data we would want to do this in parallel. We can also calculate CWC for an area, where we definitely need to utilize parallel processing.",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html#cwc-calculation-of-an-roi",
    "href": "python/03_EMIT_CWC_from_Reflectance.html#cwc-calculation-of-an-roi",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "3.5 CWC Calculation of an ROI",
    "text": "3.5 CWC Calculation of an ROI\nIn the previous notebook, we subset our region of interest and exported the file. Since the CWC calculation is computationally intensive, it can take a while to process large scenes, so it is more efficient to do this spatial subsetting up front. We can use a function included in the ewt_calc.py module to calculate CWC on a cropped image, and create a cloud-optimized GeoTIFF (COG) file containing the results.\nSet our input filepaths and output directory.\n\nfp = '../data/EMIT_L2A_RFL_001_20230401T203751_2309114_002_dangermond.nc'\n\n\nout_dir = '../data/'\n\n\nroi_ds = xr.open_dataset(fp, decode_coords='all')\nroi_ds\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 73MB\nDimensions:           (wavelengths: 285, latitude: 244, longitude: 262)\nCoordinates:\n  * wavelengths       (wavelengths) float32 1kB 381.0 388.4 ... 2.493e+03\n    fwhm              (wavelengths) float32 1kB ...\n    good_wavelengths  (wavelengths) float32 1kB ...\n  * latitude          (latitude) float64 2kB 34.57 34.57 34.57 ... 34.44 34.44\n  * longitude         (longitude) float64 2kB -120.5 -120.5 ... -120.4 -120.4\n    elev              (latitude, longitude) float32 256kB ...\n    spatial_ref       int64 8B ...\nData variables:\n    reflectance       (latitude, longitude, wavelengths) float32 73MB ...\nAttributes: (12/40)\n    ncei_template_version:             NCEI_NetCDF_Swath_Template_v2.0\n    summary:                           The Earth Surface Mineral Dust Source ...\n    keywords:                          Imaging Spectroscopy, minerals, EMIT, ...\n    Conventions:                       CF-1.63\n    sensor:                            EMIT (Earth Surface Mineral Dust Sourc...\n    instrument:                        EMIT\n    ...                                ...\n    spatial_ref:                       GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHER...\n    geotransform:                      [-1.20992414e+02  5.42232520e-04 -0.00...\n    day_night_flag:                    Day\n    title:                             EMIT L2A Estimated Surface Reflectance...\n    granule_id:                        EMIT_L2A_RFL_001_20230401T203751_23091...\n    Orthorectified:                    Truexarray.DatasetDimensions:wavelengths: 285latitude: 244longitude: 262Coordinates: (7)wavelengths(wavelengths)float32381.0 388.4 ... 2.486e+03 2.493e+03long_name :Wavelength Centersunits :nmarray([ 381.00558,  388.4092 ,  395.81583, ..., 2478.153  , 2485.5386 ,\n       2492.9238 ], dtype=float32)fwhm(wavelengths)float32...long_name :Full Width at Half Maxunits :nm[285 values with dtype=float32]good_wavelengths(wavelengths)float32...long_name :Wavelengths where reflectance is useable: 1 = good data, 0 = bad dataunits :unitless[285 values with dtype=float32]latitude(latitude)float6434.57 34.57 34.57 ... 34.44 34.44long_name :latitudeunits :degrees_northaxis :Ystandard_name :latitudearray([34.57432 , 34.573777, 34.573235, ..., 34.443642, 34.443099, 34.442557])longitude(longitude)float64-120.5 -120.5 ... -120.4 -120.4long_name :longitudeunits :degrees_eastaxis :Xstandard_name :longitudearray([-120.499254, -120.498711, -120.498169, ..., -120.358815, -120.358273,\n       -120.357731])elev(latitude, longitude)float32...long_name :Surface Elevationunits :m[63928 values with dtype=float32]spatial_ref()int64...crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-120.49952471405297 0.0005422325202563338 0.0 34.57459066750936 0.0 -0.0005422325202563691[1 values with dtype=int64]Data variables: (1)reflectance(latitude, longitude, wavelengths)float32...long_name :Surface Reflectanceunits :unitless[18219480 values with dtype=float32]Indexes: (3)wavelengthsPandasIndexPandasIndex(Index([ 381.0055847167969,  388.4092102050781,  395.8158264160156,\n       403.22540283203125, 410.63800048828125,  418.0535888671875,\n        425.4721374511719,  432.8927001953125,  440.3172607421875,\n        447.7427978515625,\n       ...\n        2426.440185546875,  2433.830322265625,   2441.21826171875,\n          2448.6064453125,  2455.994384765625,  2463.381591796875,\n        2470.767822265625,  2478.153076171875,   2485.53857421875,\n           2492.923828125],\n      dtype='float32', name='wavelengths', length=285))latitudePandasIndexPandasIndex(Index([ 34.57431955124923,  34.57377731872898,  34.57323508620872,\n       34.572692853688466, 34.572150621168205,  34.57160838864795,\n         34.5710661561277,  34.57052392360744, 34.569981691087186,\n       34.569439458566926,\n       ...\n        34.44743714150925,  34.44689490898899, 34.446352676468734,\n       34.445810443948474,  34.44526821142822,  34.44472597890796,\n        34.44418374638771,  34.44364151386745, 34.443099281347195,\n       34.442557048826934],\n      dtype='float64', name='latitude', length=244))longitudePandasIndexPandasIndex(Index([-120.49925359779284, -120.49871136527258, -120.49816913275234,\n       -120.49762690023208, -120.49708466771182, -120.49654243519157,\n       -120.49600020267131, -120.49545797015105, -120.49491573763079,\n       -120.49437350511054,\n       ...\n       -120.36261100268824, -120.36206877016798, -120.36152653764773,\n       -120.36098430512747, -120.36044207260721, -120.35989984008695,\n       -120.35935760756671, -120.35881537504645, -120.35827314252619,\n       -120.35773091000594],\n      dtype='float64', name='longitude', length=262))Attributes: (40)ncei_template_version :NCEI_NetCDF_Swath_Template_v2.0summary :The Earth Surface Mineral Dust Source Investigation (EMIT) is an Earth Ventures-Instrument (EVI-4) Mission that maps the surface mineralogy of arid dust source regions via imaging spectroscopy in the visible and short-wave infrared (VSWIR). Installed on the International Space Station (ISS), the EMIT instrument is a Dyson imaging spectrometer that uses contiguous spectroscopic measurements from 410 to 2450 nm to resolve absoprtion features of iron oxides, clays, sulfates, carbonates, and other dust-forming minerals. During its one-year mission, EMIT will observe the sunlit Earth's dust source regions that occur within +/-52° latitude and produce maps of the source regions that can be used to improve forecasts of the role of mineral dust in the radiative forcing (warming or cooling) of the atmosphere.\\n\\nThis file contains L2A estimated surface reflectances and geolocation data. Reflectance estimates are created using an Optimal Estimation technique - see ATBD for details. Reflectance values are reported as fractions (relative to 1). Geolocation data (latitude, longitude, height) and a lookup table to project the data are also included.keywords :Imaging Spectroscopy, minerals, EMIT, dust, radiative forcingConventions :CF-1.63sensor :EMIT (Earth Surface Mineral Dust Source Investigation)instrument :EMITplatform :ISSinstitution :NASA Jet Propulsion Laboratory/California Institute of Technologylicense :https://science.nasa.gov/earth-science/earth-science-data/data-information-policy/naming_authority :LPDAACdate_created :2023-04-06T02:26:24Zkeywords_vocabulary :NASA Global Change Master Directory (GCMD) Science Keywordsstdname_vocabulary :NetCDF Climate and Forecast (CF) Metadata Conventioncreator_name :Jet Propulsion Laboratory/California Institute of Technologycreator_url :https://earth.jpl.nasa.gov/emit/project :Earth Surface Mineral Dust Source Investigationproject_url :https://emit.jpl.nasa.gov/publisher_name :NASA LPDAACpublisher_url :https://lpdaac.usgs.govpublisher_email :lpdaac@usgs.govidentifier_product_doi_authority :https://doi.orgflight_line :emit20230401t203751_o09114_s000time_coverage_start :2023-04-01T20:37:51+0000time_coverage_end :2023-04-01T20:38:03+0000software_build_version :010610software_delivery_version :010610product_version :V001history :PGE Run Command: {python /beegfs/store/emit/ops/repos/emit-sds-l2a/spectrum_quality.py /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl /tmp/emit/ops/emit20230401t203751_emit.L2AReflectance_20230405t233102/output/emit20230401t203751_rfl_quality.txt}, PGE Input Files: {radiance_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_rdn_b0106_v01.img, pixel_locations_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_loc_b0106_v01.img, observation_parameters_file=/beegfs/store/emit/ops/data/acquisitions/20230401/emit20230401t203751/l1b/emit20230401t203751_o09114_s000_l1b_obs_b0106_v01.img, surface_model_config=/beegfs/store/emit/ops/repos/emit-sds-l2a/surface/surface_20221020.json}crosstrack_orientation :as seen on groundeasternmost_longitude :-119.71545648976226northernmost_latitude :34.7990749308955westernmost_longitude :-120.992414074966southernmost_latitude :33.77100207248943spatialResolution :0.000542232520256367spatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]geotransform :[-1.20992414e+02  5.42232520e-04 -0.00000000e+00  3.47990749e+01\n -0.00000000e+00 -5.42232520e-04]day_night_flag :Daytitle :EMIT L2A Estimated Surface Reflectance 60 m V001granule_id :EMIT_L2A_RFL_001_20230401T203751_2309114_002Orthorectified :True\n\n\n\nroi_ds.sel(wavelengths=850,method='nearest').hvplot.image(x='longitude',y='latitude',cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"Dangermond ROI - RFL at 850 nm\", xlabel='Longitude',ylabel='Latitude')\n\n\n\n\n\n  \n\n\n\n\nUse the calc_ewt function to calculate CWC of the cropped image. This function will also create a COG file containing the CWC results. We can also specify the number of CPUs to use manually with a n_cpu argument, or leave it blank to use all but one of the available CPUs. If we set the return_cwc argument to true, the function will also return the COG.\nThis will take some time, about 5 minutes, because we’re doing the calculation for roughly 63,000 pixels. Also note that here we provide the ewt_detection_limit to increase it from the default of 0.5 in the function. We do this because there are several regions containing plants that hold significant quantities of water in this scene.\n\n%%time\nds_cwc = calc_ewt(fp, out_dir, ewt_detection_limit=1.5, return_cwc=True)\n\n\n2024-05-20 18:20:14,143    WARNING services.py:2009 -- WARNING: The object store is using /tmp instead of /dev/shm because /dev/shm has only 67104768 bytes available. This will harm performance! You may be able to free up space by deleting files in /dev/shm. If you are inside a Docker container, you can increase /dev/shm size by passing '--shm-size=2.37gb' to 'docker run' (or add it to the run_options list in a Ray cluster config). Make sure to set this to more than 30% of available RAM.\n\n2024-05-20 18:20:14,292 INFO worker.py:1740 -- Started a local Ray instance. View the dashboard at 127.0.0.1:8265 \n\n\n\n\nCPU times: user 848 ms, sys: 351 ms, total: 1.2 s\nWall time: 5min 1s\n\n\n\nds_cwc\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n&lt;xarray.Dataset&gt; Size: 771kB\nDimensions:      (latitude: 244, longitude: 262)\nCoordinates:\n  * latitude     (latitude) float64 2kB 34.57 34.57 34.57 ... 34.44 34.44 34.44\n  * longitude    (longitude) float64 2kB -120.5 -120.5 -120.5 ... -120.4 -120.4\n    elev         (latitude, longitude) float32 256kB ...\n    spatial_ref  int64 8B ...\nData variables:\n    cwc          (latitude, longitude) float64 511kB nan nan nan ... nan nan nan\nAttributes: (12/13)\n    flight_line:            emit20230401t203751_o09114_s000\n    time_coverage_start:    2023-04-01T20:37:51+0000\n    time_coverage_end:      2023-04-01T20:38:03+0000\n    easternmost_longitude:  -119.71545648976226\n    northernmost_latitude:  34.7990749308955\n    westernmost_longitude:  -120.992414074966\n    ...                     ...\n    spatialResolution:      0.000542232520256367\n    spatial_ref:            GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84...\n    geotransform:           [-1.20992414e+02  5.42232520e-04 -0.00000000e+00 ...\n    day_night_flag:         Day\n    title:                  EMIT Estimated Equivalent Water Thickness (EWT) /...\n    granule_id:             EMIT_L2A_RFL_001_20230401T203751_2309114_002xarray.DatasetDimensions:latitude: 244longitude: 262Coordinates: (4)latitude(latitude)float6434.57 34.57 34.57 ... 34.44 34.44long_name :latitudeunits :degrees_northaxis :Ystandard_name :latitudearray([34.57432 , 34.573777, 34.573235, ..., 34.443642, 34.443099, 34.442557])longitude(longitude)float64-120.5 -120.5 ... -120.4 -120.4long_name :longitudeunits :degrees_eastaxis :Xstandard_name :longitudearray([-120.499254, -120.498711, -120.498169, ..., -120.358815, -120.358273,\n       -120.357731])elev(latitude, longitude)float32...long_name :Surface Elevationunits :m[63928 values with dtype=float32]spatial_ref()int64...crs_wkt :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]semi_major_axis :6378137.0semi_minor_axis :6356752.314245179inverse_flattening :298.257223563reference_ellipsoid_name :WGS 84longitude_of_prime_meridian :0.0prime_meridian_name :Greenwichgeographic_crs_name :WGS 84horizontal_datum_name :World Geodetic System 1984grid_mapping_name :latitude_longitudespatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]GeoTransform :-120.49952471405297 0.0005422325202563338 0.0 34.57459066750936 0.0 -0.0005422325202563691[1 values with dtype=int64]Data variables: (1)cwc(latitude, longitude)float64nan nan nan nan ... nan nan nan nanlong_name :Canopy Water Contentunits :g/cm^2_FillValue :-9999array([[nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       ...,\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan],\n       [nan, nan, nan, ..., nan, nan, nan]])Indexes: (2)latitudePandasIndexPandasIndex(Index([ 34.57431955124923,  34.57377731872898,  34.57323508620872,\n       34.572692853688466, 34.572150621168205,  34.57160838864795,\n         34.5710661561277,  34.57052392360744, 34.569981691087186,\n       34.569439458566926,\n       ...\n        34.44743714150925,  34.44689490898899, 34.446352676468734,\n       34.445810443948474,  34.44526821142822,  34.44472597890796,\n        34.44418374638771,  34.44364151386745, 34.443099281347195,\n       34.442557048826934],\n      dtype='float64', name='latitude', length=244))longitudePandasIndexPandasIndex(Index([-120.49925359779284, -120.49871136527258, -120.49816913275234,\n       -120.49762690023208, -120.49708466771182, -120.49654243519157,\n       -120.49600020267131, -120.49545797015105, -120.49491573763079,\n       -120.49437350511054,\n       ...\n       -120.36261100268824, -120.36206877016798, -120.36152653764773,\n       -120.36098430512747, -120.36044207260721, -120.35989984008695,\n       -120.35935760756671, -120.35881537504645, -120.35827314252619,\n       -120.35773091000594],\n      dtype='float64', name='longitude', length=262))Attributes: (13)flight_line :emit20230401t203751_o09114_s000time_coverage_start :2023-04-01T20:37:51+0000time_coverage_end :2023-04-01T20:38:03+0000easternmost_longitude :-119.71545648976226northernmost_latitude :34.7990749308955westernmost_longitude :-120.992414074966southernmost_latitude :33.77100207248943spatialResolution :0.000542232520256367spatial_ref :GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]]geotransform :[-1.20992414e+02  5.42232520e-04 -0.00000000e+00  3.47990749e+01\n -0.00000000e+00 -5.42232520e-04]day_night_flag :Daytitle :EMIT Estimated Equivalent Water Thickness (EWT) / Canopy Water Content (CWC)granule_id :EMIT_L2A_RFL_001_20230401T203751_2309114_002\n\n\nPlot CWC of our ROI.\n\nds_cwc.hvplot.image(x='longitude',y='latitude',cmap='viridis',geo=True, tiles='ESRI', frame_width=720,frame_height=405, alpha=0.7, fontscale=2).opts(\n    title=f\"{ds_cwc.cwc.long_name} ({ds_cwc.cwc.units})\", xlabel='Longitude',ylabel='Latitude')",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/03_EMIT_CWC_from_Reflectance.html#contact-info",
    "href": "python/03_EMIT_CWC_from_Reflectance.html#contact-info",
    "title": "3 Equivalent Water Thickness/Canopy Water Content from Imaging Spectroscopy Data",
    "section": "Contact Info:",
    "text": "Contact Info:\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-20-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Python Notebooks",
      "3 Canopy Water Content"
    ]
  },
  {
    "objectID": "python/05_SB_Vineyards.html",
    "href": "python/05_SB_Vineyards.html",
    "title": "Santa Barbara County Vineyards Analysis 2023",
    "section": "",
    "text": "Authors:\nGregory Halverson and Claire Villanueva-Weeks\nJet Propulsion Laboratory, California Institute of Technology\nThis research was carried out at the Jet Propulsion Laboratory, California Institute of Technology, and was sponsored by ECOSTRESS and the National Aeronautics and Space Administration (80NM0018D0004).\n© 2023. All rights reserved.",
    "crumbs": [
      "Python Notebooks",
      "5 Santa Barbara Vineyards"
    ]
  },
  {
    "objectID": "python/05_SB_Vineyards.html#set-up",
    "href": "python/05_SB_Vineyards.html#set-up",
    "title": "Santa Barbara County Vineyards Analysis 2023",
    "section": "5.1 Set up",
    "text": "5.1 Set up\nThese are some built-in Python functions we need for this notebook, including functions for handling filenames and dates.\nWe’re using the rioxarray package for loading raster data from a GeoTIFF file, and we’re importing it as rxr. We’re using the numpy library to handle arrays, and we’re importing it as np. We’re using the rasterio package to subset the data.\nWe’re using the geopandas library to load vector data from GeoJSON files, and we’re importing it as gpd. We’re using the shapely library to handle vector data and the pyproj library to handle projections.\nImport the emit_tools module and call use from emit_tools import emit_xarray help(emit_xarray) the help function to see how it can be used.\n\nNote: This function currently works with L1B Radiance and L2A Reflectance Data.\n\n\n# Some cells may generate warnings that we can ignore. Comment below lines to see.\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfrom os.path import join, expanduser, splitext, basename\nimport numpy as np\nimport pandas as pd\nimport geopandas as gpd\nfrom osgeo import gdal\nimport rasterio as rio\nimport rioxarray as rxr\nimport holoviews as hv\nimport hvplot.xarray\nimport hvplot.pandas\nimport geoviews as gv\nimport earthaccess\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import LinearSegmentedColormap\n\nfrom holoviews.streams import SingleTap\nfrom scipy.stats import linregress\n\nImporting Custom EMIT Tools to handle the EMIT reflectance data\n\nfrom modules.emit_tools import emit_xarray, ortho_xr\n\nWe can define these constants to prescribe the dimensions of our figures. Feel free to adjust these to fit your display. We’re setting the alpha to make the raster semi-transparent on top of the basemap.\n\nFIG_WIDTH_PX = 1080\nFIG_HEIGHT_PX = 720\nFIG_WIDTH_IN = 16\nFIG_HEIGHT_IN = 9\nFIG_ALPHA = 0.7\nBASEMAP = \"EsriImagery\"\nSEABORN_STYLE = \"whitegrid\"\nFILL_COLOR = \"none\"\nLINE_COLOR = \"red\"\nLINE_WIDTH = 3\nWEBMAP_PROJECTION = \"EPSG:4326\"\nsns.set_style(SEABORN_STYLE)",
    "crumbs": [
      "Python Notebooks",
      "5 Santa Barbara Vineyards"
    ]
  },
  {
    "objectID": "python/05_SB_Vineyards.html#locating-data-sources-and-loading-data",
    "href": "python/05_SB_Vineyards.html#locating-data-sources-and-loading-data",
    "title": "Santa Barbara County Vineyards Analysis 2023",
    "section": "5.2 Locating Data Sources and Loading Data",
    "text": "5.2 Locating Data Sources and Loading Data\nHere we are defining location of EMIT and ECOSTRESS raster files.\n\ndata_dir = \"../data/\" \ndata_dir\n\nHere we are loading in our vector data source. We can image the delineated agricultural field boundaries and take a look at the fields.\n\nag_file = join(data_dir,\"SB_ROI_ag.geojson\")\n# cut out the agroi section \nag_latlon = gpd.read_file(ag_file)\n\nag_fig = ag_latlon.to_crs(WEBMAP_PROJECTION).hvplot.polygons(\n    tiles=BASEMAP,\n    line_color = LINE_COLOR,\n    line_width = LINE_WIDTH,\n    fill_color = FILL_COLOR,\n    crs='EPSG:4326'\n) \n\nag_fig\n\n\nag_latlon\n\n\nLoading in EMIT reflectance data\nThis notebook requires downloading an EMIT scene if this was not done in a previous notebook.\n\n# Get requests https Session using Earthdata Login Info\nurl = 'https://data.lpdaac.earthdatacloud.nasa.gov/lp-prod-protected/EMITL2ARFL.001/EMIT_L2A_RFL_001_20230401T203803_2309114_003/EMIT_L2A_RFL_001_20230401T203803_2309114_003.nc'\nfs = earthaccess.get_requests_https_session()\n# Retrieve granule asset ID from URL (to maintain existing naming convention)\ngranule_asset_id = url.split('/')[-1]\n# Define Local Filepath\nfp = join(data_dir, granule_asset_id)\n# Download the Granule Asset if it doesn't exist\nif not os.path.isfile(fp):\n    with fs.get(url,stream=True) as src:\n        with open(fp,'wb') as dst:\n            for chunk in src.iter_content(chunk_size=64*1024*1024):\n                dst.write(chunk)\n\nTo open up the EMIT .nc file we will use the netCDF4, xarray and emit_tools libraries.\n\nEMIT_fp = join(data_dir, \"EMIT_L2A_RFL_001_20230401T203803_2309114_003.nc\") \nEMIT_fp\n\n\nrefl_ds = emit_xarray(EMIT_fp, ortho=True)\nrefl_ds\n\nHere some functions are set up to",
    "crumbs": [
      "Python Notebooks",
      "5 Santa Barbara Vineyards"
    ]
  },
  {
    "objectID": "python/05_SB_Vineyards.html#color-hex-codes-for-raster-plots",
    "href": "python/05_SB_Vineyards.html#color-hex-codes-for-raster-plots",
    "title": "Santa Barbara County Vineyards Analysis 2023",
    "section": "5.3 Color Hex-Codes for Raster Plots",
    "text": "5.3 Color Hex-Codes for Raster Plots\nHere we have set up convenience functions for interpolating Color Hex-Codes for Raster Plots\n\ndef interpolate_hex(hex1, hex2, ratio):\n    rgb1 = [int(hex1[i:i+2], 16) for i in (1, 3, 5)]\n    rgb2 = [int(hex2[i:i+2], 16) for i in (1, 3, 5)]\n    rgb = [int(rgb1[i] + (rgb2[i] - rgb1[i]) * ratio) for i in range(3)]\n    \n    return '#{:02x}{:02x}{:02x}'.format(*rgb)\n\ndef create_gradient(colors, steps):\n    gradient = []\n    \n    for i in range(len(colors) - 1):\n        for j in range(steps):\n            ratio = j / float(steps)\n            gradient.append(interpolate_hex(colors[i], colors[i+1], ratio))\n    \n    gradient.append(colors[-1])\n    \n    return gradient\n\ndef plot_cmap(cmap):\n    gradient = np.linspace(0, 1, 256)  # Gradient from 0 to 1\n    gradient = np.vstack((gradient, gradient))  # Make 2D image\n\n    # Display the colormap\n    plt.figure(figsize=(6, 2))\n    plt.imshow(gradient, aspect='auto', cmap=cmap)\n    plt.axis('off')\n    plt.show()\n\n\nNear-Infrared and Red Bands in EMIT Hyperspectral Cube\nThese are the nearest to 800 nm and 675 nm wavelengths, they can be used to calculate the NDVI using a ratio of the difference between the wavelengths to the sum of the wavelengths. NDVI is a metric by which we can estimate vegetation greenness.\nThe hvplot package extends xarray to allow us to plot maps. We’re reprojecting the raster geographic projection EPSG 4326 to overlay on the basemap with a latitude and longitude graticule. We will be using hvplot a few more times to look at the data we are using.\n\nDefining Color-Map for Near Infrared\n\nNIR_colors = [\"#000000\", \"#FF0000\"]\nNIR_gradient = create_gradient(NIR_colors, 100)\nNIR_cmap = LinearSegmentedColormap.from_list(name=\"NIR\", colors=NIR_colors)\nplot_cmap(NIR_cmap)\n\n\n\nExtracting 800 nm Near Infrared from EMIT\n\nNIR = refl_ds.sel(wavelengths=800, method=\"nearest\")\nNIR.rio.to_raster(join(data_dir,\"NIR_800.tif\"))\nNIR = rxr.open_rasterio(join(data_dir,\"NIR_800.tif\"), mask_and_scale=True).squeeze(\"band\", drop=True)\nNIR.data[NIR.data==-9999] = np.nan\nNIR_vmin, NIR_vmax = np.nanquantile(np.array(NIR), [0.02, 0.98])\nNIR_title = f\"{splitext(basename(EMIT_fp))[0]} ~800 nm\"\n\nNIR.rio.reproject(WEBMAP_PROJECTION).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=NIR_gradient, \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(NIR_vmin, NIR_vmax),\n    title=NIR_title\n)\n\n\n\n\nDefining Color-Map for Red\n\nred_colors = [\"#000000\", \"#FF0000\"]\nred_gradient = create_gradient(red_colors, 100)\nred_cmap = LinearSegmentedColormap.from_list(name=\"red\", colors=red_colors)\nplot_cmap(red_cmap)\n\n\nExtracting 675 nm Red from EMIT\n\nred = refl_ds.sel(wavelengths=675, method=\"nearest\")\nred.rio.to_raster(join(data_dir,\"red_675.tif\"))\nred = rxr.open_rasterio(join(data_dir,\"red_675.tif\")).squeeze(\"band\", drop=True)\nred.data[red.data==-9999] = np.nan\nred_vmin, red_vmax = np.nanquantile(np.array(red), [0.02, 0.98])\nred_title = f\"{splitext(basename(EMIT_fp))[0]} ~675 nm\"\n\nred.rio.reproject(WEBMAP_PROJECTION).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=red_gradient, \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(red_vmin, red_vmax),\n    title=red_title\n)\n\n\n\n\nDefining Color-Map for Vegetation Index\n\nNDVI_colors=[\n    \"#0000ff\",\n    \"#000000\",\n    \"#745d1a\",\n    \"#e1dea2\",\n    \"#45ff01\",\n    \"#325e32\"\n]\n\nNDVI_gradient = create_gradient(NDVI_colors, 100)\nNDVI_cmap = LinearSegmentedColormap.from_list(name=\"NDVI\", colors=NDVI_colors)\nplot_cmap(NDVI_cmap)\n\n\nCalculating Vegetation Index\n\nNDVI = (NIR - red)/(NIR + red)\nNDVI.rio.to_raster(join(data_dir,\"NDVI.tif\"))\nNDVI = rxr.open_rasterio(join(data_dir,\"NDVI.tif\")).squeeze(\"band\", drop=True)\nNDVI_vmin, NDVI_vmax = np.nanquantile(np.array(NDVI), [0.02, 0.98])\nNDVI_title = \"Normalized Difference Vegetation Index\"\n\nNDVI_scene_map = NDVI.rio.reproject(WEBMAP_PROJECTION).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=NDVI_gradient, \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(NDVI_vmin, NDVI_vmax),\n    title=NDVI_title\n)\n\nNDVI_scene_map\n\n\n\n\nLoading in and visualizing our EWT data\nWe can now open up the EWT data that you calculated earlier with the LP DAAC EWT. The dataset is now in .tif format which means we can just open this one up\n\nEWT_filename = join(data_dir,'EMIT_L2A_RFL_001_20230401T203751_2309114_002_roi_bbox_cwc_merged.tif')\nEWT_filename\n\n\nDefining Color-Map for EWT\n\nEWT_colors = [\"#FFFFFF\", \"#0000FF\"]\nEWT_gradient = create_gradient(EWT_colors, 100)\nEWT_cmap = LinearSegmentedColormap.from_list(name=\"EWT\", colors=EWT_colors)\nplot_cmap(EWT_cmap)\n\n\n\nMapping EWT with color ramp\n\nEWT = rxr.open_rasterio(EWT_filename).squeeze(\"band\", drop=True)\nEWT_vmin, EWT_vmax = np.nanquantile(np.array(EWT), [0.02, 0.98])\nEWT_title = \"Equivalent Water Thickness \"\n\nEWT.rio.reproject(WEBMAP_PROJECTION,nodata=np.nan).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=EWT_gradient, \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(EWT_vmin, EWT_vmax),\n    title=EWT_title\n)\n\n\n\n\nLoading an ECOSTRESS LST granule\n\nST_filename = join(data_dir, \"ECOv002_L2T_LSTE_26860_001_10SGD_20230401T203733_0710_01_LST.tif\") \nST_filename\n\nThe temperatures in the L2T_LSTE product are given in Kelvin. To convert them to Celsius, we subtract 273.15.\n\nST_K = rxr.open_rasterio(ST_filename).squeeze('band', drop=True)\nST_C = ST_K - 273.15\n\nST_C.rio.to_raster(join(data_dir,\"ST.tif\"))\n\nST_vmin, ST_vmax = np.nanquantile(np.array(ST_C), [0.02, 0.98])\nST_title = \"ECOSTRESS Surface Temperature (Celsius)\"\n\nST_C.rio.reproject(WEBMAP_PROJECTION,nodata=np.nan).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=\"jet\", \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(ST_vmin, ST_vmax),\n    title=ST_title\n)\n\n\n\nDefining Color Ramps\nColor ramps, are used to map valued over a range of colors based on the data and goals of visualization. We’ve traditionally used the “jet” rainbow color-scheme to represent temperature, but this color-scheme is not accessible for all vision types.\nRed-green colorblindness or deuteranomaly affects approximately 10% of people, so we are working on making a color ramp with that in mind. You can use hex codes to program in your own colors!\n\nST_colors = [\n    \"#054fb9\",\n    \"#0073e6\",\n    \"#8babf1\",\n    \"#cccccc\",\n    \"#e1ad01\",\n    \"#f57600\",\n    \"#c44601\"\n]\n\nST_gradient = create_gradient(ST_colors, 100)\nST_cmap = LinearSegmentedColormap.from_list(name=\"ST\", colors=ST_colors)\nplot_cmap(ST_cmap)\n\n\nST_vmin, ST_vmax = np.nanquantile(np.array(ST_C), [0.02, 0.98])\nST_title = \"ECOSTRESS Surface Temperature (Celsius)\"\n\nST_C.rio.reproject(WEBMAP_PROJECTION).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=ST_gradient, \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(ST_vmin, ST_vmax),\n    title=ST_title\n)\n\n\n\nLoading ECOSTRESS Evapotranspiration granule\n\nET_filename = join(data_dir, \"ECOv002_L3T_JET_26860_001_10SGD_20230401T203732_0700_01_ETdaily.tif\")\nET_filename\n\nWe have an established color-map for representing ECOSTRESS evapotranspiration that we believe to be inclusive.\n\nET_colors = [\n    \"#f6e8c3\",\n    \"#d8b365\",\n    \"#99974a\",\n    \"#53792d\",\n    \"#6bdfd2\",\n    \"#1839c5\"\n]\n\nET_gradient = create_gradient(ET_colors, 100)\nET_cmap = LinearSegmentedColormap.from_list(name=\"ET\", colors=ET_colors)\nplot_cmap(ET_cmap)\n\nThe daily evapotranspiration product from ECOSTRESS is given in millimeters per day.\n\nET = rxr.open_rasterio(ET_filename).squeeze('band', drop=True)\nET_vmin, ET_vmax = np.nanquantile(np.array(ET), [0.02, 0.98])\nET_title = \"ECOSTRESS Evapotranspiration (mm / day)\"\n\nET.rio.reproject(WEBMAP_PROJECTION).hvplot.image(\n    crs=WEBMAP_PROJECTION, \n    cmap=ET_gradient, \n    alpha=FIG_ALPHA, \n    tiles=BASEMAP,\n    clim=(ET_vmin, ET_vmax),\n    title=ET_title\n)",
    "crumbs": [
      "Python Notebooks",
      "5 Santa Barbara Vineyards"
    ]
  },
  {
    "objectID": "python/05_SB_Vineyards.html#subsetting-the-data-over-our-roi",
    "href": "python/05_SB_Vineyards.html#subsetting-the-data-over-our-roi",
    "title": "Santa Barbara County Vineyards Analysis 2023",
    "section": "5.4. Subsetting the data over our ROI",
    "text": "5.4. Subsetting the data over our ROI\nTo clip the raster image to the extent of the vector dataset, we want to subset the raster to the bounds of the vector dataset. This dataset is included here in GeoJSON format, which we’ll load in as a geodataframe using the geopandas package.\nThe GeoJSON is provided by CalAg and includes information on agricultural field boundaries in the Santa Barbara County. We can use this code to select the largest polygons that are classified as vineyards.\nUsing the rioxarray clip function, we can subset the data using a geojson. to visualize the area that are vineyards are located, I have selected a boundary polygon. Overlaid are the outlines of some of the larger vineyards in the scene.\n\ntarget_text = 'WINE'\n\nfiltered_gdf = ag_latlon[ag_latlon['crop_list'].str.contains(target_text, case=False, na=False)]\n\nQ1 = filtered_gdf['size'].quantile(0.25)\nQ3 = filtered_gdf['size'].quantile(0.75)\nIQR = Q3 - Q1\n\n# Define the threshold as Q3 + 1.5 * IQR (adjust multiplier as needed)\nthreshold_size = Q3 + 1.5 * IQR\n\n# Filter polygons based on size greater than the threshold\nvineyard_polygons = filtered_gdf[filtered_gdf['size'] &gt; threshold_size]\nvineyard_polygons\n\n\nvineyard_plot = vineyard_polygons.to_crs(\"EPSG:4326\").hvplot.polygons(\n    crs=WEBMAP_PROJECTION,\n    tiles=BASEMAP,\n    alpha=FIG_ALPHA,\n    width=FIG_WIDTH_PX,\n    height=FIG_HEIGHT_PX,\n    fill_color = 'none',\n    line_color = 'r',\n    line_width = 3,\n    title=\"Vineyards of Acreage greater than IQR3\",\n    fontscale=1.5\n    )\n\nhv.extension('bokeh')\nvineyard_plot",
    "crumbs": [
      "Python Notebooks",
      "5 Santa Barbara Vineyards"
    ]
  },
  {
    "objectID": "python/05_SB_Vineyards.html#interactively-mapping-our-data",
    "href": "python/05_SB_Vineyards.html#interactively-mapping-our-data",
    "title": "Santa Barbara County Vineyards Analysis 2023",
    "section": "Interactively mapping our data",
    "text": "Interactively mapping our data\nNow that we have selected a subset of the agriculture in the area we can subset our data using the rioxarray function clip.\n\nNDVI_vineyard = NDVI.rio.clip(vineyard_polygons.geometry.values,vineyard_polygons.crs, all_touched=True)\nNDVI_vineyard.rio.to_raster(join(data_dir,\"NDVI_vineyard.tif\"))\nNDVI_vineyard = rxr.open_rasterio(join(data_dir,\"NDVI_vineyard.tif\")).squeeze(\"band\", drop=True)\nNDVI_vmin, NDVI_vmax = np.nanquantile(np.array(NDVI_vineyard), [0.02, 0.98])\n\nST_vineyard = ST_C.rio.clip(vineyard_polygons.geometry.values,vineyard_polygons.crs, all_touched=True)\nST_vineyard.rio.to_raster(join(data_dir,\"ST_vineyard.tif\"))\nST_vineyard = rxr.open_rasterio(join(data_dir,\"ST_vineyard.tif\")).squeeze(\"band\", drop=True)\nST_vmin, ST_vmax = np.nanquantile(np.array(ST_vineyard), [0.02, 0.98])\n\nEWT_vineyard = EWT.rio.clip(vineyard_polygons.geometry.values,vineyard_polygons.crs, all_touched=True)\nEWT_vineyard.rio.to_raster(join(data_dir,\"EWT_vineyard.tif\"))\nEWT_vineyard = rxr.open_rasterio(join(data_dir,\"EWT_vineyard.tif\")).squeeze(\"band\", drop=True)\nEWT_vmin, EWT_vmax = np.nanquantile(np.array(EWT_vineyard), [0.02, 0.98])\n\nET_vineyard = ET.rio.clip(vineyard_polygons.geometry.values,vineyard_polygons.crs, all_touched=True)\nET_vineyard.rio.to_raster(join(data_dir,\"ET_vineyard.tif\"))\nET_vineyard = rxr.open_rasterio(join(data_dir,\"ET_vineyard.tif\")).squeeze(\"band\", drop=True)\nET_vmin, ET_vmax = np.nanquantile(np.array(ET_vineyard), [0.02, 0.98])\n\n\n5.5 Interactive Maps\nWe can use holoviews to plot an interactive clickable map of these subsets over our ROI. When you click a point on the map, you can see the coordinates and data for that pixel. You can uncomment the crs and tiles option lines below to visualize the data on a basemap, but this breaks thee interactive feature. Additionally, the interactive feature seems to also affect the color bar min and max limits (clim) whether applied manually or automatically. This may cause the color ramp to display incorrectly. Selecting and exporting values will work properly.\n\nclicked_values = []\n\n# Reproject raster data\nNDVI_vineyard_projected = NDVI_vineyard.rio.reproject(WEBMAP_PROJECTION, nodata=np.nan)\nST_vineyard_projected = ST_vineyard.rio.reproject(WEBMAP_PROJECTION, nodata=np.nan)\nEWT_vineyard_projected = EWT_vineyard.rio.reproject(WEBMAP_PROJECTION, nodata=np.nan)\nET_vineyard_projected = ET_vineyard.rio.reproject(WEBMAP_PROJECTION, nodata=np.nan)\n\n# Load geospatial data\nvineyard_polygons = gpd.read_file(join(data_dir,\"vineyard_polygons_filtered.geojson\"))\n\n# Set up the SingleTap stream\nstream = SingleTap()\n\n# Define a function to process clicks\ndef interactive_click(x, y):\n    if None not in [x, y]:\n        NDVI_value = NDVI_vineyard_projected.sel(x=x, y=y, method=\"nearest\").values\n        ST_value = ST_vineyard_projected.sel(x=x, y=y, method=\"nearest\").values\n        ET_value = ET_vineyard_projected.sel(x=x, y=y, method=\"nearest\").values\n        EWT_value = EWT_vineyard_projected.sel(x=x, y=y, method=\"nearest\").values\n        print(f\"Lon: {x}, Lat: {y}, NDVI: {NDVI_value}, ST: {ST_value}, EWT: {EWT_value}, ET: {ET_value}\")\n        clicked_values.append({ \"Lon\": x, \"Lat\": y, \"NDVI\": NDVI_value, \"ST\": ST_value, \"ET\": ET_value, \"EWT\": EWT_value})\n\n    return hv.Points((x, y)).opts(color='green', size=10)\n\n# Create two separate plots and share the stream between them\nNDVI_vineyard_map = NDVI_vineyard_projected.hvplot.image(\n    #crs=WEBMAP_PROJECTION,\n    cmap=NDVI_gradient,\n    clim=(NDVI_vmin, NDVI_vmax),\n    alpha=FIG_ALPHA, \n    #tiles=BASEMAP,\n    title=NDVI_title\n) * vineyard_polygons.to_crs(WEBMAP_PROJECTION).hvplot.polygons(\n    #crs=WEBMAP_PROJECTION,\n    line_color=LINE_COLOR,\n    line_width=LINE_WIDTH,\n    fill_color=FILL_COLOR\n).opts(width=600, height=400, fontscale=1.5)\n\nST_vineyard_map = ST_vineyard_projected.hvplot.image(\n    #crs=WEBMAP_PROJECTION,\n    cmap=ST_gradient, \n    #clim=(ST_vmin, ST_vmax),\n    alpha=FIG_ALPHA, \n    #tiles=BASEMAP,\n    title=ST_title\n) * vineyard_polygons.to_crs(WEBMAP_PROJECTION).hvplot.polygons(\n    #crs=WEBMAP_PROJECTION,\n    line_color=LINE_COLOR,\n    line_width=LINE_WIDTH,\n    fill_color=FILL_COLOR\n).opts(width=600, height=400, fontscale=1.5)\n\n# Duplicate the plots for the second row\nEWT_vineyard_map = EWT_vineyard_projected.hvplot.image(\n    #crs=WEBMAP_PROJECTION,\n    cmap=EWT_gradient,\n    #clim=(EWT_vmin, EWT_vmax), \n    alpha=FIG_ALPHA, \n    #tiles=BASEMAP,\n    title=EWT_title\n) * vineyard_polygons.to_crs(WEBMAP_PROJECTION).hvplot.polygons(\n    #crs=WEBMAP_PROJECTION,\n    line_color=LINE_COLOR,\n    line_width=LINE_WIDTH,\n    fill_color=FILL_COLOR\n).opts(width=600, height=400, fontscale=1.5)\n\nET_vineyard_map = ET_vineyard_projected.hvplot.image(\n    #crs=WEBMAP_PROJECTION,\n    rasterize=True, \n    cmap=ET_gradient, \n    #clim=(ET_vmin, ET_vmax),\n    alpha=FIG_ALPHA, \n    #tiles=BASEMAP,\n    title=ET_title\n) * vineyard_polygons.to_crs(WEBMAP_PROJECTION).hvplot.polygons(\n    #crs=WEBMAP_PROJECTION,\n    line_color=LINE_COLOR,\n    line_width=LINE_WIDTH,\n    fill_color=FILL_COLOR\n).opts(width=600, height=400, fontscale=1.5)\n\n# Create a DynamicMap for interactive clicks\nclick = hv.DynamicMap(interactive_click, streams=[stream])\n\n# Display the plots in a 2x2 grid with two rows along with the interactive click behavior\nlayout = (click * NDVI_vineyard_map * click + click * ST_vineyard_map * click +\n          click * EWT_vineyard_map * click + click * ET_vineyard_map * click).cols(2)\nlayout\n\nSave the data from our clicked points into a csv that we can use for plotting.\n\n# Save the clicked values to a CSV file after all clicks are processed\nif clicked_values:\n    df = pd.DataFrame(clicked_values)\n    df.to_csv(join(data_dir,\"clicked_values.csv\"), index=False)\n    print(\"Clicked values saved to 'clicked_values.csv'\")\nelse:\n    print(\"No clicked values to save.\")\n\n\n\n5.6 Scatter Plots\nNow we can use the data saved into our csv to create scatter plots to visualize the correlations among ET, ST, EWT, and NDVI.\n\n# Replace \"your_data.csv\" with the actual CSV file containing your data\ndf = pd.read_csv(join(data_dir,\"clicked_values.csv\"))\n\n# Filter out Lat/Lon Columns\nfiltered_columns = df.columns[2:]\n\n# Get combinations of all pairs of columns\ncombinations = [(col1, col2) for i, col1 in enumerate(filtered_columns) for col2 in filtered_columns[i + 1:]]\n\n# Set up subplots without equal aspect ratio and sharex/sharey set to False\nfig, axes = plt.subplots(nrows=3, ncols=2, figsize=(10, 12), sharex=False, sharey=False)\nfig.subplots_adjust(hspace=0.5)\n\n# Iterate over combinations and create scatter plots with best-fit lines\nfor (col1, col2), ax in zip(combinations, axes.flatten()):\n    sns.regplot(x=col1, y=col2, data=df, ax=ax, ci=None, line_kws={\"color\": \"red\"})\n\n    # Filter out rows with missing values in the selected columns\n    filtered_df = df[[col1, col2]].dropna()\n\n    # Calculate and plot the best-fit line using linear regression\n    slope, intercept, r_value, p_value, std_err = linregress(filtered_df[col1], filtered_df[col2])\n    x_range = np.linspace(filtered_df[col1].min(), filtered_df[col1].max(), 100)\n    y_fit = slope * x_range + intercept\n    ax.plot(x_range, y_fit, color='blue', linestyle='--', label=f'Best Fit (R={r_value:.2f})')\n\n    ax.legend()\n    ax.set_xlabel(col1)\n    ax.set_ylabel(col2)\n    ax.set_title(f\"{col1} vs {col2}\")\n\n    # Set x and y axis ticks with different values\n    ax.set_xticks(np.linspace(filtered_df[col1].min(), filtered_df[col1].max(), 5))\n    ax.set_yticks(np.linspace(filtered_df[col2].min(), filtered_df[col2].max(), 5))\n\n# Adjust layout\nplt.tight_layout()\n\n# Show the plot\nplt.show()",
    "crumbs": [
      "Python Notebooks",
      "5 Santa Barbara Vineyards"
    ]
  },
  {
    "objectID": "setup/prerequisites.html",
    "href": "setup/prerequisites.html",
    "title": "Prerequisites",
    "section": "",
    "text": "Prerequisites\nTo follow along during the workshop, or to run through the notebooks contained within the repository using the Openscapes 2i2c Cloud JupyterHub (cloud workspace), the following are required. All software or accounts are free.\n\nEarthdata Login account\n\nCreate an Earthdata Login account (if you don’t already have one) at https://urs.earthdata.nasa.gov/users/new\nRemember your username and password; you will need them to download or access data during the workshop and beyond.\n\nGitHub username\n\nCreate a GitHub account (if you don’t already have one) at https://github.com/join. Follow optional advice on choosing your username\nYour GitHub username is used to enable you access to a cloud environment during the workshop. To gain access, please request access to the NASA Openscapes JupyterHub using this form. You will receive an email invitation to join the organization on GitHub. You must join to gain access to the workspace.\n\n\nNetrc file\n\nThis file is needed to access NASA Earthdata assets from a scripting environment like Python.\nThere are multiple methods to create a .netrc file. For this workshop, earthaccess package is used to automatically create a netrc file using your Earthdata login credentials if one does not exist. There are detailed instruction available for creating a .netrc file using other methods here.\n\nLaptop or tablet\n\nParticipation in the exercises requires a laptop or tablet. Yes, a tablet works too! All workshop participants will have access to a 2i2c Jupyter Lab instance running in AWS us-west 2.",
    "crumbs": [
      "Setup Instructions",
      "Prerequisites"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html",
    "href": "setup/workshop_setup.html",
    "title": "Cloud Workspace Setup",
    "section": "",
    "text": "If you plan to use this repository with the Openscapes 2i2c JupyterHub Cloud Workspace there are no additional setup requirements for the Python environment. All packages needed are included unless specified within a notebook, in which case a cell will be dedicated to installing the necessary Python libraries using the appropriate package manager.\nAfter completing the prerequisites you will have access to the Openscapes 2i2c JupyterHub cloud workspace. Click here to start JupyterLab. Use your email and the provided password to sign in. This password will be provided in the workshop. If you’re interested in using the 2i2c cloud workspace outside of the workshop, please contact us.\nAfter signing in you will be prompted for some server options:\nBe sure to select the radio button for Python and a size of 14.8 GB RAM and up to 3.75 CPUs.\nAt this point you can use the terminal to clone the repository.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#cloning-the-vitals-repository",
    "href": "setup/workshop_setup.html#cloning-the-vitals-repository",
    "title": "Cloud Workspace Setup",
    "section": "Cloning the VITALS Repository",
    "text": "Cloning the VITALS Repository\nIf you plan to edit or contribute to the VITALS repository, we recommend following a fork and pull workflow: first fork the repository, then clone your fork to your local machine, make changes, push changes to your fork, then make a pull request back to the main repository. An example can be found in the CONTRIBUTING.md file.\nIf you just want to work with the notebooks or modules, you can simply clone the repository.\nTo clone the repository, navigate to the directory where you want to store the repository on your local machine, then type the following:\ngit clone https://github.com/nasa/VITALS.git",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#troubleshooting",
    "href": "setup/workshop_setup.html#troubleshooting",
    "title": "Cloud Workspace Setup",
    "section": "Troubleshooting",
    "text": "Troubleshooting\nWe recommend Shutting down all kernels after running each notebook. This will clear the memory used by the previous notebook, and is necessary to run some of the more memory intensive notebooks.\n\n\n\nNo single notebook exceeds roughly the limit using the provided data, but if you choose to use your own data in the notebook, or have 2 notebooks open and do not shut down the kernel, you may get an out of memory error.\nIf you elect to try this on your own data/ROI, you may need to select a larger server size. This will often happen if you are using the last EMIT scene from an orbit. In some cases those can be almost double the size of a normal scene. Please select the smallest possible.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  },
  {
    "objectID": "setup/workshop_setup.html#contact-info",
    "href": "setup/workshop_setup.html#contact-info",
    "title": "Cloud Workspace Setup",
    "section": "Contact Info",
    "text": "Contact Info\nEmail: LPDAAC@usgs.gov\nVoice: +1-866-573-3222\nOrganization: Land Processes Distributed Active Archive Center (LP DAAC)¹\nWebsite: https://lpdaac.usgs.gov/\nDate last modified: 05-24-2024\n¹Work performed under USGS contract 140G0121D0001 for NASA contract NNG14HH33I.",
    "crumbs": [
      "Setup Instructions",
      "Cloud Workspace Setup"
    ]
  }
]